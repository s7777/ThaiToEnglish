{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pbZ0caYfzkF",
        "outputId": "2f531a9e-0148-4e66-e06a-7115f7e50b44"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\t#line = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/tha.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-thai.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-thai.pkl\n",
            "[Hi] => [สวัสดี]\n",
            "[Run] => [วิ่ง]\n",
            "[Who] => [ใคร]\n",
            "[Wow] => [ว้าว]\n",
            "[Fire] => [ยิง]\n",
            "[Help] => [ช่วยด้วย]\n",
            "[Jump] => [กระโดด]\n",
            "[Jump] => [กระโดด]\n",
            "[Stop] => [หยุด]\n",
            "[Wait] => [รอก่อน]\n",
            "[Go on] => [ต่อไป]\n",
            "[Hello] => [สวัสดี]\n",
            "[Hurry] => [เร็ว ๆ]\n",
            "[I see] => [ฉันเข้าใจแล้ว]\n",
            "[I try] => [ฉันลองแล้ว]\n",
            "[I won] => [ฉันชนะ]\n",
            "[Oh no] => [ไม่นะ]\n",
            "[Relax] => [พักผ่อน]\n",
            "[Smile] => [ยิ้ม]\n",
            "[Attack] => [โจมตี]\n",
            "[Cheers] => [เชียร์]\n",
            "[Freeze] => [ฟรีส]\n",
            "[Freeze] => [แข็ง]\n",
            "[Get up] => [ตื่น]\n",
            "[Go now] => [ไปเลยตอนนี้]\n",
            "[Got it] => [เข้าใจแล้ว]\n",
            "[Got it] => [จับได้แล้ว]\n",
            "[Got it] => [ทำได้แล้ว]\n",
            "[Got it] => [เข้าใจมั้ย]\n",
            "[He ran] => [เขาวิ่ง]\n",
            "[Hop in] => [กระโดดไป]\n",
            "[Hug me] => [กอดฉัน]\n",
            "[I fell] => [ฉันล้ม]\n",
            "[I left] => [ฉันออก]\n",
            "[I lied] => [ฉันโกหก]\n",
            "[I lost] => [ฉันแพ้]\n",
            "[I quit] => [ฉันออก]\n",
            "[I work] => [ฉันทำงาน]\n",
            "[Im 19] => [ฉันอายุ 19 ปี]\n",
            "[Im OK] => [ฉันโอเค]\n",
            "[Im OK] => [ฉันไม่เป็นไร]\n",
            "[Im up] => [ฉันตื่น]\n",
            "[Listen] => [ฟัง]\n",
            "[Listen] => [ฟังนะ]\n",
            "[No way] => [ไม่มีทาง]\n",
            "[Really] => [จริงเหรอ]\n",
            "[Try it] => [ลองมัน]\n",
            "[We try] => [พวกเราลองแล้ว]\n",
            "[We won] => [เราชนะ]\n",
            "[Why me] => [ทำไมต้องเป็นฉัน]\n",
            "[Ask Tom] => [ถามทอม]\n",
            "[Awesome] => [สุดยอด]\n",
            "[Beat it] => [ทำลายมัน]\n",
            "[Call me] => [โทรหาฉัน]\n",
            "[Come in] => [เข้ามาข้างใน]\n",
            "[Get out] => [ออกไป]\n",
            "[Get out] => [ออกไป]\n",
            "[Go away] => [ไปไกล ๆ]\n",
            "[Go away] => [ไปซะ]\n",
            "[Go away] => [ไปไกล ๆ]\n",
            "[Go home] => [กลับบ้าน]\n",
            "[Goodbye] => [ลาก่อน]\n",
            "[He runs] => [เขาวิ่ง]\n",
            "[Help me] => [ช่วยฉันด้วย]\n",
            "[Help me] => [ช่วยฉัน]\n",
            "[Hi Tom] => [สวัสดีทอม]\n",
            "[Hold it] => [จับมันไว้]\n",
            "[I agree] => [ฉันเห็นด้วย]\n",
            "[Ill go] => [ฉันจะไป]\n",
            "[Im Tom] => [ฉันคือทอม]\n",
            "[Im hot] => [ฉันร้อน]\n",
            "[Im ill] => [ฉันป่วย]\n",
            "[Im shy] => [ฉันอาย]\n",
            "[Its OK] => [มันพอได้]\n",
            "[Its OK] => [มันโอเค]\n",
            "[Its me] => [ฉันเอง]\n",
            "[Me too] => [ฉันด้วยเหมือนกัน]\n",
            "[Open up] => [เปิดออก]\n",
            "[Perfect] => [เยี่ยมยอด]\n",
            "[Perfect] => [พอดีเป๊ะ]\n",
            "[Show me] => [แสดงให้ฉัน]\n",
            "[Shut up] => [เงียบ]\n",
            "[Tell me] => [บอกฉัน]\n",
            "[Tom ran] => [ทอมวิ่ง]\n",
            "[Wake up] => [ตื่น]\n",
            "[Wake up] => [ตื่น]\n",
            "[Wash up] => [ล้าง]\n",
            "[We lost] => [เราแพ้]\n",
            "[Welcome] => [ยินดีต้อนรับ]\n",
            "[Who won] => [ใครชนะ]\n",
            "[You run] => [คุณวิ่ง]\n",
            "[Be a man] => [เป็นลูกผู้ชายหน่อย]\n",
            "[Be quiet] => [เงียบ ๆ]\n",
            "[Call Tom] => [โทรหาทอม]\n",
            "[Cheer up] => [สู้ตาย]\n",
            "[Dont go] => [อย่าไป]\n",
            "[Find Tom] => [หาทอม]\n",
            "[Grab Tom] => [จับทอม]\n",
            "[Grab him] => [จับเขา]\n",
            "[Have fun] => [ขอให้สนุก]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhTkEukQ7ONz",
        "outputId": "cd031a2b-d029-4efa-f0f3-6008ac990eac"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-thai.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 2910\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:2000], dataset[910:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-thai-both.pkl')\n",
        "save_clean_data(train, 'english-thai-train.pkl')\n",
        "save_clean_data(test, 'english-thai-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-thai-both.pkl\n",
            "Saved: english-thai-train.pkl\n",
            "Saved: english-thai-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umixN0CZ7SpA",
        "outputId": "6f2af629-f92b-433f-b8b4-e35625043b6a"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Thai Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Thai Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 1977\n",
            "English Max Length: 20\n",
            "Thai Vocabulary Size: 2971\n",
            "Thai Max Length: 6\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 6, 256)            760576    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 20, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 20, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 20, 1977)          508089    \n",
            "=================================================================\n",
            "Total params: 2,319,289\n",
            "Trainable params: 2,319,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "32/32 - 21s - loss: 4.1773 - val_loss: 1.9782\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.97822, saving model to model.h5\n",
            "Epoch 2/100\n",
            "32/32 - 13s - loss: 1.9085 - val_loss: 1.8363\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.97822 to 1.83632, saving model to model.h5\n",
            "Epoch 3/100\n",
            "32/32 - 13s - loss: 1.7983 - val_loss: 1.8405\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.83632\n",
            "Epoch 4/100\n",
            "32/32 - 13s - loss: 1.7782 - val_loss: 1.8495\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.83632\n",
            "Epoch 5/100\n",
            "32/32 - 13s - loss: 1.8135 - val_loss: 1.7696\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.83632 to 1.76958, saving model to model.h5\n",
            "Epoch 6/100\n",
            "32/32 - 13s - loss: 1.7102 - val_loss: 1.7207\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.76958 to 1.72072, saving model to model.h5\n",
            "Epoch 7/100\n",
            "32/32 - 13s - loss: 1.6529 - val_loss: 1.6911\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.72072 to 1.69112, saving model to model.h5\n",
            "Epoch 8/100\n",
            "32/32 - 13s - loss: 1.6070 - val_loss: 1.6590\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.69112 to 1.65903, saving model to model.h5\n",
            "Epoch 9/100\n",
            "32/32 - 13s - loss: 1.5560 - val_loss: 1.6329\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.65903 to 1.63285, saving model to model.h5\n",
            "Epoch 10/100\n",
            "32/32 - 13s - loss: 1.5142 - val_loss: 1.6100\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.63285 to 1.60998, saving model to model.h5\n",
            "Epoch 11/100\n",
            "32/32 - 13s - loss: 1.4775 - val_loss: 1.5891\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.60998 to 1.58906, saving model to model.h5\n",
            "Epoch 12/100\n",
            "32/32 - 13s - loss: 1.4462 - val_loss: 1.5765\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.58906 to 1.57651, saving model to model.h5\n",
            "Epoch 13/100\n",
            "32/32 - 13s - loss: 1.4185 - val_loss: 1.5527\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.57651 to 1.55271, saving model to model.h5\n",
            "Epoch 14/100\n",
            "32/32 - 13s - loss: 1.3817 - val_loss: 1.5461\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.55271 to 1.54615, saving model to model.h5\n",
            "Epoch 15/100\n",
            "32/32 - 13s - loss: 1.3593 - val_loss: 1.5275\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.54615 to 1.52751, saving model to model.h5\n",
            "Epoch 16/100\n",
            "32/32 - 13s - loss: 1.3258 - val_loss: 1.5297\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.52751\n",
            "Epoch 17/100\n",
            "32/32 - 13s - loss: 1.3111 - val_loss: 1.5204\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.52751 to 1.52040, saving model to model.h5\n",
            "Epoch 18/100\n",
            "32/32 - 13s - loss: 1.2897 - val_loss: 1.5168\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.52040 to 1.51676, saving model to model.h5\n",
            "Epoch 19/100\n",
            "32/32 - 13s - loss: 1.2767 - val_loss: 1.5164\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.51676 to 1.51637, saving model to model.h5\n",
            "Epoch 20/100\n",
            "32/32 - 13s - loss: 1.2641 - val_loss: 1.5154\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.51637 to 1.51540, saving model to model.h5\n",
            "Epoch 21/100\n",
            "32/32 - 13s - loss: 1.2478 - val_loss: 1.5103\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.51540 to 1.51029, saving model to model.h5\n",
            "Epoch 22/100\n",
            "32/32 - 13s - loss: 1.2326 - val_loss: 1.5095\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.51029 to 1.50950, saving model to model.h5\n",
            "Epoch 23/100\n",
            "32/32 - 13s - loss: 1.2207 - val_loss: 1.5121\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.50950\n",
            "Epoch 24/100\n",
            "32/32 - 13s - loss: 1.2067 - val_loss: 1.5071\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.50950 to 1.50707, saving model to model.h5\n",
            "Epoch 25/100\n",
            "32/32 - 13s - loss: 1.1955 - val_loss: 1.5134\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.50707\n",
            "Epoch 26/100\n",
            "32/32 - 13s - loss: 1.1863 - val_loss: 1.5142\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.50707\n",
            "Epoch 27/100\n",
            "32/32 - 13s - loss: 1.1757 - val_loss: 1.5081\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.50707\n",
            "Epoch 28/100\n",
            "32/32 - 13s - loss: 1.1620 - val_loss: 1.5143\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.50707\n",
            "Epoch 29/100\n",
            "32/32 - 13s - loss: 1.1498 - val_loss: 1.5151\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.50707\n",
            "Epoch 30/100\n",
            "32/32 - 13s - loss: 1.1405 - val_loss: 1.5080\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.50707\n",
            "Epoch 31/100\n",
            "32/32 - 13s - loss: 1.1298 - val_loss: 1.5169\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.50707\n",
            "Epoch 32/100\n",
            "32/32 - 13s - loss: 1.1223 - val_loss: 1.5138\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.50707\n",
            "Epoch 33/100\n",
            "32/32 - 13s - loss: 1.1094 - val_loss: 1.5155\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.50707\n",
            "Epoch 34/100\n",
            "32/32 - 13s - loss: 1.0984 - val_loss: 1.5156\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.50707\n",
            "Epoch 35/100\n",
            "32/32 - 13s - loss: 1.0875 - val_loss: 1.5167\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.50707\n",
            "Epoch 36/100\n",
            "32/32 - 13s - loss: 1.0779 - val_loss: 1.5172\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.50707\n",
            "Epoch 37/100\n",
            "32/32 - 13s - loss: 1.0696 - val_loss: 1.5209\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.50707\n",
            "Epoch 38/100\n",
            "32/32 - 14s - loss: 1.0609 - val_loss: 1.5214\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.50707\n",
            "Epoch 39/100\n",
            "32/32 - 14s - loss: 1.0512 - val_loss: 1.5223\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.50707\n",
            "Epoch 40/100\n",
            "32/32 - 14s - loss: 1.0366 - val_loss: 1.5174\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.50707\n",
            "Epoch 41/100\n",
            "32/32 - 14s - loss: 1.0252 - val_loss: 1.5218\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.50707\n",
            "Epoch 42/100\n",
            "32/32 - 13s - loss: 1.0127 - val_loss: 1.5167\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.50707\n",
            "Epoch 43/100\n",
            "32/32 - 14s - loss: 1.0010 - val_loss: 1.5170\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.50707\n",
            "Epoch 44/100\n",
            "32/32 - 13s - loss: 0.9933 - val_loss: 1.5152\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.50707\n",
            "Epoch 45/100\n",
            "32/32 - 13s - loss: 0.9889 - val_loss: 1.5413\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.50707\n",
            "Epoch 46/100\n",
            "32/32 - 13s - loss: 0.9793 - val_loss: 1.5180\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.50707\n",
            "Epoch 47/100\n",
            "32/32 - 13s - loss: 0.9638 - val_loss: 1.5224\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.50707\n",
            "Epoch 48/100\n",
            "32/32 - 13s - loss: 0.9481 - val_loss: 1.5153\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.50707\n",
            "Epoch 49/100\n",
            "32/32 - 13s - loss: 0.9340 - val_loss: 1.5177\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.50707\n",
            "Epoch 50/100\n",
            "32/32 - 13s - loss: 0.9202 - val_loss: 1.5106\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.50707\n",
            "Epoch 51/100\n",
            "32/32 - 13s - loss: 0.9069 - val_loss: 1.5075\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.50707\n",
            "Epoch 52/100\n",
            "32/32 - 13s - loss: 0.8932 - val_loss: 1.5043\n",
            "\n",
            "Epoch 00052: val_loss improved from 1.50707 to 1.50429, saving model to model.h5\n",
            "Epoch 53/100\n",
            "32/32 - 13s - loss: 0.8806 - val_loss: 1.5071\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.50429\n",
            "Epoch 54/100\n",
            "32/32 - 13s - loss: 0.8690 - val_loss: 1.5049\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.50429\n",
            "Epoch 55/100\n",
            "32/32 - 13s - loss: 0.8567 - val_loss: 1.5046\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.50429\n",
            "Epoch 56/100\n",
            "32/32 - 13s - loss: 0.8420 - val_loss: 1.5025\n",
            "\n",
            "Epoch 00056: val_loss improved from 1.50429 to 1.50249, saving model to model.h5\n",
            "Epoch 57/100\n",
            "32/32 - 13s - loss: 0.8298 - val_loss: 1.5035\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.50249\n",
            "Epoch 58/100\n",
            "32/32 - 13s - loss: 0.8206 - val_loss: 1.5083\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.50249\n",
            "Epoch 59/100\n",
            "32/32 - 13s - loss: 0.8071 - val_loss: 1.4949\n",
            "\n",
            "Epoch 00059: val_loss improved from 1.50249 to 1.49492, saving model to model.h5\n",
            "Epoch 60/100\n",
            "32/32 - 13s - loss: 0.7976 - val_loss: 1.4966\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.49492\n",
            "Epoch 61/100\n",
            "32/32 - 13s - loss: 0.7831 - val_loss: 1.4897\n",
            "\n",
            "Epoch 00061: val_loss improved from 1.49492 to 1.48969, saving model to model.h5\n",
            "Epoch 62/100\n",
            "32/32 - 14s - loss: 0.7656 - val_loss: 1.4931\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.48969\n",
            "Epoch 63/100\n",
            "32/32 - 13s - loss: 0.7505 - val_loss: 1.4909\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.48969\n",
            "Epoch 64/100\n",
            "32/32 - 14s - loss: 0.7359 - val_loss: 1.4742\n",
            "\n",
            "Epoch 00064: val_loss improved from 1.48969 to 1.47424, saving model to model.h5\n",
            "Epoch 65/100\n",
            "32/32 - 13s - loss: 0.7217 - val_loss: 1.4768\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.47424\n",
            "Epoch 66/100\n",
            "32/32 - 13s - loss: 0.7070 - val_loss: 1.4726\n",
            "\n",
            "Epoch 00066: val_loss improved from 1.47424 to 1.47258, saving model to model.h5\n",
            "Epoch 67/100\n",
            "32/32 - 13s - loss: 0.6910 - val_loss: 1.4676\n",
            "\n",
            "Epoch 00067: val_loss improved from 1.47258 to 1.46757, saving model to model.h5\n",
            "Epoch 68/100\n",
            "32/32 - 13s - loss: 0.6720 - val_loss: 1.4563\n",
            "\n",
            "Epoch 00068: val_loss improved from 1.46757 to 1.45634, saving model to model.h5\n",
            "Epoch 69/100\n",
            "32/32 - 13s - loss: 0.6543 - val_loss: 1.4537\n",
            "\n",
            "Epoch 00069: val_loss improved from 1.45634 to 1.45369, saving model to model.h5\n",
            "Epoch 70/100\n",
            "32/32 - 13s - loss: 0.6351 - val_loss: 1.4410\n",
            "\n",
            "Epoch 00070: val_loss improved from 1.45369 to 1.44104, saving model to model.h5\n",
            "Epoch 71/100\n",
            "32/32 - 13s - loss: 0.6190 - val_loss: 1.4470\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.44104\n",
            "Epoch 72/100\n",
            "32/32 - 13s - loss: 0.6058 - val_loss: 1.4487\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.44104\n",
            "Epoch 73/100\n",
            "32/32 - 13s - loss: 0.5902 - val_loss: 1.4401\n",
            "\n",
            "Epoch 00073: val_loss improved from 1.44104 to 1.44008, saving model to model.h5\n",
            "Epoch 74/100\n",
            "32/32 - 13s - loss: 0.5749 - val_loss: 1.4400\n",
            "\n",
            "Epoch 00074: val_loss improved from 1.44008 to 1.43996, saving model to model.h5\n",
            "Epoch 75/100\n",
            "32/32 - 13s - loss: 0.5589 - val_loss: 1.4255\n",
            "\n",
            "Epoch 00075: val_loss improved from 1.43996 to 1.42552, saving model to model.h5\n",
            "Epoch 76/100\n",
            "32/32 - 13s - loss: 0.5461 - val_loss: 1.4454\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.42552\n",
            "Epoch 77/100\n",
            "32/32 - 13s - loss: 0.5329 - val_loss: 1.4236\n",
            "\n",
            "Epoch 00077: val_loss improved from 1.42552 to 1.42360, saving model to model.h5\n",
            "Epoch 78/100\n",
            "32/32 - 13s - loss: 0.5148 - val_loss: 1.4164\n",
            "\n",
            "Epoch 00078: val_loss improved from 1.42360 to 1.41638, saving model to model.h5\n",
            "Epoch 79/100\n",
            "32/32 - 13s - loss: 0.4982 - val_loss: 1.4225\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.41638\n",
            "Epoch 80/100\n",
            "32/32 - 13s - loss: 0.4852 - val_loss: 1.4119\n",
            "\n",
            "Epoch 00080: val_loss improved from 1.41638 to 1.41193, saving model to model.h5\n",
            "Epoch 81/100\n",
            "32/32 - 13s - loss: 0.4696 - val_loss: 1.4196\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.41193\n",
            "Epoch 82/100\n",
            "32/32 - 13s - loss: 0.4547 - val_loss: 1.4097\n",
            "\n",
            "Epoch 00082: val_loss improved from 1.41193 to 1.40972, saving model to model.h5\n",
            "Epoch 83/100\n",
            "32/32 - 13s - loss: 0.4402 - val_loss: 1.4206\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.40972\n",
            "Epoch 84/100\n",
            "32/32 - 13s - loss: 0.4268 - val_loss: 1.3973\n",
            "\n",
            "Epoch 00084: val_loss improved from 1.40972 to 1.39726, saving model to model.h5\n",
            "Epoch 85/100\n",
            "32/32 - 13s - loss: 0.4151 - val_loss: 1.4015\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.39726\n",
            "Epoch 86/100\n",
            "32/32 - 13s - loss: 0.4006 - val_loss: 1.3992\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.39726\n",
            "Epoch 87/100\n",
            "32/32 - 13s - loss: 0.3857 - val_loss: 1.3993\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.39726\n",
            "Epoch 88/100\n",
            "32/32 - 13s - loss: 0.3696 - val_loss: 1.3922\n",
            "\n",
            "Epoch 00088: val_loss improved from 1.39726 to 1.39221, saving model to model.h5\n",
            "Epoch 89/100\n",
            "32/32 - 13s - loss: 0.3569 - val_loss: 1.3959\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.39221\n",
            "Epoch 90/100\n",
            "32/32 - 13s - loss: 0.3463 - val_loss: 1.4021\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.39221\n",
            "Epoch 91/100\n",
            "32/32 - 13s - loss: 0.3362 - val_loss: 1.3927\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.39221\n",
            "Epoch 92/100\n",
            "32/32 - 13s - loss: 0.3209 - val_loss: 1.3833\n",
            "\n",
            "Epoch 00092: val_loss improved from 1.39221 to 1.38331, saving model to model.h5\n",
            "Epoch 93/100\n",
            "32/32 - 13s - loss: 0.3075 - val_loss: 1.3925\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.38331\n",
            "Epoch 94/100\n",
            "32/32 - 13s - loss: 0.2994 - val_loss: 1.3872\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.38331\n",
            "Epoch 95/100\n",
            "32/32 - 13s - loss: 0.2901 - val_loss: 1.4025\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.38331\n",
            "Epoch 96/100\n",
            "32/32 - 13s - loss: 0.2772 - val_loss: 1.3951\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.38331\n",
            "Epoch 97/100\n",
            "32/32 - 13s - loss: 0.2693 - val_loss: 1.4032\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.38331\n",
            "Epoch 98/100\n",
            "32/32 - 13s - loss: 0.2549 - val_loss: 1.3889\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.38331\n",
            "Epoch 99/100\n",
            "32/32 - 13s - loss: 0.2425 - val_loss: 1.3982\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.38331\n",
            "Epoch 100/100\n",
            "32/32 - 13s - loss: 0.2339 - val_loss: 1.3924\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.38331\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d73172940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbC8LgAXROUm",
        "outputId": "df2866bf-9e58-459a-db3e-11cf7d0d8fee"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Thai Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Thai Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 512)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 1977\n",
            "English Max Length: 20\n",
            "Thai Vocabulary Size: 2971\n",
            "Thai Max Length: 6\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 6, 512)            1521152   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 20, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 20, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 20, 1977)          1014201   \n",
            "=================================================================\n",
            "Total params: 6,733,753\n",
            "Trainable params: 6,733,753\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "32/32 - 45s - loss: 3.4421 - val_loss: 1.9997\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.99967, saving model to model.h5\n",
            "Epoch 2/100\n",
            "32/32 - 39s - loss: 1.8990 - val_loss: 1.9408\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.99967 to 1.94077, saving model to model.h5\n",
            "Epoch 3/100\n",
            "32/32 - 39s - loss: 1.8596 - val_loss: 2.0715\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.94077\n",
            "Epoch 4/100\n",
            "32/32 - 39s - loss: 1.8883 - val_loss: 1.7455\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.94077 to 1.74551, saving model to model.h5\n",
            "Epoch 5/100\n",
            "32/32 - 39s - loss: 1.6886 - val_loss: 1.6930\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.74551 to 1.69296, saving model to model.h5\n",
            "Epoch 6/100\n",
            "32/32 - 39s - loss: 1.5993 - val_loss: 1.6529\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.69296 to 1.65290, saving model to model.h5\n",
            "Epoch 7/100\n",
            "32/32 - 39s - loss: 1.5353 - val_loss: 1.6214\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.65290 to 1.62137, saving model to model.h5\n",
            "Epoch 8/100\n",
            "32/32 - 39s - loss: 1.4767 - val_loss: 1.5866\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.62137 to 1.58656, saving model to model.h5\n",
            "Epoch 9/100\n",
            "32/32 - 39s - loss: 1.4169 - val_loss: 1.5536\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.58656 to 1.55359, saving model to model.h5\n",
            "Epoch 10/100\n",
            "32/32 - 39s - loss: 1.3603 - val_loss: 1.5286\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.55359 to 1.52862, saving model to model.h5\n",
            "Epoch 11/100\n",
            "32/32 - 39s - loss: 1.3346 - val_loss: 1.5121\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.52862 to 1.51214, saving model to model.h5\n",
            "Epoch 12/100\n",
            "32/32 - 39s - loss: 1.2947 - val_loss: 1.5120\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.51214 to 1.51199, saving model to model.h5\n",
            "Epoch 13/100\n",
            "32/32 - 39s - loss: 1.2666 - val_loss: 1.4998\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.51199 to 1.49983, saving model to model.h5\n",
            "Epoch 14/100\n",
            "32/32 - 39s - loss: 1.2462 - val_loss: 1.4981\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.49983 to 1.49811, saving model to model.h5\n",
            "Epoch 15/100\n",
            "32/32 - 39s - loss: 1.2286 - val_loss: 1.4981\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.49811\n",
            "Epoch 16/100\n",
            "32/32 - 39s - loss: 1.2196 - val_loss: 1.4965\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.49811 to 1.49651, saving model to model.h5\n",
            "Epoch 17/100\n",
            "32/32 - 39s - loss: 1.2073 - val_loss: 1.4973\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.49651\n",
            "Epoch 18/100\n",
            "32/32 - 38s - loss: 1.1983 - val_loss: 1.5038\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.49651\n",
            "Epoch 19/100\n",
            "32/32 - 39s - loss: 1.1840 - val_loss: 1.4915\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.49651 to 1.49150, saving model to model.h5\n",
            "Epoch 20/100\n",
            "32/32 - 39s - loss: 1.1658 - val_loss: 1.4923\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.49150\n",
            "Epoch 21/100\n",
            "32/32 - 39s - loss: 1.1487 - val_loss: 1.4936\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.49150\n",
            "Epoch 22/100\n",
            "32/32 - 39s - loss: 1.1351 - val_loss: 1.4956\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.49150\n",
            "Epoch 23/100\n",
            "32/32 - 39s - loss: 1.1249 - val_loss: 1.4965\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.49150\n",
            "Epoch 24/100\n",
            "32/32 - 39s - loss: 1.1139 - val_loss: 1.5081\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.49150\n",
            "Epoch 25/100\n",
            "32/32 - 38s - loss: 1.1023 - val_loss: 1.5003\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.49150\n",
            "Epoch 26/100\n",
            "32/32 - 39s - loss: 1.0889 - val_loss: 1.4967\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.49150\n",
            "Epoch 27/100\n",
            "32/32 - 38s - loss: 1.0761 - val_loss: 1.5103\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.49150\n",
            "Epoch 28/100\n",
            "32/32 - 39s - loss: 1.0640 - val_loss: 1.5043\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.49150\n",
            "Epoch 29/100\n",
            "32/32 - 39s - loss: 1.0481 - val_loss: 1.4989\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.49150\n",
            "Epoch 30/100\n",
            "32/32 - 39s - loss: 1.0345 - val_loss: 1.5062\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.49150\n",
            "Epoch 31/100\n",
            "32/32 - 39s - loss: 1.0225 - val_loss: 1.5086\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.49150\n",
            "Epoch 32/100\n",
            "32/32 - 39s - loss: 1.0009 - val_loss: 1.5042\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.49150\n",
            "Epoch 33/100\n",
            "32/32 - 38s - loss: 0.9863 - val_loss: 1.5116\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.49150\n",
            "Epoch 34/100\n",
            "32/32 - 39s - loss: 0.9709 - val_loss: 1.5088\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.49150\n",
            "Epoch 35/100\n",
            "32/32 - 39s - loss: 0.9547 - val_loss: 1.5027\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.49150\n",
            "Epoch 36/100\n",
            "32/32 - 39s - loss: 0.9394 - val_loss: 1.5010\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.49150\n",
            "Epoch 37/100\n",
            "32/32 - 39s - loss: 0.9217 - val_loss: 1.4922\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.49150\n",
            "Epoch 38/100\n",
            "32/32 - 39s - loss: 0.8970 - val_loss: 1.4923\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.49150\n",
            "Epoch 39/100\n",
            "32/32 - 39s - loss: 0.8733 - val_loss: 1.4872\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.49150 to 1.48720, saving model to model.h5\n",
            "Epoch 40/100\n",
            "32/32 - 39s - loss: 0.8501 - val_loss: 1.4811\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.48720 to 1.48111, saving model to model.h5\n",
            "Epoch 41/100\n",
            "32/32 - 38s - loss: 0.8274 - val_loss: 1.4813\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.48111\n",
            "Epoch 42/100\n",
            "32/32 - 39s - loss: 0.8058 - val_loss: 1.4756\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.48111 to 1.47564, saving model to model.h5\n",
            "Epoch 43/100\n",
            "32/32 - 39s - loss: 0.7815 - val_loss: 1.4710\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.47564 to 1.47103, saving model to model.h5\n",
            "Epoch 44/100\n",
            "32/32 - 42s - loss: 0.7574 - val_loss: 1.4615\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.47103 to 1.46147, saving model to model.h5\n",
            "Epoch 45/100\n",
            "32/32 - 39s - loss: 0.7343 - val_loss: 1.4679\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.46147\n",
            "Epoch 46/100\n",
            "32/32 - 39s - loss: 0.7145 - val_loss: 1.4625\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.46147\n",
            "Epoch 47/100\n",
            "32/32 - 38s - loss: 0.6928 - val_loss: 1.4646\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.46147\n",
            "Epoch 48/100\n",
            "32/32 - 38s - loss: 0.6723 - val_loss: 1.4571\n",
            "\n",
            "Epoch 00048: val_loss improved from 1.46147 to 1.45708, saving model to model.h5\n",
            "Epoch 49/100\n",
            "32/32 - 39s - loss: 0.6554 - val_loss: 1.4612\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.45708\n",
            "Epoch 50/100\n",
            "32/32 - 39s - loss: 0.6412 - val_loss: 1.4430\n",
            "\n",
            "Epoch 00050: val_loss improved from 1.45708 to 1.44303, saving model to model.h5\n",
            "Epoch 51/100\n",
            "32/32 - 38s - loss: 0.6101 - val_loss: 1.4380\n",
            "\n",
            "Epoch 00051: val_loss improved from 1.44303 to 1.43800, saving model to model.h5\n",
            "Epoch 52/100\n",
            "32/32 - 39s - loss: 0.5823 - val_loss: 1.4333\n",
            "\n",
            "Epoch 00052: val_loss improved from 1.43800 to 1.43326, saving model to model.h5\n",
            "Epoch 53/100\n",
            "32/32 - 39s - loss: 0.5595 - val_loss: 1.4325\n",
            "\n",
            "Epoch 00053: val_loss improved from 1.43326 to 1.43245, saving model to model.h5\n",
            "Epoch 54/100\n",
            "32/32 - 39s - loss: 0.5376 - val_loss: 1.4243\n",
            "\n",
            "Epoch 00054: val_loss improved from 1.43245 to 1.42430, saving model to model.h5\n",
            "Epoch 55/100\n",
            "32/32 - 39s - loss: 0.5178 - val_loss: 1.4254\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.42430\n",
            "Epoch 56/100\n",
            "32/32 - 39s - loss: 0.4951 - val_loss: 1.4165\n",
            "\n",
            "Epoch 00056: val_loss improved from 1.42430 to 1.41651, saving model to model.h5\n",
            "Epoch 57/100\n",
            "32/32 - 39s - loss: 0.4739 - val_loss: 1.4213\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.41651\n",
            "Epoch 58/100\n",
            "32/32 - 39s - loss: 0.4525 - val_loss: 1.4121\n",
            "\n",
            "Epoch 00058: val_loss improved from 1.41651 to 1.41213, saving model to model.h5\n",
            "Epoch 59/100\n",
            "32/32 - 42s - loss: 0.4301 - val_loss: 1.4054\n",
            "\n",
            "Epoch 00059: val_loss improved from 1.41213 to 1.40544, saving model to model.h5\n",
            "Epoch 60/100\n",
            "32/32 - 39s - loss: 0.4123 - val_loss: 1.4100\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.40544\n",
            "Epoch 61/100\n",
            "32/32 - 39s - loss: 0.3951 - val_loss: 1.4055\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.40544\n",
            "Epoch 62/100\n",
            "32/32 - 39s - loss: 0.3736 - val_loss: 1.3949\n",
            "\n",
            "Epoch 00062: val_loss improved from 1.40544 to 1.39487, saving model to model.h5\n",
            "Epoch 63/100\n",
            "32/32 - 39s - loss: 0.3500 - val_loss: 1.3997\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.39487\n",
            "Epoch 64/100\n",
            "32/32 - 40s - loss: 0.3304 - val_loss: 1.3965\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.39487\n",
            "Epoch 65/100\n",
            "32/32 - 40s - loss: 0.3120 - val_loss: 1.3986\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.39487\n",
            "Epoch 66/100\n",
            "32/32 - 39s - loss: 0.2926 - val_loss: 1.3917\n",
            "\n",
            "Epoch 00066: val_loss improved from 1.39487 to 1.39167, saving model to model.h5\n",
            "Epoch 67/100\n",
            "32/32 - 39s - loss: 0.2787 - val_loss: 1.3953\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.39167\n",
            "Epoch 68/100\n",
            "32/32 - 39s - loss: 0.2660 - val_loss: 1.3944\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.39167\n",
            "Epoch 69/100\n",
            "32/32 - 39s - loss: 0.2517 - val_loss: 1.3925\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.39167\n",
            "Epoch 70/100\n",
            "32/32 - 39s - loss: 0.2359 - val_loss: 1.3906\n",
            "\n",
            "Epoch 00070: val_loss improved from 1.39167 to 1.39064, saving model to model.h5\n",
            "Epoch 71/100\n",
            "32/32 - 39s - loss: 0.2223 - val_loss: 1.3919\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.39064\n",
            "Epoch 72/100\n",
            "32/32 - 39s - loss: 0.2075 - val_loss: 1.3935\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.39064\n",
            "Epoch 73/100\n",
            "32/32 - 39s - loss: 0.1946 - val_loss: 1.3994\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.39064\n",
            "Epoch 74/100\n",
            "32/32 - 39s - loss: 0.1850 - val_loss: 1.3944\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.39064\n",
            "Epoch 75/100\n",
            "32/32 - 41s - loss: 0.1753 - val_loss: 1.3959\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.39064\n",
            "Epoch 76/100\n",
            "32/32 - 39s - loss: 0.1659 - val_loss: 1.3946\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.39064\n",
            "Epoch 77/100\n",
            "32/32 - 39s - loss: 0.1578 - val_loss: 1.3960\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.39064\n",
            "Epoch 78/100\n",
            "32/32 - 39s - loss: 0.1452 - val_loss: 1.3917\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.39064\n",
            "Epoch 79/100\n",
            "32/32 - 39s - loss: 0.1340 - val_loss: 1.3947\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.39064\n",
            "Epoch 80/100\n",
            "32/32 - 40s - loss: 0.1261 - val_loss: 1.3968\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.39064\n",
            "Epoch 81/100\n",
            "32/32 - 39s - loss: 0.1162 - val_loss: 1.3972\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.39064\n",
            "Epoch 82/100\n",
            "32/32 - 39s - loss: 0.1083 - val_loss: 1.3964\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.39064\n",
            "Epoch 83/100\n",
            "32/32 - 39s - loss: 0.1004 - val_loss: 1.4020\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.39064\n",
            "Epoch 84/100\n",
            "32/32 - 39s - loss: 0.0962 - val_loss: 1.4057\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.39064\n",
            "Epoch 85/100\n",
            "32/32 - 39s - loss: 0.0890 - val_loss: 1.4068\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.39064\n",
            "Epoch 86/100\n",
            "32/32 - 38s - loss: 0.0847 - val_loss: 1.4104\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.39064\n",
            "Epoch 87/100\n",
            "32/32 - 39s - loss: 0.0788 - val_loss: 1.4088\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.39064\n",
            "Epoch 88/100\n",
            "32/32 - 38s - loss: 0.0729 - val_loss: 1.4100\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.39064\n",
            "Epoch 89/100\n",
            "32/32 - 39s - loss: 0.0671 - val_loss: 1.4122\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.39064\n",
            "Epoch 90/100\n",
            "32/32 - 41s - loss: 0.0639 - val_loss: 1.4139\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.39064\n",
            "Epoch 91/100\n",
            "32/32 - 39s - loss: 0.0621 - val_loss: 1.4170\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.39064\n",
            "Epoch 92/100\n",
            "32/32 - 39s - loss: 0.0589 - val_loss: 1.4179\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.39064\n",
            "Epoch 93/100\n",
            "32/32 - 39s - loss: 0.0563 - val_loss: 1.4237\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.39064\n",
            "Epoch 94/100\n",
            "32/32 - 39s - loss: 0.0536 - val_loss: 1.4256\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.39064\n",
            "Epoch 95/100\n",
            "32/32 - 39s - loss: 0.0492 - val_loss: 1.4224\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.39064\n",
            "Epoch 96/100\n",
            "32/32 - 38s - loss: 0.0447 - val_loss: 1.4243\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.39064\n",
            "Epoch 97/100\n",
            "32/32 - 38s - loss: 0.0434 - val_loss: 1.4287\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.39064\n",
            "Epoch 98/100\n",
            "32/32 - 38s - loss: 0.0413 - val_loss: 1.4275\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.39064\n",
            "Epoch 99/100\n",
            "32/32 - 39s - loss: 0.0383 - val_loss: 1.4339\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.39064\n",
            "Epoch 100/100\n",
            "32/32 - 39s - loss: 0.0361 - val_loss: 1.4365\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.39064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d60c87c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5KNnJ7CyiQA",
        "outputId": "2e956bae-9389-427f-fc7d-03d307c3f6c8"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Thai Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Thai Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 128)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 1977\n",
            "English Max Length: 20\n",
            "Thai Vocabulary Size: 2971\n",
            "Thai Max Length: 6\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 6, 128)            380288    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 20, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 20, 128)           131584    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 20, 1977)          255033    \n",
            "=================================================================\n",
            "Total params: 898,489\n",
            "Trainable params: 898,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "32/32 - 14s - loss: 5.5859 - val_loss: 2.0369\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.03685, saving model to model.h5\n",
            "Epoch 2/100\n",
            "32/32 - 7s - loss: 1.9046 - val_loss: 1.8414\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.03685 to 1.84136, saving model to model.h5\n",
            "Epoch 3/100\n",
            "32/32 - 7s - loss: 1.8212 - val_loss: 1.8045\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.84136 to 1.80455, saving model to model.h5\n",
            "Epoch 4/100\n",
            "32/32 - 7s - loss: 1.7770 - val_loss: 1.8613\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.80455\n",
            "Epoch 5/100\n",
            "32/32 - 7s - loss: 1.7758 - val_loss: 1.7714\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.80455 to 1.77141, saving model to model.h5\n",
            "Epoch 6/100\n",
            "32/32 - 7s - loss: 1.7198 - val_loss: 1.7637\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.77141 to 1.76369, saving model to model.h5\n",
            "Epoch 7/100\n",
            "32/32 - 7s - loss: 1.7006 - val_loss: 1.7401\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.76369 to 1.74013, saving model to model.h5\n",
            "Epoch 8/100\n",
            "32/32 - 7s - loss: 1.6748 - val_loss: 1.7254\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.74013 to 1.72540, saving model to model.h5\n",
            "Epoch 9/100\n",
            "32/32 - 7s - loss: 1.6464 - val_loss: 1.7141\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.72540 to 1.71409, saving model to model.h5\n",
            "Epoch 10/100\n",
            "32/32 - 7s - loss: 1.6330 - val_loss: 1.6977\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.71409 to 1.69766, saving model to model.h5\n",
            "Epoch 11/100\n",
            "32/32 - 7s - loss: 1.6147 - val_loss: 1.6886\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.69766 to 1.68863, saving model to model.h5\n",
            "Epoch 12/100\n",
            "32/32 - 7s - loss: 1.6010 - val_loss: 1.6758\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.68863 to 1.67580, saving model to model.h5\n",
            "Epoch 13/100\n",
            "32/32 - 7s - loss: 1.5870 - val_loss: 1.6656\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.67580 to 1.66562, saving model to model.h5\n",
            "Epoch 14/100\n",
            "32/32 - 7s - loss: 1.5723 - val_loss: 1.6515\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.66562 to 1.65148, saving model to model.h5\n",
            "Epoch 15/100\n",
            "32/32 - 7s - loss: 1.5543 - val_loss: 1.6356\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.65148 to 1.63559, saving model to model.h5\n",
            "Epoch 16/100\n",
            "32/32 - 7s - loss: 1.5335 - val_loss: 1.6179\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.63559 to 1.61789, saving model to model.h5\n",
            "Epoch 17/100\n",
            "32/32 - 7s - loss: 1.5073 - val_loss: 1.6001\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.61789 to 1.60012, saving model to model.h5\n",
            "Epoch 18/100\n",
            "32/32 - 7s - loss: 1.4758 - val_loss: 1.5895\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.60012 to 1.58949, saving model to model.h5\n",
            "Epoch 19/100\n",
            "32/32 - 7s - loss: 1.4428 - val_loss: 1.5736\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.58949 to 1.57357, saving model to model.h5\n",
            "Epoch 20/100\n",
            "32/32 - 7s - loss: 1.4102 - val_loss: 1.5515\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.57357 to 1.55147, saving model to model.h5\n",
            "Epoch 21/100\n",
            "32/32 - 7s - loss: 1.3801 - val_loss: 1.5357\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.55147 to 1.53567, saving model to model.h5\n",
            "Epoch 22/100\n",
            "32/32 - 7s - loss: 1.3469 - val_loss: 1.5268\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.53567 to 1.52681, saving model to model.h5\n",
            "Epoch 23/100\n",
            "32/32 - 7s - loss: 1.3197 - val_loss: 1.5173\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.52681 to 1.51725, saving model to model.h5\n",
            "Epoch 24/100\n",
            "32/32 - 7s - loss: 1.3014 - val_loss: 1.5265\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.51725\n",
            "Epoch 25/100\n",
            "32/32 - 7s - loss: 1.2876 - val_loss: 1.5049\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.51725 to 1.50490, saving model to model.h5\n",
            "Epoch 26/100\n",
            "32/32 - 7s - loss: 1.2739 - val_loss: 1.5138\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.50490\n",
            "Epoch 27/100\n",
            "32/32 - 7s - loss: 1.2582 - val_loss: 1.5005\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.50490 to 1.50053, saving model to model.h5\n",
            "Epoch 28/100\n",
            "32/32 - 7s - loss: 1.2540 - val_loss: 1.5154\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.50053\n",
            "Epoch 29/100\n",
            "32/32 - 7s - loss: 1.2433 - val_loss: 1.5076\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.50053\n",
            "Epoch 30/100\n",
            "32/32 - 7s - loss: 1.2242 - val_loss: 1.5101\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.50053\n",
            "Epoch 31/100\n",
            "32/32 - 7s - loss: 1.2120 - val_loss: 1.5016\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.50053\n",
            "Epoch 32/100\n",
            "32/32 - 7s - loss: 1.2013 - val_loss: 1.5085\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.50053\n",
            "Epoch 33/100\n",
            "32/32 - 7s - loss: 1.1915 - val_loss: 1.5014\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.50053\n",
            "Epoch 34/100\n",
            "32/32 - 9s - loss: 1.1833 - val_loss: 1.5062\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.50053\n",
            "Epoch 35/100\n",
            "32/32 - 7s - loss: 1.1762 - val_loss: 1.5084\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.50053\n",
            "Epoch 36/100\n",
            "32/32 - 7s - loss: 1.1678 - val_loss: 1.5244\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.50053\n",
            "Epoch 37/100\n",
            "32/32 - 7s - loss: 1.1577 - val_loss: 1.5103\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.50053\n",
            "Epoch 38/100\n",
            "32/32 - 7s - loss: 1.1477 - val_loss: 1.5061\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.50053\n",
            "Epoch 39/100\n",
            "32/32 - 7s - loss: 1.1376 - val_loss: 1.5124\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.50053\n",
            "Epoch 40/100\n",
            "32/32 - 7s - loss: 1.1296 - val_loss: 1.5150\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.50053\n",
            "Epoch 41/100\n",
            "32/32 - 7s - loss: 1.1223 - val_loss: 1.5182\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.50053\n",
            "Epoch 42/100\n",
            "32/32 - 7s - loss: 1.1145 - val_loss: 1.5256\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.50053\n",
            "Epoch 43/100\n",
            "32/32 - 7s - loss: 1.1056 - val_loss: 1.5178\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.50053\n",
            "Epoch 44/100\n",
            "32/32 - 7s - loss: 1.0963 - val_loss: 1.5302\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.50053\n",
            "Epoch 45/100\n",
            "32/32 - 7s - loss: 1.0895 - val_loss: 1.5191\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.50053\n",
            "Epoch 46/100\n",
            "32/32 - 7s - loss: 1.0810 - val_loss: 1.5274\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.50053\n",
            "Epoch 47/100\n",
            "32/32 - 7s - loss: 1.0692 - val_loss: 1.5208\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.50053\n",
            "Epoch 48/100\n",
            "32/32 - 7s - loss: 1.0601 - val_loss: 1.5248\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.50053\n",
            "Epoch 49/100\n",
            "32/32 - 7s - loss: 1.0501 - val_loss: 1.5252\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.50053\n",
            "Epoch 50/100\n",
            "32/32 - 7s - loss: 1.0408 - val_loss: 1.5285\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.50053\n",
            "Epoch 51/100\n",
            "32/32 - 7s - loss: 1.0323 - val_loss: 1.5253\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.50053\n",
            "Epoch 52/100\n",
            "32/32 - 7s - loss: 1.0250 - val_loss: 1.5325\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.50053\n",
            "Epoch 53/100\n",
            "32/32 - 7s - loss: 1.0189 - val_loss: 1.5539\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.50053\n",
            "Epoch 54/100\n",
            "32/32 - 7s - loss: 1.0123 - val_loss: 1.5326\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.50053\n",
            "Epoch 55/100\n",
            "32/32 - 7s - loss: 1.0038 - val_loss: 1.5291\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.50053\n",
            "Epoch 56/100\n",
            "32/32 - 7s - loss: 0.9954 - val_loss: 1.5244\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.50053\n",
            "Epoch 57/100\n",
            "32/32 - 7s - loss: 0.9866 - val_loss: 1.5473\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.50053\n",
            "Epoch 58/100\n",
            "32/32 - 7s - loss: 0.9769 - val_loss: 1.5387\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.50053\n",
            "Epoch 59/100\n",
            "32/32 - 7s - loss: 0.9665 - val_loss: 1.5213\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.50053\n",
            "Epoch 60/100\n",
            "32/32 - 7s - loss: 0.9545 - val_loss: 1.5337\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.50053\n",
            "Epoch 61/100\n",
            "32/32 - 7s - loss: 0.9451 - val_loss: 1.5305\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.50053\n",
            "Epoch 62/100\n",
            "32/32 - 7s - loss: 0.9365 - val_loss: 1.5409\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.50053\n",
            "Epoch 63/100\n",
            "32/32 - 7s - loss: 0.9268 - val_loss: 1.5201\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.50053\n",
            "Epoch 64/100\n",
            "32/32 - 7s - loss: 0.9162 - val_loss: 1.5248\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.50053\n",
            "Epoch 65/100\n",
            "32/32 - 7s - loss: 0.9055 - val_loss: 1.5210\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.50053\n",
            "Epoch 66/100\n",
            "32/32 - 7s - loss: 0.8953 - val_loss: 1.5187\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.50053\n",
            "Epoch 67/100\n",
            "32/32 - 7s - loss: 0.8852 - val_loss: 1.5256\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.50053\n",
            "Epoch 68/100\n",
            "32/32 - 7s - loss: 0.8741 - val_loss: 1.5063\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.50053\n",
            "Epoch 69/100\n",
            "32/32 - 7s - loss: 0.8640 - val_loss: 1.5137\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.50053\n",
            "Epoch 70/100\n",
            "32/32 - 7s - loss: 0.8555 - val_loss: 1.5267\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.50053\n",
            "Epoch 71/100\n",
            "32/32 - 7s - loss: 0.8451 - val_loss: 1.5032\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.50053\n",
            "Epoch 72/100\n",
            "32/32 - 7s - loss: 0.8348 - val_loss: 1.5096\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.50053\n",
            "Epoch 73/100\n",
            "32/32 - 7s - loss: 0.8231 - val_loss: 1.5177\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.50053\n",
            "Epoch 74/100\n",
            "32/32 - 7s - loss: 0.8162 - val_loss: 1.5259\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.50053\n",
            "Epoch 75/100\n",
            "32/32 - 7s - loss: 0.8078 - val_loss: 1.5069\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.50053\n",
            "Epoch 76/100\n",
            "32/32 - 7s - loss: 0.7944 - val_loss: 1.5103\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.50053\n",
            "Epoch 77/100\n",
            "32/32 - 7s - loss: 0.7833 - val_loss: 1.5209\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.50053\n",
            "Epoch 78/100\n",
            "32/32 - 7s - loss: 0.7735 - val_loss: 1.5167\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.50053\n",
            "Epoch 79/100\n",
            "32/32 - 7s - loss: 0.7599 - val_loss: 1.4879\n",
            "\n",
            "Epoch 00079: val_loss improved from 1.50053 to 1.48787, saving model to model.h5\n",
            "Epoch 80/100\n",
            "32/32 - 7s - loss: 0.7471 - val_loss: 1.4923\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.48787\n",
            "Epoch 81/100\n",
            "32/32 - 7s - loss: 0.7349 - val_loss: 1.5027\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.48787\n",
            "Epoch 82/100\n",
            "32/32 - 7s - loss: 0.7230 - val_loss: 1.4951\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.48787\n",
            "Epoch 83/100\n",
            "32/32 - 7s - loss: 0.7102 - val_loss: 1.4989\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.48787\n",
            "Epoch 84/100\n",
            "32/32 - 7s - loss: 0.7007 - val_loss: 1.4911\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.48787\n",
            "Epoch 85/100\n",
            "32/32 - 7s - loss: 0.6917 - val_loss: 1.4996\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.48787\n",
            "Epoch 86/100\n",
            "32/32 - 7s - loss: 0.6832 - val_loss: 1.5045\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.48787\n",
            "Epoch 87/100\n",
            "32/32 - 7s - loss: 0.6712 - val_loss: 1.4963\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.48787\n",
            "Epoch 88/100\n",
            "32/32 - 7s - loss: 0.6590 - val_loss: 1.4884\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.48787\n",
            "Epoch 89/100\n",
            "32/32 - 7s - loss: 0.6507 - val_loss: 1.4822\n",
            "\n",
            "Epoch 00089: val_loss improved from 1.48787 to 1.48216, saving model to model.h5\n",
            "Epoch 90/100\n",
            "32/32 - 7s - loss: 0.6418 - val_loss: 1.4953\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.48216\n",
            "Epoch 91/100\n",
            "32/32 - 7s - loss: 0.6317 - val_loss: 1.4811\n",
            "\n",
            "Epoch 00091: val_loss improved from 1.48216 to 1.48108, saving model to model.h5\n",
            "Epoch 92/100\n",
            "32/32 - 7s - loss: 0.6195 - val_loss: 1.4971\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.48108\n",
            "Epoch 93/100\n",
            "32/32 - 7s - loss: 0.6093 - val_loss: 1.4909\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.48108\n",
            "Epoch 94/100\n",
            "32/32 - 7s - loss: 0.6018 - val_loss: 1.4907\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.48108\n",
            "Epoch 95/100\n",
            "32/32 - 7s - loss: 0.5952 - val_loss: 1.4832\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.48108\n",
            "Epoch 96/100\n",
            "32/32 - 7s - loss: 0.5862 - val_loss: 1.4746\n",
            "\n",
            "Epoch 00096: val_loss improved from 1.48108 to 1.47460, saving model to model.h5\n",
            "Epoch 97/100\n",
            "32/32 - 7s - loss: 0.5732 - val_loss: 1.5046\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.47460\n",
            "Epoch 98/100\n",
            "32/32 - 7s - loss: 0.5630 - val_loss: 1.4877\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.47460\n",
            "Epoch 99/100\n",
            "32/32 - 7s - loss: 0.5530 - val_loss: 1.4865\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.47460\n",
            "Epoch 100/100\n",
            "32/32 - 7s - loss: 0.5420 - val_loss: 1.4820\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.47460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d5eba0a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CclrQhRZ2hUp",
        "outputId": "ea089652-48e1-47b9-8384-aa9bf5163654"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Thai Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Thai Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 2048)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 1977\n",
            "English Max Length: 20\n",
            "Thai Vocabulary Size: 2971\n",
            "Thai Max Length: 6\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 6, 2048)           6084608   \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 2048)              33562624  \n",
            "_________________________________________________________________\n",
            "repeat_vector_3 (RepeatVecto (None, 20, 2048)          0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 20, 2048)          33562624  \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 20, 1977)          4050873   \n",
            "=================================================================\n",
            "Total params: 77,260,729\n",
            "Trainable params: 77,260,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "32/32 - 599s - loss: 2.9676 - val_loss: 2.4212\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.42125, saving model to model.h5\n",
            "Epoch 2/100\n",
            "32/32 - 591s - loss: 2.0923 - val_loss: 1.9396\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.42125 to 1.93963, saving model to model.h5\n",
            "Epoch 3/100\n",
            "32/32 - 588s - loss: 1.7746 - val_loss: 1.7375\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.93963 to 1.73755, saving model to model.h5\n",
            "Epoch 4/100\n",
            "32/32 - 590s - loss: 1.6215 - val_loss: 1.6590\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.73755 to 1.65902, saving model to model.h5\n",
            "Epoch 5/100\n",
            "32/32 - 589s - loss: 1.5069 - val_loss: 1.5993\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.65902 to 1.59929, saving model to model.h5\n",
            "Epoch 6/100\n",
            "32/32 - 592s - loss: 1.4098 - val_loss: 1.5751\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.59929 to 1.57514, saving model to model.h5\n",
            "Epoch 7/100\n",
            "32/32 - 590s - loss: 1.3333 - val_loss: 1.5578\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.57514 to 1.55784, saving model to model.h5\n",
            "Epoch 8/100\n",
            "32/32 - 589s - loss: 1.2849 - val_loss: 1.5457\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.55784 to 1.54573, saving model to model.h5\n",
            "Epoch 9/100\n",
            "32/32 - 593s - loss: 1.2431 - val_loss: 1.5399\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.54573 to 1.53986, saving model to model.h5\n",
            "Epoch 10/100\n",
            "32/32 - 591s - loss: 1.2128 - val_loss: 1.5264\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.53986 to 1.52642, saving model to model.h5\n",
            "Epoch 11/100\n",
            "32/32 - 589s - loss: 1.1756 - val_loss: 1.5084\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.52642 to 1.50844, saving model to model.h5\n",
            "Epoch 12/100\n",
            "32/32 - 595s - loss: 1.1443 - val_loss: 1.4952\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.50844 to 1.49521, saving model to model.h5\n",
            "Epoch 13/100\n",
            "32/32 - 590s - loss: 1.1100 - val_loss: 1.4834\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.49521 to 1.48341, saving model to model.h5\n",
            "Epoch 14/100\n",
            "32/32 - 591s - loss: 1.0747 - val_loss: 1.4720\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.48341 to 1.47205, saving model to model.h5\n",
            "Epoch 15/100\n",
            "32/32 - 591s - loss: 1.0431 - val_loss: 1.4641\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.47205 to 1.46415, saving model to model.h5\n",
            "Epoch 16/100\n",
            "32/32 - 594s - loss: 1.0139 - val_loss: 1.4594\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.46415 to 1.45937, saving model to model.h5\n",
            "Epoch 17/100\n",
            "32/32 - 589s - loss: 0.9834 - val_loss: 1.4493\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.45937 to 1.44934, saving model to model.h5\n",
            "Epoch 18/100\n",
            "32/32 - 590s - loss: 0.9654 - val_loss: 1.4328\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.44934 to 1.43278, saving model to model.h5\n",
            "Epoch 19/100\n",
            "32/32 - 598s - loss: 0.9375 - val_loss: 1.4333\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.43278\n",
            "Epoch 20/100\n",
            "32/32 - 588s - loss: 0.9124 - val_loss: 1.4261\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.43278 to 1.42611, saving model to model.h5\n",
            "Epoch 21/100\n",
            "32/32 - 590s - loss: 0.8881 - val_loss: 1.4243\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.42611 to 1.42432, saving model to model.h5\n",
            "Epoch 22/100\n",
            "32/32 - 595s - loss: 0.8734 - val_loss: 1.4172\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.42432 to 1.41718, saving model to model.h5\n",
            "Epoch 23/100\n",
            "32/32 - 592s - loss: 0.8456 - val_loss: 1.4147\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.41718 to 1.41466, saving model to model.h5\n",
            "Epoch 24/100\n",
            "32/32 - 597s - loss: 0.8235 - val_loss: 1.4044\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.41466 to 1.40441, saving model to model.h5\n",
            "Epoch 25/100\n",
            "32/32 - 591s - loss: 0.7977 - val_loss: 1.4004\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.40441 to 1.40038, saving model to model.h5\n",
            "Epoch 26/100\n",
            "32/32 - 588s - loss: 0.7682 - val_loss: 1.3887\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.40038 to 1.38871, saving model to model.h5\n",
            "Epoch 27/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-026330d1e5f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr6YfXKc7Yqi",
        "outputId": "6c311a0f-697b-4564-bdb4-b1d5eabab9a2"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณเกิดที่นี่ไหม], target=[Were you born here], predicted=[you you born here]\n",
            "src=[ใครสน], target=[Who cares], predicted=[who cares]\n",
            "src=[ฉันกังวล], target=[Im worried], predicted=[im worried]\n",
            "src=[ปัญหาคือคุณยังเด็กเกินไป], target=[The trouble is that you are too young], predicted=[the trouble is are are are too young]\n",
            "src=[ทอมไม่มา], target=[Tom isnt coming], predicted=[tom isnt coming]\n",
            "src=[เราจะทำอย่างนั้นได้อย่างไร], target=[How do we do that], predicted=[how do we do that]\n",
            "src=[ฉันนอนหลับดีมากเมื่อคืน], target=[I slept very well last night], predicted=[i slept very well last night]\n",
            "src=[เธอปฏิเสธข้อเสนอของเขา], target=[She refused his offer], predicted=[she never his offer]\n",
            "src=[ฉันได้ยินเด็ก ๆ ร้องเพลงด้วยกัน], target=[I heard the children singing together], predicted=[i heard the children singing together]\n",
            "src=[ฉันกำลังต้มน้ำ], target=[I am boiling water], predicted=[i am boiling today]\n",
            "BLEU-1: 0.569653\n",
            "BLEU-2: 0.463111\n",
            "BLEU-3: 0.399564\n",
            "BLEU-4: 0.260862\n",
            "test\n",
            "src=[ฉันรอทอม], target=[I waited for Tom], predicted=[i waited this tom]\n",
            "src=[พวกนี้ไม่ใช่หนังสือของคุณเหรอ], target=[Arent these your books], predicted=[your your your books]\n",
            "src=[หมายความว่าอะไร], target=[What does that mean], predicted=[what does that mean]\n",
            "src=[ตื่น], target=[Wake up], predicted=[wake up]\n",
            "src=[ฉันไม่ใช่คนดัง], target=[Im not a celebrity], predicted=[im not a celebrity]\n",
            "src=[ทำไมคุณถึงเรียนภาษาฝรั่งเศส], target=[Why are you studying French], predicted=[you are you studying french]\n",
            "src=[ฉันพอใจกับงานของฉัน], target=[Im content with my job], predicted=[im satisfied with my work]\n",
            "src=[ฉันชอบเพลงอาร์แอนด์บี], target=[I like R  B], predicted=[i like r b]\n",
            "src=[ฉันเป็นผู้หญิง], target=[Im a woman], predicted=[im a woman]\n",
            "src=[คุณหมายถึงอะไร], target=[What do you mean], predicted=[what do you mean]\n",
            "BLEU-1: 0.316375\n",
            "BLEU-2: 0.248595\n",
            "BLEU-3: 0.223346\n",
            "BLEU-4: 0.132261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB1GAlrsxVry",
        "outputId": "9348030b-dd02-4665-a34f-5ea203bd4d47"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณเกิดที่นี่ไหม], target=[Were you born here], predicted=[were you born here]\n",
            "src=[ใครสน], target=[Who cares], predicted=[who cares]\n",
            "src=[ฉันกังวล], target=[Im worried], predicted=[im worried]\n",
            "src=[ปัญหาคือคุณยังเด็กเกินไป], target=[The trouble is that you are too young], predicted=[the trouble the that are is too]\n",
            "src=[ทอมไม่มา], target=[Tom isnt coming], predicted=[tom isnt coming]\n",
            "src=[เราจะทำอย่างนั้นได้อย่างไร], target=[How do we do that], predicted=[how do we do that]\n",
            "src=[ฉันนอนหลับดีมากเมื่อคืน], target=[I slept very well last night], predicted=[i slept very well last night]\n",
            "src=[เธอปฏิเสธข้อเสนอของเขา], target=[She refused his offer], predicted=[she refused his offer]\n",
            "src=[ฉันได้ยินเด็ก ๆ ร้องเพลงด้วยกัน], target=[I heard the children singing together], predicted=[i heard the children singing together]\n",
            "src=[ฉันกำลังต้มน้ำ], target=[I am boiling water], predicted=[i am boiling water]\n",
            "BLEU-1: 0.626647\n",
            "BLEU-2: 0.533923\n",
            "BLEU-3: 0.475710\n",
            "BLEU-4: 0.337213\n",
            "test\n",
            "src=[ฉันรอทอม], target=[I waited for Tom], predicted=[i waited for tom]\n",
            "src=[พวกนี้ไม่ใช่หนังสือของคุณเหรอ], target=[Arent these your books], predicted=[arent these your books]\n",
            "src=[หมายความว่าอะไร], target=[What does that mean], predicted=[what does that mean]\n",
            "src=[ตื่น], target=[Wake up], predicted=[get up]\n",
            "src=[ฉันไม่ใช่คนดัง], target=[Im not a celebrity], predicted=[im not a celebrity]\n",
            "src=[ทำไมคุณถึงเรียนภาษาฝรั่งเศส], target=[Why are you studying French], predicted=[why are you studying french]\n",
            "src=[ฉันพอใจกับงานของฉัน], target=[Im content with my job], predicted=[im satisfied with my french]\n",
            "src=[ฉันชอบเพลงอาร์แอนด์บี], target=[I like R  B], predicted=[i like r b]\n",
            "src=[ฉันเป็นผู้หญิง], target=[Im a woman], predicted=[im a woman]\n",
            "src=[คุณหมายถึงอะไร], target=[What do you mean], predicted=[what do you mean]\n",
            "BLEU-1: 0.344372\n",
            "BLEU-2: 0.287925\n",
            "BLEU-3: 0.269976\n",
            "BLEU-4: 0.179281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho84ObYq1rei",
        "outputId": "54326d58-cda7-4dd9-95b7-419ca98b089e"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณเกิดที่นี่ไหม], target=[Were you born here], predicted=[tom you born here]\n",
            "src=[ใครสน], target=[Who cares], predicted=[who cares]\n",
            "src=[ฉันกังวล], target=[Im worried], predicted=[im worried]\n",
            "src=[ปัญหาคือคุณยังเด็กเกินไป], target=[The trouble is that you are too young], predicted=[the is is you you you too]\n",
            "src=[ทอมไม่มา], target=[Tom isnt coming], predicted=[tom isnt happy]\n",
            "src=[เราจะทำอย่างนั้นได้อย่างไร], target=[How do we do that], predicted=[what do do do that]\n",
            "src=[ฉันนอนหลับดีมากเมื่อคืน], target=[I slept very well last night], predicted=[i slept very in last night]\n",
            "src=[เธอปฏิเสธข้อเสนอของเขา], target=[She refused his offer], predicted=[i never his offer]\n",
            "src=[ฉันได้ยินเด็ก ๆ ร้องเพลงด้วยกัน], target=[I heard the children singing together], predicted=[i had the the the morning]\n",
            "src=[ฉันกำลังต้มน้ำ], target=[I am boiling water], predicted=[i am be french]\n",
            "BLEU-1: 0.357712\n",
            "BLEU-2: 0.230587\n",
            "BLEU-3: 0.166469\n",
            "BLEU-4: 0.072304\n",
            "test\n",
            "src=[ฉันรอทอม], target=[I waited for Tom], predicted=[i made tom tom]\n",
            "src=[พวกนี้ไม่ใช่หนังสือของคุณเหรอ], target=[Arent these your books], predicted=[that was my small]\n",
            "src=[หมายความว่าอะไร], target=[What does that mean], predicted=[what that that that]\n",
            "src=[ตื่น], target=[Wake up], predicted=[go up]\n",
            "src=[ฉันไม่ใช่คนดัง], target=[Im not a celebrity], predicted=[im not a spy]\n",
            "src=[ทำไมคุณถึงเรียนภาษาฝรั่งเศส], target=[Why are you studying French], predicted=[tom are you french french]\n",
            "src=[ฉันพอใจกับงานของฉัน], target=[Im content with my job], predicted=[im like to my question]\n",
            "src=[ฉันชอบเพลงอาร์แอนด์บี], target=[I like R  B], predicted=[i like like air]\n",
            "src=[ฉันเป็นผู้หญิง], target=[Im a woman], predicted=[im a woman]\n",
            "src=[คุณหมายถึงอะไร], target=[What do you mean], predicted=[what do you it]\n",
            "BLEU-1: 0.200883\n",
            "BLEU-2: 0.122882\n",
            "BLEU-3: 0.088166\n",
            "BLEU-4: 0.033255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5A-nCBY2v-y",
        "outputId": "999037c3-f62d-4d0e-922e-6215855e96ab"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณเกิดที่นี่ไหม], target=[Were you born here], predicted=[she you met them]\n",
            "src=[ใครสน], target=[Who cares], predicted=[im quarreled]\n",
            "src=[ฉันกังวล], target=[Im worried], predicted=[im fell]\n",
            "src=[ปัญหาคือคุณยังเด็กเกินไป], target=[The trouble is that you are too young], predicted=[the husband is the good as a]\n",
            "src=[ทอมไม่มา], target=[Tom isnt coming], predicted=[it isnt coming]\n",
            "src=[เราจะทำอย่างนั้นได้อย่างไร], target=[How do we do that], predicted=[what i to to you]\n",
            "src=[ฉันนอนหลับดีมากเมื่อคืน], target=[I slept very well last night], predicted=[i have to to last]\n",
            "src=[เธอปฏิเสธข้อเสนอของเขา], target=[She refused his offer], predicted=[he may very me]\n",
            "src=[ฉันได้ยินเด็ก ๆ ร้องเพลงด้วยกัน], target=[I heard the children singing together], predicted=[i had breakfast at at]\n",
            "src=[ฉันกำลังต้มน้ำ], target=[I am boiling water], predicted=[i dont a it]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_XTmr5KY9TW",
        "outputId": "1fc1f275-f4cd-4f37-b07a-ba2868b3a628"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\t#table = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\t#line = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\t#line = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/tha.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-thai.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-thai.pkl\n",
            "[Hi.] => [สวัสดี]\n",
            "[Run.] => [วิ่ง]\n",
            "[Who?] => [ใคร?]\n",
            "[Wow!] => [ว้าว!]\n",
            "[Fire!] => [ยิง!]\n",
            "[Help!] => [ช่วยด้วย!]\n",
            "[Jump!] => [กระโดด!]\n",
            "[Jump.] => [กระโดด]\n",
            "[Stop!] => [หยุด!]\n",
            "[Wait.] => [รอก่อน]\n",
            "[Go on.] => [ต่อไป]\n",
            "[Hello!] => [สวัสดี!]\n",
            "[Hurry!] => [เร็ว ๆ!]\n",
            "[I see.] => [ฉันเข้าใจแล้ว]\n",
            "[I try.] => [ฉันลองแล้ว]\n",
            "[I won!] => [ฉันชนะ!]\n",
            "[Oh no!] => [ไม่นะ!]\n",
            "[Relax.] => [พักผ่อน]\n",
            "[Smile.] => [ยิ้ม!]\n",
            "[Attack!] => [โจมตี!]\n",
            "[Cheers!] => [เชียร์!]\n",
            "[Freeze!] => [ฟรีส!]\n",
            "[Freeze!] => [แข็ง!]\n",
            "[Get up.] => [ตื่น]\n",
            "[Go now.] => [ไปเลยตอนนี้]\n",
            "[Got it!] => [เข้าใจแล้ว!]\n",
            "[Got it!] => [จับได้แล้ว!]\n",
            "[Got it!] => [ทำได้แล้ว!]\n",
            "[Got it?] => [เข้าใจมั้ย?]\n",
            "[He ran.] => [เขาวิ่ง]\n",
            "[Hop in.] => [กระโดดไป]\n",
            "[Hug me.] => [กอดฉัน]\n",
            "[I fell.] => [ฉันล้ม]\n",
            "[I left.] => [ฉันออก]\n",
            "[I lied.] => [ฉันโกหก]\n",
            "[I lost.] => [ฉันแพ้]\n",
            "[I quit.] => [ฉันออก]\n",
            "[I work.] => [ฉันทำงาน]\n",
            "[I'm 19.] => [ฉันอายุ 19 ปี]\n",
            "[I'm OK.] => [ฉันโอเค]\n",
            "[I'm OK.] => [ฉันไม่เป็นไร]\n",
            "[I'm up.] => [ฉันตื่น]\n",
            "[Listen.] => [ฟัง]\n",
            "[Listen.] => [ฟังนะ]\n",
            "[No way!] => [ไม่มีทาง!]\n",
            "[Really?] => [จริงเหรอ!]\n",
            "[Try it.] => [ลองมัน]\n",
            "[We try.] => [พวกเราลองแล้ว]\n",
            "[We won.] => [เราชนะ]\n",
            "[Why me?] => [ทำไมต้องเป็นฉัน?]\n",
            "[Ask Tom.] => [ถามทอม]\n",
            "[Awesome!] => [สุดยอด!]\n",
            "[Beat it.] => [ทำลายมัน]\n",
            "[Call me.] => [โทรหาฉัน]\n",
            "[Come in.] => [เข้ามาข้างใน]\n",
            "[Get out!] => [ออกไป!]\n",
            "[Get out.] => [ออกไป]\n",
            "[Go away!] => [ไปไกล ๆ!]\n",
            "[Go away.] => [ไปซะ]\n",
            "[Go away.] => [ไปไกล ๆ]\n",
            "[Go home.] => [กลับบ้าน]\n",
            "[Goodbye!] => [ลาก่อน!]\n",
            "[He runs.] => [เขาวิ่ง]\n",
            "[Help me!] => [ช่วยฉันด้วย!]\n",
            "[Help me.] => [ช่วยฉัน]\n",
            "[Hi, Tom.] => [สวัสดีทอม]\n",
            "[Hold it!] => [จับมันไว้!]\n",
            "[I agree.] => [ฉันเห็นด้วย]\n",
            "[I'll go.] => [ฉันจะไป]\n",
            "[I'm Tom.] => [ฉันคือทอม]\n",
            "[I'm hot.] => [ฉันร้อน]\n",
            "[I'm ill.] => [ฉันป่วย]\n",
            "[I'm shy.] => [ฉันอาย]\n",
            "[It's OK.] => [มันพอได้]\n",
            "[It's OK.] => [มันโอเค]\n",
            "[It's me.] => [ฉันเอง]\n",
            "[Me, too.] => [ฉันด้วยเหมือนกัน]\n",
            "[Open up.] => [เปิดออก]\n",
            "[Perfect!] => [เยี่ยมยอด!]\n",
            "[Perfect!] => [พอดีเป๊ะ!]\n",
            "[Show me.] => [แสดงให้ฉัน]\n",
            "[Shut up!] => [เงียบ!]\n",
            "[Tell me.] => [บอกฉัน]\n",
            "[Tom ran.] => [ทอมวิ่ง]\n",
            "[Wake up!] => [ตื่น!]\n",
            "[Wake up.] => [ตื่น]\n",
            "[Wash up.] => [ล้าง]\n",
            "[We lost.] => [เราแพ้]\n",
            "[Welcome.] => [ยินดีต้อนรับ]\n",
            "[Who won?] => [ใครชนะ?]\n",
            "[You run.] => [คุณวิ่ง]\n",
            "[Be a man.] => [เป็นลูกผู้ชายหน่อย]\n",
            "[Be quiet.] => [เงียบ ๆ]\n",
            "[Call Tom.] => [โทรหาทอม]\n",
            "[Cheer up!] => [สู้ตาย!]\n",
            "[Don't go.] => [อย่าไป]\n",
            "[Find Tom.] => [หาทอม]\n",
            "[Grab Tom.] => [จับทอม]\n",
            "[Grab him.] => [จับเขา]\n",
            "[Have fun.] => [ขอให้สนุก]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mi5kvG5ZiQz",
        "outputId": "7b0c4721-e44a-4314-c923-81a9f97f7831"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-thai.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 10000\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:9000], dataset[1000:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-thai-both.pkl')\n",
        "save_clean_data(train, 'english-thai-train.pkl')\n",
        "save_clean_data(test, 'english-thai-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-thai-both.pkl\n",
            "Saved: english-thai-train.pkl\n",
            "Saved: english-thai-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb52vjtZdej9",
        "outputId": "bea5435f-1961-4a06-807c-b9a2527a581e"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Thai Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Thai Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=300, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 1983\n",
            "English Max Length: 20\n",
            "Thai Vocabulary Size: 2973\n",
            "Thai Max Length: 6\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 6, 256)            761088    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 20, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 20, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 20, 1983)          509631    \n",
            "=================================================================\n",
            "Total params: 2,321,343\n",
            "Trainable params: 2,321,343\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/300\n",
            "46/46 - 21s - loss: 3.4964 - val_loss: 1.9258\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.92576, saving model to model.h5\n",
            "Epoch 2/300\n",
            "46/46 - 14s - loss: 1.8583 - val_loss: 1.8795\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.92576 to 1.87953, saving model to model.h5\n",
            "Epoch 3/300\n",
            "46/46 - 14s - loss: 1.7873 - val_loss: 2.1007\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.87953\n",
            "Epoch 4/300\n",
            "46/46 - 14s - loss: 1.7808 - val_loss: 1.7257\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.87953 to 1.72573, saving model to model.h5\n",
            "Epoch 5/300\n",
            "46/46 - 14s - loss: 1.6880 - val_loss: 1.6806\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.72573 to 1.68063, saving model to model.h5\n",
            "Epoch 6/300\n",
            "46/46 - 14s - loss: 1.6496 - val_loss: 1.6450\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.68063 to 1.64500, saving model to model.h5\n",
            "Epoch 7/300\n",
            "46/46 - 14s - loss: 1.6077 - val_loss: 1.5889\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.64500 to 1.58892, saving model to model.h5\n",
            "Epoch 8/300\n",
            "46/46 - 14s - loss: 1.5633 - val_loss: 1.5479\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.58892 to 1.54789, saving model to model.h5\n",
            "Epoch 9/300\n",
            "46/46 - 14s - loss: 1.5234 - val_loss: 1.5076\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.54789 to 1.50763, saving model to model.h5\n",
            "Epoch 10/300\n",
            "46/46 - 14s - loss: 1.4867 - val_loss: 1.4707\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.50763 to 1.47067, saving model to model.h5\n",
            "Epoch 11/300\n",
            "46/46 - 14s - loss: 1.4482 - val_loss: 1.4267\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.47067 to 1.42667, saving model to model.h5\n",
            "Epoch 12/300\n",
            "46/46 - 14s - loss: 1.4066 - val_loss: 1.3829\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.42667 to 1.38286, saving model to model.h5\n",
            "Epoch 13/300\n",
            "46/46 - 14s - loss: 1.3663 - val_loss: 1.3438\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.38286 to 1.34384, saving model to model.h5\n",
            "Epoch 14/300\n",
            "46/46 - 14s - loss: 1.3316 - val_loss: 1.3172\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.34384 to 1.31721, saving model to model.h5\n",
            "Epoch 15/300\n",
            "46/46 - 14s - loss: 1.3071 - val_loss: 1.2961\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.31721 to 1.29606, saving model to model.h5\n",
            "Epoch 16/300\n",
            "46/46 - 14s - loss: 1.2894 - val_loss: 1.2809\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.29606 to 1.28086, saving model to model.h5\n",
            "Epoch 17/300\n",
            "46/46 - 14s - loss: 1.2762 - val_loss: 1.2672\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.28086 to 1.26717, saving model to model.h5\n",
            "Epoch 18/300\n",
            "46/46 - 14s - loss: 1.2577 - val_loss: 1.2484\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.26717 to 1.24843, saving model to model.h5\n",
            "Epoch 19/300\n",
            "46/46 - 14s - loss: 1.2423 - val_loss: 1.2408\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.24843 to 1.24083, saving model to model.h5\n",
            "Epoch 20/300\n",
            "46/46 - 14s - loss: 1.2323 - val_loss: 1.2278\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.24083 to 1.22780, saving model to model.h5\n",
            "Epoch 21/300\n",
            "46/46 - 14s - loss: 1.2212 - val_loss: 1.2094\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.22780 to 1.20940, saving model to model.h5\n",
            "Epoch 22/300\n",
            "46/46 - 14s - loss: 1.2054 - val_loss: 1.1954\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.20940 to 1.19539, saving model to model.h5\n",
            "Epoch 23/300\n",
            "46/46 - 14s - loss: 1.1898 - val_loss: 1.1802\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.19539 to 1.18019, saving model to model.h5\n",
            "Epoch 24/300\n",
            "46/46 - 14s - loss: 1.1771 - val_loss: 1.1752\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.18019 to 1.17521, saving model to model.h5\n",
            "Epoch 25/300\n",
            "46/46 - 14s - loss: 1.1693 - val_loss: 1.1579\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.17521 to 1.15789, saving model to model.h5\n",
            "Epoch 26/300\n",
            "46/46 - 14s - loss: 1.1568 - val_loss: 1.1445\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.15789 to 1.14446, saving model to model.h5\n",
            "Epoch 27/300\n",
            "46/46 - 14s - loss: 1.1443 - val_loss: 1.1404\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.14446 to 1.14038, saving model to model.h5\n",
            "Epoch 28/300\n",
            "46/46 - 14s - loss: 1.1329 - val_loss: 1.1184\n",
            "\n",
            "Epoch 00028: val_loss improved from 1.14038 to 1.11843, saving model to model.h5\n",
            "Epoch 29/300\n",
            "46/46 - 14s - loss: 1.1172 - val_loss: 1.1042\n",
            "\n",
            "Epoch 00029: val_loss improved from 1.11843 to 1.10419, saving model to model.h5\n",
            "Epoch 30/300\n",
            "46/46 - 14s - loss: 1.1001 - val_loss: 1.0894\n",
            "\n",
            "Epoch 00030: val_loss improved from 1.10419 to 1.08937, saving model to model.h5\n",
            "Epoch 31/300\n",
            "46/46 - 14s - loss: 1.0861 - val_loss: 1.0713\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.08937 to 1.07130, saving model to model.h5\n",
            "Epoch 32/300\n",
            "46/46 - 14s - loss: 1.0720 - val_loss: 1.0580\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.07130 to 1.05799, saving model to model.h5\n",
            "Epoch 33/300\n",
            "46/46 - 14s - loss: 1.0614 - val_loss: 1.0480\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.05799 to 1.04799, saving model to model.h5\n",
            "Epoch 34/300\n",
            "46/46 - 14s - loss: 1.0499 - val_loss: 1.0303\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.04799 to 1.03029, saving model to model.h5\n",
            "Epoch 35/300\n",
            "46/46 - 14s - loss: 1.0338 - val_loss: 1.0141\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.03029 to 1.01412, saving model to model.h5\n",
            "Epoch 36/300\n",
            "46/46 - 14s - loss: 1.0181 - val_loss: 0.9971\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.01412 to 0.99712, saving model to model.h5\n",
            "Epoch 37/300\n",
            "46/46 - 14s - loss: 1.0021 - val_loss: 0.9884\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.99712 to 0.98843, saving model to model.h5\n",
            "Epoch 38/300\n",
            "46/46 - 14s - loss: 0.9906 - val_loss: 0.9709\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.98843 to 0.97089, saving model to model.h5\n",
            "Epoch 39/300\n",
            "46/46 - 14s - loss: 0.9757 - val_loss: 0.9593\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.97089 to 0.95925, saving model to model.h5\n",
            "Epoch 40/300\n",
            "46/46 - 14s - loss: 0.9613 - val_loss: 0.9388\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.95925 to 0.93878, saving model to model.h5\n",
            "Epoch 41/300\n",
            "46/46 - 14s - loss: 0.9453 - val_loss: 0.9230\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.93878 to 0.92303, saving model to model.h5\n",
            "Epoch 42/300\n",
            "46/46 - 14s - loss: 0.9308 - val_loss: 0.9095\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.92303 to 0.90947, saving model to model.h5\n",
            "Epoch 43/300\n",
            "46/46 - 14s - loss: 0.9130 - val_loss: 0.8913\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.90947 to 0.89133, saving model to model.h5\n",
            "Epoch 44/300\n",
            "46/46 - 14s - loss: 0.8946 - val_loss: 0.8697\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.89133 to 0.86972, saving model to model.h5\n",
            "Epoch 45/300\n",
            "46/46 - 14s - loss: 0.8766 - val_loss: 0.8582\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.86972 to 0.85824, saving model to model.h5\n",
            "Epoch 46/300\n",
            "46/46 - 14s - loss: 0.8635 - val_loss: 0.8394\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.85824 to 0.83943, saving model to model.h5\n",
            "Epoch 47/300\n",
            "46/46 - 14s - loss: 0.8467 - val_loss: 0.8216\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.83943 to 0.82160, saving model to model.h5\n",
            "Epoch 48/300\n",
            "46/46 - 14s - loss: 0.8276 - val_loss: 0.8001\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.82160 to 0.80010, saving model to model.h5\n",
            "Epoch 49/300\n",
            "46/46 - 14s - loss: 0.8093 - val_loss: 0.7845\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.80010 to 0.78454, saving model to model.h5\n",
            "Epoch 50/300\n",
            "46/46 - 14s - loss: 0.7912 - val_loss: 0.7625\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.78454 to 0.76252, saving model to model.h5\n",
            "Epoch 51/300\n",
            "46/46 - 14s - loss: 0.7706 - val_loss: 0.7457\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.76252 to 0.74571, saving model to model.h5\n",
            "Epoch 52/300\n",
            "46/46 - 14s - loss: 0.7540 - val_loss: 0.7278\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.74571 to 0.72778, saving model to model.h5\n",
            "Epoch 53/300\n",
            "46/46 - 14s - loss: 0.7399 - val_loss: 0.7140\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.72778 to 0.71399, saving model to model.h5\n",
            "Epoch 54/300\n",
            "46/46 - 14s - loss: 0.7259 - val_loss: 0.7034\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.71399 to 0.70342, saving model to model.h5\n",
            "Epoch 55/300\n",
            "46/46 - 14s - loss: 0.7103 - val_loss: 0.6834\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.70342 to 0.68336, saving model to model.h5\n",
            "Epoch 56/300\n",
            "46/46 - 14s - loss: 0.6947 - val_loss: 0.6636\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.68336 to 0.66365, saving model to model.h5\n",
            "Epoch 57/300\n",
            "46/46 - 14s - loss: 0.6730 - val_loss: 0.6441\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.66365 to 0.64412, saving model to model.h5\n",
            "Epoch 58/300\n",
            "46/46 - 14s - loss: 0.6544 - val_loss: 0.6296\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.64412 to 0.62956, saving model to model.h5\n",
            "Epoch 59/300\n",
            "46/46 - 14s - loss: 0.6372 - val_loss: 0.6102\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.62956 to 0.61018, saving model to model.h5\n",
            "Epoch 60/300\n",
            "46/46 - 14s - loss: 0.6189 - val_loss: 0.5918\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.61018 to 0.59175, saving model to model.h5\n",
            "Epoch 61/300\n",
            "46/46 - 14s - loss: 0.5998 - val_loss: 0.5748\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.59175 to 0.57482, saving model to model.h5\n",
            "Epoch 62/300\n",
            "46/46 - 14s - loss: 0.5822 - val_loss: 0.5560\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.57482 to 0.55596, saving model to model.h5\n",
            "Epoch 63/300\n",
            "46/46 - 14s - loss: 0.5656 - val_loss: 0.5420\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.55596 to 0.54204, saving model to model.h5\n",
            "Epoch 64/300\n",
            "46/46 - 14s - loss: 0.5515 - val_loss: 0.5268\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.54204 to 0.52681, saving model to model.h5\n",
            "Epoch 65/300\n",
            "46/46 - 14s - loss: 0.5378 - val_loss: 0.5085\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.52681 to 0.50854, saving model to model.h5\n",
            "Epoch 66/300\n",
            "46/46 - 14s - loss: 0.5228 - val_loss: 0.4965\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.50854 to 0.49652, saving model to model.h5\n",
            "Epoch 67/300\n",
            "46/46 - 14s - loss: 0.5040 - val_loss: 0.4768\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.49652 to 0.47685, saving model to model.h5\n",
            "Epoch 68/300\n",
            "46/46 - 14s - loss: 0.4855 - val_loss: 0.4636\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.47685 to 0.46365, saving model to model.h5\n",
            "Epoch 69/300\n",
            "46/46 - 14s - loss: 0.4704 - val_loss: 0.4456\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.46365 to 0.44562, saving model to model.h5\n",
            "Epoch 70/300\n",
            "46/46 - 14s - loss: 0.4546 - val_loss: 0.4309\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.44562 to 0.43089, saving model to model.h5\n",
            "Epoch 71/300\n",
            "46/46 - 14s - loss: 0.4385 - val_loss: 0.4124\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.43089 to 0.41240, saving model to model.h5\n",
            "Epoch 72/300\n",
            "46/46 - 14s - loss: 0.4225 - val_loss: 0.3975\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.41240 to 0.39751, saving model to model.h5\n",
            "Epoch 73/300\n",
            "46/46 - 15s - loss: 0.4077 - val_loss: 0.3852\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.39751 to 0.38525, saving model to model.h5\n",
            "Epoch 74/300\n",
            "46/46 - 14s - loss: 0.3960 - val_loss: 0.3735\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.38525 to 0.37351, saving model to model.h5\n",
            "Epoch 75/300\n",
            "46/46 - 14s - loss: 0.3824 - val_loss: 0.3674\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.37351 to 0.36738, saving model to model.h5\n",
            "Epoch 76/300\n",
            "46/46 - 14s - loss: 0.3685 - val_loss: 0.3464\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.36738 to 0.34644, saving model to model.h5\n",
            "Epoch 77/300\n",
            "46/46 - 14s - loss: 0.3554 - val_loss: 0.3345\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.34644 to 0.33452, saving model to model.h5\n",
            "Epoch 78/300\n",
            "46/46 - 14s - loss: 0.3426 - val_loss: 0.3241\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.33452 to 0.32411, saving model to model.h5\n",
            "Epoch 79/300\n",
            "46/46 - 14s - loss: 0.3265 - val_loss: 0.3051\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.32411 to 0.30515, saving model to model.h5\n",
            "Epoch 80/300\n",
            "46/46 - 14s - loss: 0.3134 - val_loss: 0.2945\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.30515 to 0.29448, saving model to model.h5\n",
            "Epoch 81/300\n",
            "46/46 - 14s - loss: 0.2993 - val_loss: 0.2811\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.29448 to 0.28106, saving model to model.h5\n",
            "Epoch 82/300\n",
            "46/46 - 14s - loss: 0.2839 - val_loss: 0.2658\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.28106 to 0.26582, saving model to model.h5\n",
            "Epoch 83/300\n",
            "46/46 - 14s - loss: 0.2706 - val_loss: 0.2524\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.26582 to 0.25236, saving model to model.h5\n",
            "Epoch 84/300\n",
            "46/46 - 14s - loss: 0.2576 - val_loss: 0.2435\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.25236 to 0.24347, saving model to model.h5\n",
            "Epoch 85/300\n",
            "46/46 - 14s - loss: 0.2453 - val_loss: 0.2295\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.24347 to 0.22955, saving model to model.h5\n",
            "Epoch 86/300\n",
            "46/46 - 14s - loss: 0.2343 - val_loss: 0.2241\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.22955 to 0.22407, saving model to model.h5\n",
            "Epoch 87/300\n",
            "46/46 - 14s - loss: 0.2263 - val_loss: 0.2127\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.22407 to 0.21271, saving model to model.h5\n",
            "Epoch 88/300\n",
            "46/46 - 14s - loss: 0.2176 - val_loss: 0.2050\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.21271 to 0.20502, saving model to model.h5\n",
            "Epoch 89/300\n",
            "46/46 - 14s - loss: 0.2070 - val_loss: 0.1944\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.20502 to 0.19444, saving model to model.h5\n",
            "Epoch 90/300\n",
            "46/46 - 14s - loss: 0.1973 - val_loss: 0.1851\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.19444 to 0.18508, saving model to model.h5\n",
            "Epoch 91/300\n",
            "46/46 - 14s - loss: 0.1885 - val_loss: 0.1760\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.18508 to 0.17600, saving model to model.h5\n",
            "Epoch 92/300\n",
            "46/46 - 14s - loss: 0.1782 - val_loss: 0.1658\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.17600 to 0.16579, saving model to model.h5\n",
            "Epoch 93/300\n",
            "46/46 - 14s - loss: 0.1690 - val_loss: 0.1592\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.16579 to 0.15925, saving model to model.h5\n",
            "Epoch 94/300\n",
            "46/46 - 14s - loss: 0.1604 - val_loss: 0.1523\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.15925 to 0.15227, saving model to model.h5\n",
            "Epoch 95/300\n",
            "46/46 - 14s - loss: 0.1531 - val_loss: 0.1434\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.15227 to 0.14337, saving model to model.h5\n",
            "Epoch 96/300\n",
            "46/46 - 14s - loss: 0.1448 - val_loss: 0.1391\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.14337 to 0.13914, saving model to model.h5\n",
            "Epoch 97/300\n",
            "46/46 - 14s - loss: 0.1402 - val_loss: 0.1339\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.13914 to 0.13389, saving model to model.h5\n",
            "Epoch 98/300\n",
            "46/46 - 14s - loss: 0.1382 - val_loss: 0.1294\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.13389 to 0.12937, saving model to model.h5\n",
            "Epoch 99/300\n",
            "46/46 - 14s - loss: 0.1323 - val_loss: 0.1222\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.12937 to 0.12223, saving model to model.h5\n",
            "Epoch 100/300\n",
            "46/46 - 14s - loss: 0.1241 - val_loss: 0.1150\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.12223 to 0.11496, saving model to model.h5\n",
            "Epoch 101/300\n",
            "46/46 - 14s - loss: 0.1176 - val_loss: 0.1111\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.11496 to 0.11114, saving model to model.h5\n",
            "Epoch 102/300\n",
            "46/46 - 14s - loss: 0.1122 - val_loss: 0.1035\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.11114 to 0.10353, saving model to model.h5\n",
            "Epoch 103/300\n",
            "46/46 - 14s - loss: 0.1046 - val_loss: 0.0982\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.10353 to 0.09821, saving model to model.h5\n",
            "Epoch 104/300\n",
            "46/46 - 14s - loss: 0.0987 - val_loss: 0.0920\n",
            "\n",
            "Epoch 00104: val_loss improved from 0.09821 to 0.09196, saving model to model.h5\n",
            "Epoch 105/300\n",
            "46/46 - 14s - loss: 0.0928 - val_loss: 0.0862\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.09196 to 0.08620, saving model to model.h5\n",
            "Epoch 106/300\n",
            "46/46 - 14s - loss: 0.0870 - val_loss: 0.0811\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.08620 to 0.08115, saving model to model.h5\n",
            "Epoch 107/300\n",
            "46/46 - 14s - loss: 0.0826 - val_loss: 0.0773\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.08115 to 0.07727, saving model to model.h5\n",
            "Epoch 108/300\n",
            "46/46 - 14s - loss: 0.0779 - val_loss: 0.0730\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.07727 to 0.07296, saving model to model.h5\n",
            "Epoch 109/300\n",
            "46/46 - 14s - loss: 0.0736 - val_loss: 0.0687\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.07296 to 0.06869, saving model to model.h5\n",
            "Epoch 110/300\n",
            "46/46 - 14s - loss: 0.0706 - val_loss: 0.0678\n",
            "\n",
            "Epoch 00110: val_loss improved from 0.06869 to 0.06779, saving model to model.h5\n",
            "Epoch 111/300\n",
            "46/46 - 14s - loss: 0.0697 - val_loss: 0.0657\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.06779 to 0.06567, saving model to model.h5\n",
            "Epoch 112/300\n",
            "46/46 - 14s - loss: 0.0666 - val_loss: 0.0614\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.06567 to 0.06144, saving model to model.h5\n",
            "Epoch 113/300\n",
            "46/46 - 14s - loss: 0.0646 - val_loss: 0.0603\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.06144 to 0.06033, saving model to model.h5\n",
            "Epoch 114/300\n",
            "46/46 - 14s - loss: 0.0612 - val_loss: 0.0573\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.06033 to 0.05728, saving model to model.h5\n",
            "Epoch 115/300\n",
            "46/46 - 14s - loss: 0.0577 - val_loss: 0.0558\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.05728 to 0.05580, saving model to model.h5\n",
            "Epoch 116/300\n",
            "46/46 - 14s - loss: 0.0554 - val_loss: 0.0530\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.05580 to 0.05302, saving model to model.h5\n",
            "Epoch 117/300\n",
            "46/46 - 14s - loss: 0.0541 - val_loss: 0.0562\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.05302\n",
            "Epoch 118/300\n",
            "46/46 - 14s - loss: 0.0537 - val_loss: 0.0476\n",
            "\n",
            "Epoch 00118: val_loss improved from 0.05302 to 0.04762, saving model to model.h5\n",
            "Epoch 119/300\n",
            "46/46 - 14s - loss: 0.0502 - val_loss: 0.0468\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.04762 to 0.04685, saving model to model.h5\n",
            "Epoch 120/300\n",
            "46/46 - 14s - loss: 0.0492 - val_loss: 0.0458\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.04685 to 0.04576, saving model to model.h5\n",
            "Epoch 121/300\n",
            "46/46 - 14s - loss: 0.0471 - val_loss: 0.0432\n",
            "\n",
            "Epoch 00121: val_loss improved from 0.04576 to 0.04319, saving model to model.h5\n",
            "Epoch 122/300\n",
            "46/46 - 14s - loss: 0.0440 - val_loss: 0.0406\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.04319 to 0.04064, saving model to model.h5\n",
            "Epoch 123/300\n",
            "46/46 - 14s - loss: 0.0438 - val_loss: 0.0412\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.04064\n",
            "Epoch 124/300\n",
            "46/46 - 14s - loss: 0.0423 - val_loss: 0.0396\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.04064 to 0.03963, saving model to model.h5\n",
            "Epoch 125/300\n",
            "46/46 - 14s - loss: 0.0396 - val_loss: 0.0360\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.03963 to 0.03596, saving model to model.h5\n",
            "Epoch 126/300\n",
            "46/46 - 14s - loss: 0.0366 - val_loss: 0.0338\n",
            "\n",
            "Epoch 00126: val_loss improved from 0.03596 to 0.03380, saving model to model.h5\n",
            "Epoch 127/300\n",
            "46/46 - 14s - loss: 0.0347 - val_loss: 0.0347\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.03380\n",
            "Epoch 128/300\n",
            "46/46 - 14s - loss: 0.0336 - val_loss: 0.0324\n",
            "\n",
            "Epoch 00128: val_loss improved from 0.03380 to 0.03241, saving model to model.h5\n",
            "Epoch 129/300\n",
            "46/46 - 14s - loss: 0.0333 - val_loss: 0.0315\n",
            "\n",
            "Epoch 00129: val_loss improved from 0.03241 to 0.03153, saving model to model.h5\n",
            "Epoch 130/300\n",
            "46/46 - 14s - loss: 0.0316 - val_loss: 0.0289\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.03153 to 0.02892, saving model to model.h5\n",
            "Epoch 131/300\n",
            "46/46 - 14s - loss: 0.0294 - val_loss: 0.0268\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.02892 to 0.02682, saving model to model.h5\n",
            "Epoch 132/300\n",
            "46/46 - 14s - loss: 0.0270 - val_loss: 0.0264\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.02682 to 0.02638, saving model to model.h5\n",
            "Epoch 133/300\n",
            "46/46 - 14s - loss: 0.0262 - val_loss: 0.0250\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.02638 to 0.02501, saving model to model.h5\n",
            "Epoch 134/300\n",
            "46/46 - 14s - loss: 0.0266 - val_loss: 0.0261\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.02501\n",
            "Epoch 135/300\n",
            "46/46 - 14s - loss: 0.0272 - val_loss: 0.0272\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.02501\n",
            "Epoch 136/300\n",
            "46/46 - 14s - loss: 0.0276 - val_loss: 0.0261\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.02501\n",
            "Epoch 137/300\n",
            "46/46 - 14s - loss: 0.0292 - val_loss: 0.0285\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.02501\n",
            "Epoch 138/300\n",
            "46/46 - 14s - loss: 0.0289 - val_loss: 0.0277\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.02501\n",
            "Epoch 139/300\n",
            "46/46 - 14s - loss: 0.0293 - val_loss: 0.0264\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.02501\n",
            "Epoch 140/300\n",
            "46/46 - 14s - loss: 0.0297 - val_loss: 0.0271\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.02501\n",
            "Epoch 141/300\n",
            "46/46 - 14s - loss: 0.0293 - val_loss: 0.0282\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.02501\n",
            "Epoch 142/300\n",
            "46/46 - 14s - loss: 0.0280 - val_loss: 0.0247\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.02501 to 0.02468, saving model to model.h5\n",
            "Epoch 143/300\n",
            "46/46 - 14s - loss: 0.0271 - val_loss: 0.0243\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.02468 to 0.02435, saving model to model.h5\n",
            "Epoch 144/300\n",
            "46/46 - 14s - loss: 0.0252 - val_loss: 0.0210\n",
            "\n",
            "Epoch 00144: val_loss improved from 0.02435 to 0.02102, saving model to model.h5\n",
            "Epoch 145/300\n",
            "46/46 - 14s - loss: 0.0233 - val_loss: 0.0204\n",
            "\n",
            "Epoch 00145: val_loss improved from 0.02102 to 0.02036, saving model to model.h5\n",
            "Epoch 146/300\n",
            "46/46 - 14s - loss: 0.0215 - val_loss: 0.0202\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.02036 to 0.02024, saving model to model.h5\n",
            "Epoch 147/300\n",
            "46/46 - 14s - loss: 0.0210 - val_loss: 0.0196\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.02024 to 0.01955, saving model to model.h5\n",
            "Epoch 148/300\n",
            "46/46 - 14s - loss: 0.0188 - val_loss: 0.0176\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.01955 to 0.01763, saving model to model.h5\n",
            "Epoch 149/300\n",
            "46/46 - 14s - loss: 0.0184 - val_loss: 0.0167\n",
            "\n",
            "Epoch 00149: val_loss improved from 0.01763 to 0.01669, saving model to model.h5\n",
            "Epoch 150/300\n",
            "46/46 - 14s - loss: 0.0180 - val_loss: 0.0170\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.01669\n",
            "Epoch 151/300\n",
            "46/46 - 14s - loss: 0.0185 - val_loss: 0.0180\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.01669\n",
            "Epoch 152/300\n",
            "46/46 - 14s - loss: 0.0186 - val_loss: 0.0204\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.01669\n",
            "Epoch 153/300\n",
            "46/46 - 14s - loss: 0.0198 - val_loss: 0.0194\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.01669\n",
            "Epoch 154/300\n",
            "46/46 - 14s - loss: 0.0204 - val_loss: 0.0203\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.01669\n",
            "Epoch 155/300\n",
            "46/46 - 14s - loss: 0.0250 - val_loss: 0.0191\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.01669\n",
            "Epoch 156/300\n",
            "46/46 - 14s - loss: 0.0212 - val_loss: 0.0189\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.01669\n",
            "Epoch 157/300\n",
            "46/46 - 14s - loss: 0.0211 - val_loss: 0.0207\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.01669\n",
            "Epoch 158/300\n",
            "46/46 - 14s - loss: 0.0215 - val_loss: 0.0177\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.01669\n",
            "Epoch 159/300\n",
            "46/46 - 14s - loss: 0.0200 - val_loss: 0.0173\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.01669\n",
            "Epoch 160/300\n",
            "46/46 - 14s - loss: 0.0196 - val_loss: 0.0164\n",
            "\n",
            "Epoch 00160: val_loss improved from 0.01669 to 0.01641, saving model to model.h5\n",
            "Epoch 161/300\n",
            "46/46 - 14s - loss: 0.0177 - val_loss: 0.0177\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.01641\n",
            "Epoch 162/300\n",
            "46/46 - 14s - loss: 0.0184 - val_loss: 0.0158\n",
            "\n",
            "Epoch 00162: val_loss improved from 0.01641 to 0.01576, saving model to model.h5\n",
            "Epoch 163/300\n",
            "46/46 - 14s - loss: 0.0180 - val_loss: 0.0151\n",
            "\n",
            "Epoch 00163: val_loss improved from 0.01576 to 0.01510, saving model to model.h5\n",
            "Epoch 164/300\n",
            "46/46 - 14s - loss: 0.0175 - val_loss: 0.0196\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.01510\n",
            "Epoch 165/300\n",
            "46/46 - 14s - loss: 0.0182 - val_loss: 0.0191\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.01510\n",
            "Epoch 166/300\n",
            "46/46 - 14s - loss: 0.0178 - val_loss: 0.0175\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.01510\n",
            "Epoch 167/300\n",
            "46/46 - 14s - loss: 0.0186 - val_loss: 0.0147\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.01510 to 0.01471, saving model to model.h5\n",
            "Epoch 168/300\n",
            "46/46 - 14s - loss: 0.0173 - val_loss: 0.0152\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.01471\n",
            "Epoch 169/300\n",
            "46/46 - 14s - loss: 0.0177 - val_loss: 0.0144\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.01471 to 0.01441, saving model to model.h5\n",
            "Epoch 170/300\n",
            "46/46 - 14s - loss: 0.0169 - val_loss: 0.0137\n",
            "\n",
            "Epoch 00170: val_loss improved from 0.01441 to 0.01368, saving model to model.h5\n",
            "Epoch 171/300\n",
            "46/46 - 14s - loss: 0.0146 - val_loss: 0.0116\n",
            "\n",
            "Epoch 00171: val_loss improved from 0.01368 to 0.01157, saving model to model.h5\n",
            "Epoch 172/300\n",
            "46/46 - 14s - loss: 0.0123 - val_loss: 0.0101\n",
            "\n",
            "Epoch 00172: val_loss improved from 0.01157 to 0.01010, saving model to model.h5\n",
            "Epoch 173/300\n",
            "46/46 - 14s - loss: 0.0112 - val_loss: 0.0102\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.01010\n",
            "Epoch 174/300\n",
            "46/46 - 14s - loss: 0.0112 - val_loss: 0.0099\n",
            "\n",
            "Epoch 00174: val_loss improved from 0.01010 to 0.00994, saving model to model.h5\n",
            "Epoch 175/300\n",
            "46/46 - 14s - loss: 0.0107 - val_loss: 0.0088\n",
            "\n",
            "Epoch 00175: val_loss improved from 0.00994 to 0.00880, saving model to model.h5\n",
            "Epoch 176/300\n",
            "46/46 - 14s - loss: 0.0105 - val_loss: 0.0096\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.00880\n",
            "Epoch 177/300\n",
            "46/46 - 14s - loss: 0.0110 - val_loss: 0.0097\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00880\n",
            "Epoch 178/300\n",
            "46/46 - 14s - loss: 0.0103 - val_loss: 0.0088\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00880\n",
            "Epoch 179/300\n",
            "46/46 - 14s - loss: 0.0105 - val_loss: 0.0095\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00880\n",
            "Epoch 180/300\n",
            "46/46 - 14s - loss: 0.0109 - val_loss: 0.0106\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00880\n",
            "Epoch 181/300\n",
            "46/46 - 14s - loss: 0.0121 - val_loss: 0.0100\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.00880\n",
            "Epoch 182/300\n",
            "46/46 - 14s - loss: 0.0117 - val_loss: 0.0126\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00880\n",
            "Epoch 183/300\n",
            "46/46 - 14s - loss: 0.0139 - val_loss: 0.0129\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00880\n",
            "Epoch 184/300\n",
            "46/46 - 14s - loss: 0.0139 - val_loss: 0.0147\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00880\n",
            "Epoch 185/300\n",
            "46/46 - 14s - loss: 0.0138 - val_loss: 0.0174\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00880\n",
            "Epoch 186/300\n",
            "46/46 - 14s - loss: 0.0205 - val_loss: 0.0270\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00880\n",
            "Epoch 187/300\n",
            "46/46 - 14s - loss: 0.0284 - val_loss: 0.0310\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00880\n",
            "Epoch 188/300\n",
            "46/46 - 14s - loss: 0.0413 - val_loss: 0.0319\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00880\n",
            "Epoch 189/300\n",
            "46/46 - 14s - loss: 0.0417 - val_loss: 0.0306\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00880\n",
            "Epoch 190/300\n",
            "46/46 - 14s - loss: 0.0335 - val_loss: 0.0230\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00880\n",
            "Epoch 191/300\n",
            "46/46 - 14s - loss: 0.0255 - val_loss: 0.0142\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00880\n",
            "Epoch 192/300\n",
            "46/46 - 14s - loss: 0.0164 - val_loss: 0.0112\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00880\n",
            "Epoch 193/300\n",
            "46/46 - 14s - loss: 0.0122 - val_loss: 0.0100\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.00880\n",
            "Epoch 194/300\n",
            "46/46 - 14s - loss: 0.0110 - val_loss: 0.0085\n",
            "\n",
            "Epoch 00194: val_loss improved from 0.00880 to 0.00855, saving model to model.h5\n",
            "Epoch 195/300\n",
            "46/46 - 14s - loss: 0.0100 - val_loss: 0.0082\n",
            "\n",
            "Epoch 00195: val_loss improved from 0.00855 to 0.00824, saving model to model.h5\n",
            "Epoch 196/300\n",
            "46/46 - 14s - loss: 0.0099 - val_loss: 0.0088\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.00824\n",
            "Epoch 197/300\n",
            "46/46 - 14s - loss: 0.0095 - val_loss: 0.0077\n",
            "\n",
            "Epoch 00197: val_loss improved from 0.00824 to 0.00775, saving model to model.h5\n",
            "Epoch 198/300\n",
            "46/46 - 14s - loss: 0.0084 - val_loss: 0.0072\n",
            "\n",
            "Epoch 00198: val_loss improved from 0.00775 to 0.00715, saving model to model.h5\n",
            "Epoch 199/300\n",
            "46/46 - 14s - loss: 0.0081 - val_loss: 0.0074\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00715\n",
            "Epoch 200/300\n",
            "46/46 - 14s - loss: 0.0089 - val_loss: 0.0077\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00715\n",
            "Epoch 201/300\n",
            "46/46 - 14s - loss: 0.0100 - val_loss: 0.0086\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.00715\n",
            "Epoch 202/300\n",
            "46/46 - 15s - loss: 0.0093 - val_loss: 0.0104\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.00715\n",
            "Epoch 203/300\n",
            "46/46 - 14s - loss: 0.0087 - val_loss: 0.0075\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.00715\n",
            "Epoch 204/300\n",
            "46/46 - 14s - loss: 0.0087 - val_loss: 0.0069\n",
            "\n",
            "Epoch 00204: val_loss improved from 0.00715 to 0.00694, saving model to model.h5\n",
            "Epoch 205/300\n",
            "46/46 - 14s - loss: 0.0080 - val_loss: 0.0066\n",
            "\n",
            "Epoch 00205: val_loss improved from 0.00694 to 0.00658, saving model to model.h5\n",
            "Epoch 206/300\n",
            "46/46 - 14s - loss: 0.0074 - val_loss: 0.0060\n",
            "\n",
            "Epoch 00206: val_loss improved from 0.00658 to 0.00598, saving model to model.h5\n",
            "Epoch 207/300\n",
            "46/46 - 15s - loss: 0.0068 - val_loss: 0.0056\n",
            "\n",
            "Epoch 00207: val_loss improved from 0.00598 to 0.00562, saving model to model.h5\n",
            "Epoch 208/300\n",
            "46/46 - 14s - loss: 0.0066 - val_loss: 0.0056\n",
            "\n",
            "Epoch 00208: val_loss improved from 0.00562 to 0.00559, saving model to model.h5\n",
            "Epoch 209/300\n",
            "46/46 - 14s - loss: 0.0064 - val_loss: 0.0054\n",
            "\n",
            "Epoch 00209: val_loss improved from 0.00559 to 0.00542, saving model to model.h5\n",
            "Epoch 210/300\n",
            "46/46 - 14s - loss: 0.0065 - val_loss: 0.0055\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.00542\n",
            "Epoch 211/300\n",
            "46/46 - 14s - loss: 0.0069 - val_loss: 0.0057\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.00542\n",
            "Epoch 212/300\n",
            "46/46 - 14s - loss: 0.0071 - val_loss: 0.0065\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.00542\n",
            "Epoch 213/300\n",
            "46/46 - 14s - loss: 0.0078 - val_loss: 0.0085\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.00542\n",
            "Epoch 214/300\n",
            "46/46 - 14s - loss: 0.0090 - val_loss: 0.0088\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.00542\n",
            "Epoch 215/300\n",
            "46/46 - 14s - loss: 0.0100 - val_loss: 0.0099\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.00542\n",
            "Epoch 216/300\n",
            "46/46 - 14s - loss: 0.0111 - val_loss: 0.0091\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.00542\n",
            "Epoch 217/300\n",
            "46/46 - 14s - loss: 0.0141 - val_loss: 0.0086\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.00542\n",
            "Epoch 218/300\n",
            "46/46 - 14s - loss: 0.0097 - val_loss: 0.0082\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.00542\n",
            "Epoch 219/300\n",
            "46/46 - 14s - loss: 0.0118 - val_loss: 0.0084\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.00542\n",
            "Epoch 220/300\n",
            "46/46 - 14s - loss: 0.0113 - val_loss: 0.0125\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.00542\n",
            "Epoch 221/300\n",
            "46/46 - 14s - loss: 0.0167 - val_loss: 0.0164\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.00542\n",
            "Epoch 222/300\n",
            "46/46 - 14s - loss: 0.0231 - val_loss: 0.0195\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.00542\n",
            "Epoch 223/300\n",
            "46/46 - 14s - loss: 0.0276 - val_loss: 0.0247\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.00542\n",
            "Epoch 224/300\n",
            "46/46 - 14s - loss: 0.0303 - val_loss: 0.0281\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.00542\n",
            "Epoch 225/300\n",
            "46/46 - 14s - loss: 0.0305 - val_loss: 0.0203\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.00542\n",
            "Epoch 226/300\n",
            "46/46 - 14s - loss: 0.0222 - val_loss: 0.0143\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.00542\n",
            "Epoch 227/300\n",
            "46/46 - 14s - loss: 0.0174 - val_loss: 0.0115\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.00542\n",
            "Epoch 228/300\n",
            "46/46 - 14s - loss: 0.0143 - val_loss: 0.0092\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.00542\n",
            "Epoch 229/300\n",
            "46/46 - 14s - loss: 0.0112 - val_loss: 0.0082\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.00542\n",
            "Epoch 230/300\n",
            "46/46 - 14s - loss: 0.0086 - val_loss: 0.0060\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.00542\n",
            "Epoch 231/300\n",
            "46/46 - 14s - loss: 0.0082 - val_loss: 0.0069\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.00542\n",
            "Epoch 232/300\n",
            "46/46 - 14s - loss: 0.0077 - val_loss: 0.0075\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.00542\n",
            "Epoch 233/300\n",
            "46/46 - 14s - loss: 0.0084 - val_loss: 0.0092\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.00542\n",
            "Epoch 234/300\n",
            "46/46 - 14s - loss: 0.0092 - val_loss: 0.0064\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.00542\n",
            "Epoch 235/300\n",
            "46/46 - 14s - loss: 0.0074 - val_loss: 0.0055\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.00542\n",
            "Epoch 236/300\n",
            "46/46 - 14s - loss: 0.0065 - val_loss: 0.0052\n",
            "\n",
            "Epoch 00236: val_loss improved from 0.00542 to 0.00518, saving model to model.h5\n",
            "Epoch 237/300\n",
            "46/46 - 14s - loss: 0.0064 - val_loss: 0.0053\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.00518\n",
            "Epoch 238/300\n",
            "46/46 - 14s - loss: 0.0061 - val_loss: 0.0050\n",
            "\n",
            "Epoch 00238: val_loss improved from 0.00518 to 0.00505, saving model to model.h5\n",
            "Epoch 239/300\n",
            "46/46 - 14s - loss: 0.0059 - val_loss: 0.0050\n",
            "\n",
            "Epoch 00239: val_loss improved from 0.00505 to 0.00500, saving model to model.h5\n",
            "Epoch 240/300\n",
            "46/46 - 14s - loss: 0.0058 - val_loss: 0.0050\n",
            "\n",
            "Epoch 00240: val_loss improved from 0.00500 to 0.00500, saving model to model.h5\n",
            "Epoch 241/300\n",
            "46/46 - 14s - loss: 0.0058 - val_loss: 0.0048\n",
            "\n",
            "Epoch 00241: val_loss improved from 0.00500 to 0.00484, saving model to model.h5\n",
            "Epoch 242/300\n",
            "46/46 - 14s - loss: 0.0055 - val_loss: 0.0048\n",
            "\n",
            "Epoch 00242: val_loss improved from 0.00484 to 0.00482, saving model to model.h5\n",
            "Epoch 243/300\n",
            "46/46 - 14s - loss: 0.0055 - val_loss: 0.0047\n",
            "\n",
            "Epoch 00243: val_loss improved from 0.00482 to 0.00467, saving model to model.h5\n",
            "Epoch 244/300\n",
            "46/46 - 14s - loss: 0.0057 - val_loss: 0.0051\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.00467\n",
            "Epoch 245/300\n",
            "46/46 - 14s - loss: 0.0059 - val_loss: 0.0053\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.00467\n",
            "Epoch 246/300\n",
            "46/46 - 14s - loss: 0.0063 - val_loss: 0.0056\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.00467\n",
            "Epoch 247/300\n",
            "46/46 - 14s - loss: 0.0066 - val_loss: 0.0054\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.00467\n",
            "Epoch 248/300\n",
            "46/46 - 14s - loss: 0.0062 - val_loss: 0.0055\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.00467\n",
            "Epoch 249/300\n",
            "46/46 - 14s - loss: 0.0064 - val_loss: 0.0050\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.00467\n",
            "Epoch 250/300\n",
            "46/46 - 14s - loss: 0.0059 - val_loss: 0.0046\n",
            "\n",
            "Epoch 00250: val_loss improved from 0.00467 to 0.00459, saving model to model.h5\n",
            "Epoch 251/300\n",
            "46/46 - 14s - loss: 0.0054 - val_loss: 0.0044\n",
            "\n",
            "Epoch 00251: val_loss improved from 0.00459 to 0.00441, saving model to model.h5\n",
            "Epoch 252/300\n",
            "46/46 - 14s - loss: 0.0053 - val_loss: 0.0045\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.00441\n",
            "Epoch 253/300\n",
            "46/46 - 14s - loss: 0.0057 - val_loss: 0.0053\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.00441\n",
            "Epoch 254/300\n",
            "46/46 - 14s - loss: 0.0059 - val_loss: 0.0051\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.00441\n",
            "Epoch 255/300\n",
            "46/46 - 14s - loss: 0.0064 - val_loss: 0.0059\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.00441\n",
            "Epoch 256/300\n",
            "46/46 - 14s - loss: 0.0069 - val_loss: 0.0081\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.00441\n",
            "Epoch 257/300\n",
            "46/46 - 14s - loss: 0.0084 - val_loss: 0.0078\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.00441\n",
            "Epoch 258/300\n",
            "46/46 - 14s - loss: 0.0097 - val_loss: 0.0088\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.00441\n",
            "Epoch 259/300\n",
            "46/46 - 14s - loss: 0.0124 - val_loss: 0.0103\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.00441\n",
            "Epoch 260/300\n",
            "46/46 - 14s - loss: 0.0151 - val_loss: 0.0119\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.00441\n",
            "Epoch 261/300\n",
            "46/46 - 14s - loss: 0.0172 - val_loss: 0.0124\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.00441\n",
            "Epoch 262/300\n",
            "46/46 - 14s - loss: 0.0185 - val_loss: 0.0156\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.00441\n",
            "Epoch 263/300\n",
            "46/46 - 14s - loss: 0.0186 - val_loss: 0.0158\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.00441\n",
            "Epoch 264/300\n",
            "46/46 - 14s - loss: 0.0172 - val_loss: 0.0115\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.00441\n",
            "Epoch 265/300\n",
            "46/46 - 14s - loss: 0.0138 - val_loss: 0.0114\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.00441\n",
            "Epoch 266/300\n",
            "46/46 - 14s - loss: 0.0116 - val_loss: 0.0078\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.00441\n",
            "Epoch 267/300\n",
            "46/46 - 14s - loss: 0.0095 - val_loss: 0.0071\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.00441\n",
            "Epoch 268/300\n",
            "46/46 - 14s - loss: 0.0077 - val_loss: 0.0058\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.00441\n",
            "Epoch 269/300\n",
            "46/46 - 14s - loss: 0.0069 - val_loss: 0.0055\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.00441\n",
            "Epoch 270/300\n",
            "46/46 - 14s - loss: 0.0060 - val_loss: 0.0046\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.00441\n",
            "Epoch 271/300\n",
            "46/46 - 14s - loss: 0.0055 - val_loss: 0.0044\n",
            "\n",
            "Epoch 00271: val_loss improved from 0.00441 to 0.00439, saving model to model.h5\n",
            "Epoch 272/300\n",
            "46/46 - 14s - loss: 0.0055 - val_loss: 0.0046\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.00439\n",
            "Epoch 273/300\n",
            "46/46 - 14s - loss: 0.0054 - val_loss: 0.0047\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.00439\n",
            "Epoch 274/300\n",
            "46/46 - 14s - loss: 0.0056 - val_loss: 0.0044\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.00439\n",
            "Epoch 275/300\n",
            "46/46 - 14s - loss: 0.0052 - val_loss: 0.0044\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.00439\n",
            "Epoch 276/300\n",
            "46/46 - 14s - loss: 0.0054 - val_loss: 0.0043\n",
            "\n",
            "Epoch 00276: val_loss improved from 0.00439 to 0.00433, saving model to model.h5\n",
            "Epoch 277/300\n",
            "46/46 - 14s - loss: 0.0054 - val_loss: 0.0043\n",
            "\n",
            "Epoch 00277: val_loss improved from 0.00433 to 0.00432, saving model to model.h5\n",
            "Epoch 278/300\n",
            "46/46 - 14s - loss: 0.0058 - val_loss: 0.0064\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.00432\n",
            "Epoch 279/300\n",
            "46/46 - 14s - loss: 0.0066 - val_loss: 0.0056\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.00432\n",
            "Epoch 280/300\n",
            "46/46 - 14s - loss: 0.0063 - val_loss: 0.0048\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.00432\n",
            "Epoch 281/300\n",
            "46/46 - 14s - loss: 0.0056 - val_loss: 0.0045\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.00432\n",
            "Epoch 282/300\n",
            "46/46 - 14s - loss: 0.0055 - val_loss: 0.0047\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.00432\n",
            "Epoch 283/300\n",
            "46/46 - 14s - loss: 0.0056 - val_loss: 0.0042\n",
            "\n",
            "Epoch 00283: val_loss improved from 0.00432 to 0.00425, saving model to model.h5\n",
            "Epoch 284/300\n",
            "46/46 - 14s - loss: 0.0052 - val_loss: 0.0041\n",
            "\n",
            "Epoch 00284: val_loss improved from 0.00425 to 0.00405, saving model to model.h5\n",
            "Epoch 285/300\n",
            "46/46 - 14s - loss: 0.0050 - val_loss: 0.0043\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.00405\n",
            "Epoch 286/300\n",
            "46/46 - 14s - loss: 0.0052 - val_loss: 0.0040\n",
            "\n",
            "Epoch 00286: val_loss improved from 0.00405 to 0.00402, saving model to model.h5\n",
            "Epoch 287/300\n",
            "46/46 - 14s - loss: 0.0050 - val_loss: 0.0040\n",
            "\n",
            "Epoch 00287: val_loss improved from 0.00402 to 0.00400, saving model to model.h5\n",
            "Epoch 288/300\n",
            "46/46 - 14s - loss: 0.0050 - val_loss: 0.0041\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.00400\n",
            "Epoch 289/300\n",
            "46/46 - 14s - loss: 0.0052 - val_loss: 0.0040\n",
            "\n",
            "Epoch 00289: val_loss improved from 0.00400 to 0.00398, saving model to model.h5\n",
            "Epoch 290/300\n",
            "46/46 - 14s - loss: 0.0047 - val_loss: 0.0041\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.00398\n",
            "Epoch 291/300\n",
            "46/46 - 14s - loss: 0.0049 - val_loss: 0.0040\n",
            "\n",
            "Epoch 00291: val_loss improved from 0.00398 to 0.00398, saving model to model.h5\n",
            "Epoch 292/300\n",
            "46/46 - 14s - loss: 0.0048 - val_loss: 0.0043\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.00398\n",
            "Epoch 293/300\n",
            "46/46 - 14s - loss: 0.0051 - val_loss: 0.0043\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.00398\n",
            "Epoch 294/300\n",
            "46/46 - 14s - loss: 0.0054 - val_loss: 0.0039\n",
            "\n",
            "Epoch 00294: val_loss improved from 0.00398 to 0.00388, saving model to model.h5\n",
            "Epoch 295/300\n",
            "46/46 - 14s - loss: 0.0049 - val_loss: 0.0040\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.00388\n",
            "Epoch 296/300\n",
            "46/46 - 14s - loss: 0.0051 - val_loss: 0.0046\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.00388\n",
            "Epoch 297/300\n",
            "46/46 - 14s - loss: 0.0052 - val_loss: 0.0051\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.00388\n",
            "Epoch 298/300\n",
            "46/46 - 14s - loss: 0.0056 - val_loss: 0.0056\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.00388\n",
            "Epoch 299/300\n",
            "46/46 - 14s - loss: 0.0064 - val_loss: 0.0088\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.00388\n",
            "Epoch 300/300\n",
            "46/46 - 14s - loss: 0.0092 - val_loss: 0.0117\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.00388\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0bf5b93128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs8sA_4nk101",
        "outputId": "9a9c5a97-00f0-4012-855d-3e2d0488dbde"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[ไม่มีทาง!], target=[No way!], predicted=[no way]\n",
            "src=[ทอมเป็นมังสวิรัติ], target=[Tom is a vegetarian.], predicted=[tom is a vegetarian]\n",
            "src=[เดินให้เร็วที่สุดเท่าที่เป็นไปได้], target=[Walk as fast as possible.], predicted=[walk as fast as possible]\n",
            "src=[คุณจะอยู่นานไหม?], target=[Are you going to stay long?], predicted=[are you going to stay right]\n",
            "src=[ทอมไม่รู้อะไรเลยเกี่ยวกับบอสตัน], target=[Tom knows nothing about Boston.], predicted=[tom knows nothing to boston]\n",
            "src=[เธอไม่ได้แต่งงาน], target=[She isn't married.], predicted=[she isn't married]\n",
            "src=[คุณจะกลับบ้านเมื่อไหร่?], target=[When will you come home?], predicted=[when will you come home]\n",
            "src=[ลืมทอม], target=[Forget Tom.], predicted=[forget tom]\n",
            "src=[ยิ้ม!], target=[Smile.], predicted=[smile]\n",
            "src=[เขาไม่ชอบเรา], target=[He doesn't like us.], predicted=[he doesn't like us]\n",
            "BLEU-1: 0.517351\n",
            "BLEU-2: 0.446284\n",
            "BLEU-3: 0.410203\n",
            "BLEU-4: 0.298274\n",
            "test\n",
            "src=[คุณสามารถอยู่ได้จนถึงคืนนี้], target=[You can stay till tonight.], predicted=[you can stay till tonight]\n",
            "src=[พวกเขาทำลายชีวิตของฉัน], target=[They ruined my life.], predicted=[they ruined my life]\n",
            "src=[ทอมอาจจะรู้จักแมรี่], target=[Tom might know Mary.], predicted=[tom might know mary]\n",
            "src=[ตื่น], target=[Wake up.], predicted=[wake up]\n",
            "src=[คุณอยากทานอะไร?], target=[What would you like to eat?], predicted=[what would you like to eat]\n",
            "src=[คุณชอบร้องเพลงไหม?], target=[Do you like singing?], predicted=[do you like singing]\n",
            "src=[พี่ชายของฉันไปเป็นพ่อครัว], target=[My brother became a cook.], predicted=[my brother became a cook]\n",
            "src=[จินตนาการของทอมถูกกระตุ้น], target=[Tom's imagination was aroused.], predicted=[tom's imagination was aroused]\n",
            "src=[ฉันอยากเรียนที่ปารีส], target=[I'd like to study in Paris.], predicted=[i'd like to study the paris]\n",
            "src=[ตอนนั้นฉันอยู่ในเขา], target=[I was in the mountains.], predicted=[i was in the mountains]\n",
            "BLEU-1: 0.522020\n",
            "BLEU-2: 0.452521\n",
            "BLEU-3: 0.417103\n",
            "BLEU-4: 0.305574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP9nkmuAllyW",
        "outputId": "deb0740a-e04e-4aa7-e1ef-ba9c0f4c7e09"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณมีแผนจะทำอะไรพรุ่งนี้ไหม?], target=[Do you have any plans for tomorrow?], predicted=[do you have any plans for tomorrow]\n",
            "src=[บ้านของเขาอยู่ใกล้รถไฟใต้ดิน], target=[His house is near the subway.], predicted=[his house is near the subway]\n",
            "src=[หนึ่งศตวรรษคือหนึ่งร้อยปี], target=[A century is one hundred years.], predicted=[a century is one hundred years]\n",
            "src=[กรุณากรอกแบบฟอร์มใบสมัครนี้], target=[Please fill in this application form.], predicted=[please fill in this application form]\n",
            "src=[คุณจะกลับบ้านเมื่อไหร่?], target=[When will you come home?], predicted=[when will you come home]\n",
            "src=[คุณรังเกียจไหมถ้าฉันเปิดประตู?], target=[Do you mind if I open the door?], predicted=[do you mind if i open the door]\n",
            "src=[คุณใช่ลูกสาวของทอมไหม?], target=[Are you Tom's daughter?], predicted=[are you tom's daughter]\n",
            "src=[ฉันจะหาแท็กซี่ได้ที่ไหน?], target=[Where can I get a taxi?], predicted=[where can i get a taxi]\n",
            "src=[เธอขายดอกไม้], target=[She sells flowers.], predicted=[she sells flowers]\n",
            "src=[ไม่มีน้ำ], target=[There is no water.], predicted=[there is no water]\n",
            "BLEU-1: 0.541890\n",
            "BLEU-2: 0.485651\n",
            "BLEU-3: 0.463319\n",
            "BLEU-4: 0.365406\n",
            "test\n",
            "src=[ฉันสงสัยว่าจะแขวนรูปที่ทอมให้มาตรงไหนดี], target=[I wonder where to hang this picture that Tom gave me.], predicted=[i wonder where to hang this picture that tom gave me]\n",
            "src=[ยาสีฟันอยู่ที่ไหน?], target=[Where's the toothpaste?], predicted=[where's the the toothpaste]\n",
            "src=[กระต่ายมีหูยาว], target=[Rabbits have long ears.], predicted=[rabbits have long ears]\n",
            "src=[นั่นคือสิ่งที่โง่ที่สุดที่ฉันเคยพูด], target=[That's the stupidest thing I've ever said.], predicted=[that's the stupidest thing i've ever said]\n",
            "src=[ฉันรู้จักร้านอาหารอิตาเลียนที่ดี], target=[I know a good Italian restaurant.], predicted=[i know a good italian restaurant]\n",
            "src=[อย่าหยุดเขา], target=[Don't stop him.], predicted=[don't stop him]\n",
            "src=[เรากำลังกินแอปเปิ้ล], target=[We're eating apples.], predicted=[we're eating apples]\n",
            "src=[ลองกัดชิมดูสักคำซิ], target=[Take a bite.], predicted=[take a bite]\n",
            "src=[ฉันไม่จำเป็นต้องตอบคำถามของคุณ], target=[I don't have to answer your question.], predicted=[i don't have to answer your question]\n",
            "src=[หนังสือกองนี้ของฉัน ส่วนกองโน้นน่ะของเค้า], target=[These books are mine and those books are his.], predicted=[these books are mine and those books are his]\n",
            "BLEU-1: 0.545384\n",
            "BLEU-2: 0.488889\n",
            "BLEU-3: 0.466125\n",
            "BLEU-4: 0.368639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-03rZBGpHcFa",
        "outputId": "24487e0c-e4c0-4a49-efd3-05a6e81e640b"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณมีแผนจะทำอะไรพรุ่งนี้ไหม?], target=[Do you have any plans for tomorrow?], predicted=[do you have any plans for tomorrow]\n",
            "src=[บ้านของเขาอยู่ใกล้รถไฟใต้ดิน], target=[His house is near the subway.], predicted=[his house is near the subway]\n",
            "src=[หนึ่งศตวรรษคือหนึ่งร้อยปี], target=[A century is one hundred years.], predicted=[a century is one hundred years]\n",
            "src=[กรุณากรอกแบบฟอร์มใบสมัครนี้], target=[Please fill in this application form.], predicted=[please fill in this application form]\n",
            "src=[คุณจะกลับบ้านเมื่อไหร่?], target=[When will you come home?], predicted=[when will you come home]\n",
            "src=[คุณรังเกียจไหมถ้าฉันเปิดประตู?], target=[Do you mind if I open the door?], predicted=[do you mind if i open the door]\n",
            "src=[คุณใช่ลูกสาวของทอมไหม?], target=[Are you Tom's daughter?], predicted=[are you tom's daughter]\n",
            "src=[ฉันจะหาแท็กซี่ได้ที่ไหน?], target=[Where can I get a taxi?], predicted=[where can i get a taxi]\n",
            "src=[เธอขายดอกไม้], target=[She sells flowers.], predicted=[she sells flowers]\n",
            "src=[ไม่มีน้ำ], target=[There is no water.], predicted=[there is no water]\n",
            "BLEU-1: 0.542163\n",
            "BLEU-2: 0.485939\n",
            "BLEU-3: 0.463690\n",
            "BLEU-4: 0.365729\n",
            "test\n",
            "src=[ฉันสงสัยว่าจะแขวนรูปที่ทอมให้มาตรงไหนดี], target=[I wonder where to hang this picture that Tom gave me.], predicted=[i wonder where to hang this picture that tom gave me]\n",
            "src=[ยาสีฟันอยู่ที่ไหน?], target=[Where's the toothpaste?], predicted=[where's the toothpaste]\n",
            "src=[กระต่ายมีหูยาว], target=[Rabbits have long ears.], predicted=[rabbits have long ears]\n",
            "src=[นั่นคือสิ่งที่โง่ที่สุดที่ฉันเคยพูด], target=[That's the stupidest thing I've ever said.], predicted=[that's the stupidest thing i've ever said]\n",
            "src=[ฉันรู้จักร้านอาหารอิตาเลียนที่ดี], target=[I know a good Italian restaurant.], predicted=[i know a good italian restaurant]\n",
            "src=[อย่าหยุดเขา], target=[Don't stop him.], predicted=[don't stop him]\n",
            "src=[เรากำลังกินแอปเปิ้ล], target=[We're eating apples.], predicted=[we're eating apples]\n",
            "src=[ลองกัดชิมดูสักคำซิ], target=[Take a bite.], predicted=[take a bite]\n",
            "src=[ฉันไม่จำเป็นต้องตอบคำถามของคุณ], target=[I don't have to answer your question.], predicted=[i don't have to answer your question]\n",
            "src=[หนังสือกองนี้ของฉัน ส่วนกองโน้นน่ะของเค้า], target=[These books are mine and those books are his.], predicted=[these books are mine and those books are his]\n",
            "BLEU-1: 0.545102\n",
            "BLEU-2: 0.488571\n",
            "BLEU-3: 0.466125\n",
            "BLEU-4: 0.368832\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgxV9Wxi8GQe",
        "outputId": "1a0fcfaa-d004-4ad5-8695-f3cb392ae9c4"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณมีแผนจะทำอะไรพรุ่งนี้ไหม?], target=[Do you have any plans for tomorrow?], predicted=[do you have any plans for tomorrow]\n",
            "src=[บ้านของเขาอยู่ใกล้รถไฟใต้ดิน], target=[His house is near the subway.], predicted=[his house is near the subway]\n",
            "src=[หนึ่งศตวรรษคือหนึ่งร้อยปี], target=[A century is one hundred years.], predicted=[a century is one hundred years]\n",
            "src=[กรุณากรอกแบบฟอร์มใบสมัครนี้], target=[Please fill in this application form.], predicted=[please fill in this application form]\n",
            "src=[คุณจะกลับบ้านเมื่อไหร่?], target=[When will you come home?], predicted=[when will you come home]\n",
            "src=[คุณรังเกียจไหมถ้าฉันเปิดประตู?], target=[Do you mind if I open the door?], predicted=[do you mind if i open the door]\n",
            "src=[คุณใช่ลูกสาวของทอมไหม?], target=[Are you Tom's daughter?], predicted=[are you tom's daughter]\n",
            "src=[ฉันจะหาแท็กซี่ได้ที่ไหน?], target=[Where can I get a taxi?], predicted=[where can i get a taxi]\n",
            "src=[เธอขายดอกไม้], target=[She sells flowers.], predicted=[she sells flowers]\n",
            "src=[ไม่มีน้ำ], target=[There is no water.], predicted=[there is no water]\n",
            "BLEU-1: 0.541890\n",
            "BLEU-2: 0.485651\n",
            "BLEU-3: 0.463319\n",
            "BLEU-4: 0.365406\n",
            "test\n",
            "src=[ฉันสงสัยว่าจะแขวนรูปที่ทอมให้มาตรงไหนดี], target=[I wonder where to hang this picture that Tom gave me.], predicted=[i wonder where to hang this picture that tom gave me]\n",
            "src=[ยาสีฟันอยู่ที่ไหน?], target=[Where's the toothpaste?], predicted=[where's the the toothpaste]\n",
            "src=[กระต่ายมีหูยาว], target=[Rabbits have long ears.], predicted=[rabbits have long ears]\n",
            "src=[นั่นคือสิ่งที่โง่ที่สุดที่ฉันเคยพูด], target=[That's the stupidest thing I've ever said.], predicted=[that's the stupidest thing i've ever said]\n",
            "src=[ฉันรู้จักร้านอาหารอิตาเลียนที่ดี], target=[I know a good Italian restaurant.], predicted=[i know a good italian restaurant]\n",
            "src=[อย่าหยุดเขา], target=[Don't stop him.], predicted=[don't stop him]\n",
            "src=[เรากำลังกินแอปเปิ้ล], target=[We're eating apples.], predicted=[we're eating apples]\n",
            "src=[ลองกัดชิมดูสักคำซิ], target=[Take a bite.], predicted=[take a bite]\n",
            "src=[ฉันไม่จำเป็นต้องตอบคำถามของคุณ], target=[I don't have to answer your question.], predicted=[i don't have to answer your question]\n",
            "src=[หนังสือกองนี้ของฉัน ส่วนกองโน้นน่ะของเค้า], target=[These books are mine and those books are his.], predicted=[these books are mine and those books are his]\n",
            "BLEU-1: 0.545384\n",
            "BLEU-2: 0.488889\n",
            "BLEU-3: 0.466125\n",
            "BLEU-4: 0.368639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiGDAdE-mOiJ",
        "outputId": "0cce8479-8bc4-4003-bd2a-573de8ae4d8b"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,train = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[ไม่มีทาง!], target=[No way!], predicted=[no way]\n",
            "src=[ทอมเป็นมังสวิรัติ], target=[Tom is a vegetarian.], predicted=[tom is a vegetarian]\n",
            "src=[เดินให้เร็วที่สุดเท่าที่เป็นไปได้], target=[Walk as fast as possible.], predicted=[walk as fast as possible]\n",
            "src=[คุณจะอยู่นานไหม?], target=[Are you going to stay long?], predicted=[are you going to stay right]\n",
            "src=[ทอมไม่รู้อะไรเลยเกี่ยวกับบอสตัน], target=[Tom knows nothing about Boston.], predicted=[tom knows nothing to boston]\n",
            "src=[เธอไม่ได้แต่งงาน], target=[She isn't married.], predicted=[she isn't married]\n",
            "src=[คุณจะกลับบ้านเมื่อไหร่?], target=[When will you come home?], predicted=[when will you come home]\n",
            "src=[ลืมทอม], target=[Forget Tom.], predicted=[forget tom]\n",
            "src=[ยิ้ม!], target=[Smile.], predicted=[smile]\n",
            "src=[เขาไม่ชอบเรา], target=[He doesn't like us.], predicted=[he doesn't like us]\n",
            "BLEU-1: 0.517351\n",
            "BLEU-2: 0.446284\n",
            "BLEU-3: 0.410203\n",
            "BLEU-4: 0.298274\n",
            "test\n",
            "src=[คุณสามารถอยู่ได้จนถึงคืนนี้], target=[You can stay till tonight.], predicted=[you can stay till tonight]\n",
            "src=[พวกเขาทำลายชีวิตของฉัน], target=[They ruined my life.], predicted=[they ruined my life]\n",
            "src=[ทอมอาจจะรู้จักแมรี่], target=[Tom might know Mary.], predicted=[tom might know mary]\n",
            "src=[ตื่น], target=[Wake up.], predicted=[wake up]\n",
            "src=[คุณอยากทานอะไร?], target=[What would you like to eat?], predicted=[what would you like to eat]\n",
            "src=[คุณชอบร้องเพลงไหม?], target=[Do you like singing?], predicted=[do you like singing]\n",
            "src=[พี่ชายของฉันไปเป็นพ่อครัว], target=[My brother became a cook.], predicted=[my brother became a cook]\n",
            "src=[จินตนาการของทอมถูกกระตุ้น], target=[Tom's imagination was aroused.], predicted=[tom's imagination was aroused]\n",
            "src=[ฉันอยากเรียนที่ปารีส], target=[I'd like to study in Paris.], predicted=[i'd like to study the paris]\n",
            "src=[ตอนนั้นฉันอยู่ในเขา], target=[I was in the mountains.], predicted=[i was in the mountains]\n",
            "BLEU-1: 0.522020\n",
            "BLEU-2: 0.452521\n",
            "BLEU-3: 0.417103\n",
            "BLEU-4: 0.305574\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}