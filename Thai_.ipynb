{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pbZ0caYfzkF",
        "outputId": "2f531a9e-0148-4e66-e06a-7115f7e50b44"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "\n",
        "# load doc into memory\n",
        "\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# split a loaded document into sentences\n",
        "def to_pairs(doc):\n",
        "\tlines = doc.strip().split('\\n')\n",
        "\tpairs = [line.split('\\t') for line in  lines]\n",
        "\treturn pairs\n",
        "\n",
        "# clean a list of lines\n",
        "def clean_pairs(lines):\n",
        "\tcleaned = list()\n",
        "\t# prepare regex for char filtering\n",
        "\t#re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor pair in lines:\n",
        "\t\tclean_pair = list()\n",
        "\t\tfor line in pair:\n",
        "\t\t\t# normalize unicode characters\n",
        "\t\t\t#line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "\t\t\t#line = line.decode('UTF-8')\n",
        "\t\t\t# tokenize on white space\n",
        "\t\t\tline = line.split()\n",
        "\t\t\t# convert to lowercase\n",
        "\t\t\t#line = [word.lower() for word in line]\n",
        "\t\t\t# remove punctuation from each token\n",
        "\t\t\tline = [word.translate(table) for word in line]\n",
        "\t\t\t# remove non-printable chars form each token\n",
        "\t\t\t#line = [re_print.sub('', w) for w in line]\n",
        "\t\t\t# remove tokens with numbers in them\n",
        "\t\t\t#line = [word for word in line if word.isalpha()]\n",
        "\t\t\t# store as string\n",
        "\t\t\tclean_pair.append(' '.join(line))\n",
        "\t\tcleaned.append(clean_pair)\n",
        "\treturn array(cleaned)\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "filename = '/content/tha.txt'\n",
        "doc = load_doc(filename)\n",
        "# split into english-german pairs\n",
        "pairs = to_pairs(doc)\n",
        "# clean sentences\n",
        "clean_pairs = clean_pairs(pairs)\n",
        "# save clean pairs to file\n",
        "save_clean_data(clean_pairs, 'english-thai.pkl')\n",
        "# spot check\n",
        "for i in range(100):\n",
        "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-thai.pkl\n",
            "[Hi] => [สวัสดี]\n",
            "[Run] => [วิ่ง]\n",
            "[Who] => [ใคร]\n",
            "[Wow] => [ว้าว]\n",
            "[Fire] => [ยิง]\n",
            "[Help] => [ช่วยด้วย]\n",
            "[Jump] => [กระโดด]\n",
            "[Jump] => [กระโดด]\n",
            "[Stop] => [หยุด]\n",
            "[Wait] => [รอก่อน]\n",
            "[Go on] => [ต่อไป]\n",
            "[Hello] => [สวัสดี]\n",
            "[Hurry] => [เร็ว ๆ]\n",
            "[I see] => [ฉันเข้าใจแล้ว]\n",
            "[I try] => [ฉันลองแล้ว]\n",
            "[I won] => [ฉันชนะ]\n",
            "[Oh no] => [ไม่นะ]\n",
            "[Relax] => [พักผ่อน]\n",
            "[Smile] => [ยิ้ม]\n",
            "[Attack] => [โจมตี]\n",
            "[Cheers] => [เชียร์]\n",
            "[Freeze] => [ฟรีส]\n",
            "[Freeze] => [แข็ง]\n",
            "[Get up] => [ตื่น]\n",
            "[Go now] => [ไปเลยตอนนี้]\n",
            "[Got it] => [เข้าใจแล้ว]\n",
            "[Got it] => [จับได้แล้ว]\n",
            "[Got it] => [ทำได้แล้ว]\n",
            "[Got it] => [เข้าใจมั้ย]\n",
            "[He ran] => [เขาวิ่ง]\n",
            "[Hop in] => [กระโดดไป]\n",
            "[Hug me] => [กอดฉัน]\n",
            "[I fell] => [ฉันล้ม]\n",
            "[I left] => [ฉันออก]\n",
            "[I lied] => [ฉันโกหก]\n",
            "[I lost] => [ฉันแพ้]\n",
            "[I quit] => [ฉันออก]\n",
            "[I work] => [ฉันทำงาน]\n",
            "[Im 19] => [ฉันอายุ 19 ปี]\n",
            "[Im OK] => [ฉันโอเค]\n",
            "[Im OK] => [ฉันไม่เป็นไร]\n",
            "[Im up] => [ฉันตื่น]\n",
            "[Listen] => [ฟัง]\n",
            "[Listen] => [ฟังนะ]\n",
            "[No way] => [ไม่มีทาง]\n",
            "[Really] => [จริงเหรอ]\n",
            "[Try it] => [ลองมัน]\n",
            "[We try] => [พวกเราลองแล้ว]\n",
            "[We won] => [เราชนะ]\n",
            "[Why me] => [ทำไมต้องเป็นฉัน]\n",
            "[Ask Tom] => [ถามทอม]\n",
            "[Awesome] => [สุดยอด]\n",
            "[Beat it] => [ทำลายมัน]\n",
            "[Call me] => [โทรหาฉัน]\n",
            "[Come in] => [เข้ามาข้างใน]\n",
            "[Get out] => [ออกไป]\n",
            "[Get out] => [ออกไป]\n",
            "[Go away] => [ไปไกล ๆ]\n",
            "[Go away] => [ไปซะ]\n",
            "[Go away] => [ไปไกล ๆ]\n",
            "[Go home] => [กลับบ้าน]\n",
            "[Goodbye] => [ลาก่อน]\n",
            "[He runs] => [เขาวิ่ง]\n",
            "[Help me] => [ช่วยฉันด้วย]\n",
            "[Help me] => [ช่วยฉัน]\n",
            "[Hi Tom] => [สวัสดีทอม]\n",
            "[Hold it] => [จับมันไว้]\n",
            "[I agree] => [ฉันเห็นด้วย]\n",
            "[Ill go] => [ฉันจะไป]\n",
            "[Im Tom] => [ฉันคือทอม]\n",
            "[Im hot] => [ฉันร้อน]\n",
            "[Im ill] => [ฉันป่วย]\n",
            "[Im shy] => [ฉันอาย]\n",
            "[Its OK] => [มันพอได้]\n",
            "[Its OK] => [มันโอเค]\n",
            "[Its me] => [ฉันเอง]\n",
            "[Me too] => [ฉันด้วยเหมือนกัน]\n",
            "[Open up] => [เปิดออก]\n",
            "[Perfect] => [เยี่ยมยอด]\n",
            "[Perfect] => [พอดีเป๊ะ]\n",
            "[Show me] => [แสดงให้ฉัน]\n",
            "[Shut up] => [เงียบ]\n",
            "[Tell me] => [บอกฉัน]\n",
            "[Tom ran] => [ทอมวิ่ง]\n",
            "[Wake up] => [ตื่น]\n",
            "[Wake up] => [ตื่น]\n",
            "[Wash up] => [ล้าง]\n",
            "[We lost] => [เราแพ้]\n",
            "[Welcome] => [ยินดีต้อนรับ]\n",
            "[Who won] => [ใครชนะ]\n",
            "[You run] => [คุณวิ่ง]\n",
            "[Be a man] => [เป็นลูกผู้ชายหน่อย]\n",
            "[Be quiet] => [เงียบ ๆ]\n",
            "[Call Tom] => [โทรหาทอม]\n",
            "[Cheer up] => [สู้ตาย]\n",
            "[Dont go] => [อย่าไป]\n",
            "[Find Tom] => [หาทอม]\n",
            "[Grab Tom] => [จับทอม]\n",
            "[Grab him] => [จับเขา]\n",
            "[Have fun] => [ขอให้สนุก]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhTkEukQ7ONz",
        "outputId": "cd031a2b-d029-4efa-f0f3-6008ac990eac"
      },
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import rand\n",
        "from numpy.random import shuffle\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# save a list of clean sentences to file\n",
        "def save_clean_data(sentences, filename):\n",
        "\tdump(sentences, open(filename, 'wb'))\n",
        "\tprint('Saved: %s' % filename)\n",
        "\n",
        "# load dataset\n",
        "raw_dataset = load_clean_sentences('english-thai.pkl')\n",
        "\n",
        "# reduce dataset size\n",
        "n_sentences = 2910\n",
        "dataset = raw_dataset[:n_sentences, :]\n",
        "# random shuffle\n",
        "shuffle(dataset)\n",
        "# split into train/test\n",
        "train, test = dataset[:2000], dataset[910:]\n",
        "# save\n",
        "save_clean_data(dataset, 'english-thai-both.pkl')\n",
        "save_clean_data(train, 'english-thai-train.pkl')\n",
        "save_clean_data(test, 'english-thai-test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved: english-thai-both.pkl\n",
            "Saved: english-thai-train.pkl\n",
            "Saved: english-thai-test.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umixN0CZ7SpA",
        "outputId": "6f2af629-f92b-433f-b8b4-e35625043b6a"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Thai Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Thai Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 1977\n",
            "English Max Length: 20\n",
            "Thai Vocabulary Size: 2971\n",
            "Thai Max Length: 6\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 6, 256)            760576    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 256)               525312    \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 20, 256)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 20, 256)           525312    \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 20, 1977)          508089    \n",
            "=================================================================\n",
            "Total params: 2,319,289\n",
            "Trainable params: 2,319,289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "32/32 - 21s - loss: 4.1773 - val_loss: 1.9782\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.97822, saving model to model.h5\n",
            "Epoch 2/100\n",
            "32/32 - 13s - loss: 1.9085 - val_loss: 1.8363\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.97822 to 1.83632, saving model to model.h5\n",
            "Epoch 3/100\n",
            "32/32 - 13s - loss: 1.7983 - val_loss: 1.8405\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.83632\n",
            "Epoch 4/100\n",
            "32/32 - 13s - loss: 1.7782 - val_loss: 1.8495\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.83632\n",
            "Epoch 5/100\n",
            "32/32 - 13s - loss: 1.8135 - val_loss: 1.7696\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.83632 to 1.76958, saving model to model.h5\n",
            "Epoch 6/100\n",
            "32/32 - 13s - loss: 1.7102 - val_loss: 1.7207\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.76958 to 1.72072, saving model to model.h5\n",
            "Epoch 7/100\n",
            "32/32 - 13s - loss: 1.6529 - val_loss: 1.6911\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.72072 to 1.69112, saving model to model.h5\n",
            "Epoch 8/100\n",
            "32/32 - 13s - loss: 1.6070 - val_loss: 1.6590\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.69112 to 1.65903, saving model to model.h5\n",
            "Epoch 9/100\n",
            "32/32 - 13s - loss: 1.5560 - val_loss: 1.6329\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.65903 to 1.63285, saving model to model.h5\n",
            "Epoch 10/100\n",
            "32/32 - 13s - loss: 1.5142 - val_loss: 1.6100\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.63285 to 1.60998, saving model to model.h5\n",
            "Epoch 11/100\n",
            "32/32 - 13s - loss: 1.4775 - val_loss: 1.5891\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.60998 to 1.58906, saving model to model.h5\n",
            "Epoch 12/100\n",
            "32/32 - 13s - loss: 1.4462 - val_loss: 1.5765\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.58906 to 1.57651, saving model to model.h5\n",
            "Epoch 13/100\n",
            "32/32 - 13s - loss: 1.4185 - val_loss: 1.5527\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.57651 to 1.55271, saving model to model.h5\n",
            "Epoch 14/100\n",
            "32/32 - 13s - loss: 1.3817 - val_loss: 1.5461\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.55271 to 1.54615, saving model to model.h5\n",
            "Epoch 15/100\n",
            "32/32 - 13s - loss: 1.3593 - val_loss: 1.5275\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.54615 to 1.52751, saving model to model.h5\n",
            "Epoch 16/100\n",
            "32/32 - 13s - loss: 1.3258 - val_loss: 1.5297\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.52751\n",
            "Epoch 17/100\n",
            "32/32 - 13s - loss: 1.3111 - val_loss: 1.5204\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.52751 to 1.52040, saving model to model.h5\n",
            "Epoch 18/100\n",
            "32/32 - 13s - loss: 1.2897 - val_loss: 1.5168\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.52040 to 1.51676, saving model to model.h5\n",
            "Epoch 19/100\n",
            "32/32 - 13s - loss: 1.2767 - val_loss: 1.5164\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.51676 to 1.51637, saving model to model.h5\n",
            "Epoch 20/100\n",
            "32/32 - 13s - loss: 1.2641 - val_loss: 1.5154\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.51637 to 1.51540, saving model to model.h5\n",
            "Epoch 21/100\n",
            "32/32 - 13s - loss: 1.2478 - val_loss: 1.5103\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.51540 to 1.51029, saving model to model.h5\n",
            "Epoch 22/100\n",
            "32/32 - 13s - loss: 1.2326 - val_loss: 1.5095\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.51029 to 1.50950, saving model to model.h5\n",
            "Epoch 23/100\n",
            "32/32 - 13s - loss: 1.2207 - val_loss: 1.5121\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.50950\n",
            "Epoch 24/100\n",
            "32/32 - 13s - loss: 1.2067 - val_loss: 1.5071\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.50950 to 1.50707, saving model to model.h5\n",
            "Epoch 25/100\n",
            "32/32 - 13s - loss: 1.1955 - val_loss: 1.5134\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.50707\n",
            "Epoch 26/100\n",
            "32/32 - 13s - loss: 1.1863 - val_loss: 1.5142\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.50707\n",
            "Epoch 27/100\n",
            "32/32 - 13s - loss: 1.1757 - val_loss: 1.5081\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.50707\n",
            "Epoch 28/100\n",
            "32/32 - 13s - loss: 1.1620 - val_loss: 1.5143\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.50707\n",
            "Epoch 29/100\n",
            "32/32 - 13s - loss: 1.1498 - val_loss: 1.5151\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.50707\n",
            "Epoch 30/100\n",
            "32/32 - 13s - loss: 1.1405 - val_loss: 1.5080\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.50707\n",
            "Epoch 31/100\n",
            "32/32 - 13s - loss: 1.1298 - val_loss: 1.5169\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.50707\n",
            "Epoch 32/100\n",
            "32/32 - 13s - loss: 1.1223 - val_loss: 1.5138\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.50707\n",
            "Epoch 33/100\n",
            "32/32 - 13s - loss: 1.1094 - val_loss: 1.5155\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.50707\n",
            "Epoch 34/100\n",
            "32/32 - 13s - loss: 1.0984 - val_loss: 1.5156\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.50707\n",
            "Epoch 35/100\n",
            "32/32 - 13s - loss: 1.0875 - val_loss: 1.5167\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.50707\n",
            "Epoch 36/100\n",
            "32/32 - 13s - loss: 1.0779 - val_loss: 1.5172\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.50707\n",
            "Epoch 37/100\n",
            "32/32 - 13s - loss: 1.0696 - val_loss: 1.5209\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.50707\n",
            "Epoch 38/100\n",
            "32/32 - 14s - loss: 1.0609 - val_loss: 1.5214\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.50707\n",
            "Epoch 39/100\n",
            "32/32 - 14s - loss: 1.0512 - val_loss: 1.5223\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.50707\n",
            "Epoch 40/100\n",
            "32/32 - 14s - loss: 1.0366 - val_loss: 1.5174\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.50707\n",
            "Epoch 41/100\n",
            "32/32 - 14s - loss: 1.0252 - val_loss: 1.5218\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.50707\n",
            "Epoch 42/100\n",
            "32/32 - 13s - loss: 1.0127 - val_loss: 1.5167\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.50707\n",
            "Epoch 43/100\n",
            "32/32 - 14s - loss: 1.0010 - val_loss: 1.5170\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.50707\n",
            "Epoch 44/100\n",
            "32/32 - 13s - loss: 0.9933 - val_loss: 1.5152\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.50707\n",
            "Epoch 45/100\n",
            "32/32 - 13s - loss: 0.9889 - val_loss: 1.5413\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.50707\n",
            "Epoch 46/100\n",
            "32/32 - 13s - loss: 0.9793 - val_loss: 1.5180\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.50707\n",
            "Epoch 47/100\n",
            "32/32 - 13s - loss: 0.9638 - val_loss: 1.5224\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.50707\n",
            "Epoch 48/100\n",
            "32/32 - 13s - loss: 0.9481 - val_loss: 1.5153\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.50707\n",
            "Epoch 49/100\n",
            "32/32 - 13s - loss: 0.9340 - val_loss: 1.5177\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.50707\n",
            "Epoch 50/100\n",
            "32/32 - 13s - loss: 0.9202 - val_loss: 1.5106\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.50707\n",
            "Epoch 51/100\n",
            "32/32 - 13s - loss: 0.9069 - val_loss: 1.5075\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.50707\n",
            "Epoch 52/100\n",
            "32/32 - 13s - loss: 0.8932 - val_loss: 1.5043\n",
            "\n",
            "Epoch 00052: val_loss improved from 1.50707 to 1.50429, saving model to model.h5\n",
            "Epoch 53/100\n",
            "32/32 - 13s - loss: 0.8806 - val_loss: 1.5071\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.50429\n",
            "Epoch 54/100\n",
            "32/32 - 13s - loss: 0.8690 - val_loss: 1.5049\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.50429\n",
            "Epoch 55/100\n",
            "32/32 - 13s - loss: 0.8567 - val_loss: 1.5046\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.50429\n",
            "Epoch 56/100\n",
            "32/32 - 13s - loss: 0.8420 - val_loss: 1.5025\n",
            "\n",
            "Epoch 00056: val_loss improved from 1.50429 to 1.50249, saving model to model.h5\n",
            "Epoch 57/100\n",
            "32/32 - 13s - loss: 0.8298 - val_loss: 1.5035\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.50249\n",
            "Epoch 58/100\n",
            "32/32 - 13s - loss: 0.8206 - val_loss: 1.5083\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.50249\n",
            "Epoch 59/100\n",
            "32/32 - 13s - loss: 0.8071 - val_loss: 1.4949\n",
            "\n",
            "Epoch 00059: val_loss improved from 1.50249 to 1.49492, saving model to model.h5\n",
            "Epoch 60/100\n",
            "32/32 - 13s - loss: 0.7976 - val_loss: 1.4966\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.49492\n",
            "Epoch 61/100\n",
            "32/32 - 13s - loss: 0.7831 - val_loss: 1.4897\n",
            "\n",
            "Epoch 00061: val_loss improved from 1.49492 to 1.48969, saving model to model.h5\n",
            "Epoch 62/100\n",
            "32/32 - 14s - loss: 0.7656 - val_loss: 1.4931\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.48969\n",
            "Epoch 63/100\n",
            "32/32 - 13s - loss: 0.7505 - val_loss: 1.4909\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.48969\n",
            "Epoch 64/100\n",
            "32/32 - 14s - loss: 0.7359 - val_loss: 1.4742\n",
            "\n",
            "Epoch 00064: val_loss improved from 1.48969 to 1.47424, saving model to model.h5\n",
            "Epoch 65/100\n",
            "32/32 - 13s - loss: 0.7217 - val_loss: 1.4768\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.47424\n",
            "Epoch 66/100\n",
            "32/32 - 13s - loss: 0.7070 - val_loss: 1.4726\n",
            "\n",
            "Epoch 00066: val_loss improved from 1.47424 to 1.47258, saving model to model.h5\n",
            "Epoch 67/100\n",
            "32/32 - 13s - loss: 0.6910 - val_loss: 1.4676\n",
            "\n",
            "Epoch 00067: val_loss improved from 1.47258 to 1.46757, saving model to model.h5\n",
            "Epoch 68/100\n",
            "32/32 - 13s - loss: 0.6720 - val_loss: 1.4563\n",
            "\n",
            "Epoch 00068: val_loss improved from 1.46757 to 1.45634, saving model to model.h5\n",
            "Epoch 69/100\n",
            "32/32 - 13s - loss: 0.6543 - val_loss: 1.4537\n",
            "\n",
            "Epoch 00069: val_loss improved from 1.45634 to 1.45369, saving model to model.h5\n",
            "Epoch 70/100\n",
            "32/32 - 13s - loss: 0.6351 - val_loss: 1.4410\n",
            "\n",
            "Epoch 00070: val_loss improved from 1.45369 to 1.44104, saving model to model.h5\n",
            "Epoch 71/100\n",
            "32/32 - 13s - loss: 0.6190 - val_loss: 1.4470\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.44104\n",
            "Epoch 72/100\n",
            "32/32 - 13s - loss: 0.6058 - val_loss: 1.4487\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.44104\n",
            "Epoch 73/100\n",
            "32/32 - 13s - loss: 0.5902 - val_loss: 1.4401\n",
            "\n",
            "Epoch 00073: val_loss improved from 1.44104 to 1.44008, saving model to model.h5\n",
            "Epoch 74/100\n",
            "32/32 - 13s - loss: 0.5749 - val_loss: 1.4400\n",
            "\n",
            "Epoch 00074: val_loss improved from 1.44008 to 1.43996, saving model to model.h5\n",
            "Epoch 75/100\n",
            "32/32 - 13s - loss: 0.5589 - val_loss: 1.4255\n",
            "\n",
            "Epoch 00075: val_loss improved from 1.43996 to 1.42552, saving model to model.h5\n",
            "Epoch 76/100\n",
            "32/32 - 13s - loss: 0.5461 - val_loss: 1.4454\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.42552\n",
            "Epoch 77/100\n",
            "32/32 - 13s - loss: 0.5329 - val_loss: 1.4236\n",
            "\n",
            "Epoch 00077: val_loss improved from 1.42552 to 1.42360, saving model to model.h5\n",
            "Epoch 78/100\n",
            "32/32 - 13s - loss: 0.5148 - val_loss: 1.4164\n",
            "\n",
            "Epoch 00078: val_loss improved from 1.42360 to 1.41638, saving model to model.h5\n",
            "Epoch 79/100\n",
            "32/32 - 13s - loss: 0.4982 - val_loss: 1.4225\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.41638\n",
            "Epoch 80/100\n",
            "32/32 - 13s - loss: 0.4852 - val_loss: 1.4119\n",
            "\n",
            "Epoch 00080: val_loss improved from 1.41638 to 1.41193, saving model to model.h5\n",
            "Epoch 81/100\n",
            "32/32 - 13s - loss: 0.4696 - val_loss: 1.4196\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.41193\n",
            "Epoch 82/100\n",
            "32/32 - 13s - loss: 0.4547 - val_loss: 1.4097\n",
            "\n",
            "Epoch 00082: val_loss improved from 1.41193 to 1.40972, saving model to model.h5\n",
            "Epoch 83/100\n",
            "32/32 - 13s - loss: 0.4402 - val_loss: 1.4206\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.40972\n",
            "Epoch 84/100\n",
            "32/32 - 13s - loss: 0.4268 - val_loss: 1.3973\n",
            "\n",
            "Epoch 00084: val_loss improved from 1.40972 to 1.39726, saving model to model.h5\n",
            "Epoch 85/100\n",
            "32/32 - 13s - loss: 0.4151 - val_loss: 1.4015\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.39726\n",
            "Epoch 86/100\n",
            "32/32 - 13s - loss: 0.4006 - val_loss: 1.3992\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.39726\n",
            "Epoch 87/100\n",
            "32/32 - 13s - loss: 0.3857 - val_loss: 1.3993\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.39726\n",
            "Epoch 88/100\n",
            "32/32 - 13s - loss: 0.3696 - val_loss: 1.3922\n",
            "\n",
            "Epoch 00088: val_loss improved from 1.39726 to 1.39221, saving model to model.h5\n",
            "Epoch 89/100\n",
            "32/32 - 13s - loss: 0.3569 - val_loss: 1.3959\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.39221\n",
            "Epoch 90/100\n",
            "32/32 - 13s - loss: 0.3463 - val_loss: 1.4021\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.39221\n",
            "Epoch 91/100\n",
            "32/32 - 13s - loss: 0.3362 - val_loss: 1.3927\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.39221\n",
            "Epoch 92/100\n",
            "32/32 - 13s - loss: 0.3209 - val_loss: 1.3833\n",
            "\n",
            "Epoch 00092: val_loss improved from 1.39221 to 1.38331, saving model to model.h5\n",
            "Epoch 93/100\n",
            "32/32 - 13s - loss: 0.3075 - val_loss: 1.3925\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.38331\n",
            "Epoch 94/100\n",
            "32/32 - 13s - loss: 0.2994 - val_loss: 1.3872\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.38331\n",
            "Epoch 95/100\n",
            "32/32 - 13s - loss: 0.2901 - val_loss: 1.4025\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.38331\n",
            "Epoch 96/100\n",
            "32/32 - 13s - loss: 0.2772 - val_loss: 1.3951\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.38331\n",
            "Epoch 97/100\n",
            "32/32 - 13s - loss: 0.2693 - val_loss: 1.4032\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.38331\n",
            "Epoch 98/100\n",
            "32/32 - 13s - loss: 0.2549 - val_loss: 1.3889\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.38331\n",
            "Epoch 99/100\n",
            "32/32 - 13s - loss: 0.2425 - val_loss: 1.3982\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.38331\n",
            "Epoch 100/100\n",
            "32/32 - 13s - loss: 0.2339 - val_loss: 1.3924\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.38331\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d73172940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbC8LgAXROUm",
        "outputId": "df2866bf-9e58-459a-db3e-11cf7d0d8fee"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Thai Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Thai Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 512)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 1977\n",
            "English Max Length: 20\n",
            "Thai Vocabulary Size: 2971\n",
            "Thai Max Length: 6\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 6, 512)            1521152   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 512)               2099200   \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 20, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 20, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 20, 1977)          1014201   \n",
            "=================================================================\n",
            "Total params: 6,733,753\n",
            "Trainable params: 6,733,753\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "32/32 - 45s - loss: 3.4421 - val_loss: 1.9997\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.99967, saving model to model.h5\n",
            "Epoch 2/100\n",
            "32/32 - 39s - loss: 1.8990 - val_loss: 1.9408\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.99967 to 1.94077, saving model to model.h5\n",
            "Epoch 3/100\n",
            "32/32 - 39s - loss: 1.8596 - val_loss: 2.0715\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 1.94077\n",
            "Epoch 4/100\n",
            "32/32 - 39s - loss: 1.8883 - val_loss: 1.7455\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.94077 to 1.74551, saving model to model.h5\n",
            "Epoch 5/100\n",
            "32/32 - 39s - loss: 1.6886 - val_loss: 1.6930\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.74551 to 1.69296, saving model to model.h5\n",
            "Epoch 6/100\n",
            "32/32 - 39s - loss: 1.5993 - val_loss: 1.6529\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.69296 to 1.65290, saving model to model.h5\n",
            "Epoch 7/100\n",
            "32/32 - 39s - loss: 1.5353 - val_loss: 1.6214\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.65290 to 1.62137, saving model to model.h5\n",
            "Epoch 8/100\n",
            "32/32 - 39s - loss: 1.4767 - val_loss: 1.5866\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.62137 to 1.58656, saving model to model.h5\n",
            "Epoch 9/100\n",
            "32/32 - 39s - loss: 1.4169 - val_loss: 1.5536\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.58656 to 1.55359, saving model to model.h5\n",
            "Epoch 10/100\n",
            "32/32 - 39s - loss: 1.3603 - val_loss: 1.5286\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.55359 to 1.52862, saving model to model.h5\n",
            "Epoch 11/100\n",
            "32/32 - 39s - loss: 1.3346 - val_loss: 1.5121\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.52862 to 1.51214, saving model to model.h5\n",
            "Epoch 12/100\n",
            "32/32 - 39s - loss: 1.2947 - val_loss: 1.5120\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.51214 to 1.51199, saving model to model.h5\n",
            "Epoch 13/100\n",
            "32/32 - 39s - loss: 1.2666 - val_loss: 1.4998\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.51199 to 1.49983, saving model to model.h5\n",
            "Epoch 14/100\n",
            "32/32 - 39s - loss: 1.2462 - val_loss: 1.4981\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.49983 to 1.49811, saving model to model.h5\n",
            "Epoch 15/100\n",
            "32/32 - 39s - loss: 1.2286 - val_loss: 1.4981\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.49811\n",
            "Epoch 16/100\n",
            "32/32 - 39s - loss: 1.2196 - val_loss: 1.4965\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.49811 to 1.49651, saving model to model.h5\n",
            "Epoch 17/100\n",
            "32/32 - 39s - loss: 1.2073 - val_loss: 1.4973\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.49651\n",
            "Epoch 18/100\n",
            "32/32 - 38s - loss: 1.1983 - val_loss: 1.5038\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 1.49651\n",
            "Epoch 19/100\n",
            "32/32 - 39s - loss: 1.1840 - val_loss: 1.4915\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.49651 to 1.49150, saving model to model.h5\n",
            "Epoch 20/100\n",
            "32/32 - 39s - loss: 1.1658 - val_loss: 1.4923\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.49150\n",
            "Epoch 21/100\n",
            "32/32 - 39s - loss: 1.1487 - val_loss: 1.4936\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 1.49150\n",
            "Epoch 22/100\n",
            "32/32 - 39s - loss: 1.1351 - val_loss: 1.4956\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 1.49150\n",
            "Epoch 23/100\n",
            "32/32 - 39s - loss: 1.1249 - val_loss: 1.4965\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 1.49150\n",
            "Epoch 24/100\n",
            "32/32 - 39s - loss: 1.1139 - val_loss: 1.5081\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.49150\n",
            "Epoch 25/100\n",
            "32/32 - 38s - loss: 1.1023 - val_loss: 1.5003\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 1.49150\n",
            "Epoch 26/100\n",
            "32/32 - 39s - loss: 1.0889 - val_loss: 1.4967\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.49150\n",
            "Epoch 27/100\n",
            "32/32 - 38s - loss: 1.0761 - val_loss: 1.5103\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 1.49150\n",
            "Epoch 28/100\n",
            "32/32 - 39s - loss: 1.0640 - val_loss: 1.5043\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.49150\n",
            "Epoch 29/100\n",
            "32/32 - 39s - loss: 1.0481 - val_loss: 1.4989\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.49150\n",
            "Epoch 30/100\n",
            "32/32 - 39s - loss: 1.0345 - val_loss: 1.5062\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.49150\n",
            "Epoch 31/100\n",
            "32/32 - 39s - loss: 1.0225 - val_loss: 1.5086\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.49150\n",
            "Epoch 32/100\n",
            "32/32 - 39s - loss: 1.0009 - val_loss: 1.5042\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.49150\n",
            "Epoch 33/100\n",
            "32/32 - 38s - loss: 0.9863 - val_loss: 1.5116\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.49150\n",
            "Epoch 34/100\n",
            "32/32 - 39s - loss: 0.9709 - val_loss: 1.5088\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.49150\n",
            "Epoch 35/100\n",
            "32/32 - 39s - loss: 0.9547 - val_loss: 1.5027\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.49150\n",
            "Epoch 36/100\n",
            "32/32 - 39s - loss: 0.9394 - val_loss: 1.5010\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.49150\n",
            "Epoch 37/100\n",
            "32/32 - 39s - loss: 0.9217 - val_loss: 1.4922\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.49150\n",
            "Epoch 38/100\n",
            "32/32 - 39s - loss: 0.8970 - val_loss: 1.4923\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.49150\n",
            "Epoch 39/100\n",
            "32/32 - 39s - loss: 0.8733 - val_loss: 1.4872\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.49150 to 1.48720, saving model to model.h5\n",
            "Epoch 40/100\n",
            "32/32 - 39s - loss: 0.8501 - val_loss: 1.4811\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.48720 to 1.48111, saving model to model.h5\n",
            "Epoch 41/100\n",
            "32/32 - 38s - loss: 0.8274 - val_loss: 1.4813\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.48111\n",
            "Epoch 42/100\n",
            "32/32 - 39s - loss: 0.8058 - val_loss: 1.4756\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.48111 to 1.47564, saving model to model.h5\n",
            "Epoch 43/100\n",
            "32/32 - 39s - loss: 0.7815 - val_loss: 1.4710\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.47564 to 1.47103, saving model to model.h5\n",
            "Epoch 44/100\n",
            "32/32 - 42s - loss: 0.7574 - val_loss: 1.4615\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.47103 to 1.46147, saving model to model.h5\n",
            "Epoch 45/100\n",
            "32/32 - 39s - loss: 0.7343 - val_loss: 1.4679\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.46147\n",
            "Epoch 46/100\n",
            "32/32 - 39s - loss: 0.7145 - val_loss: 1.4625\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.46147\n",
            "Epoch 47/100\n",
            "32/32 - 38s - loss: 0.6928 - val_loss: 1.4646\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.46147\n",
            "Epoch 48/100\n",
            "32/32 - 38s - loss: 0.6723 - val_loss: 1.4571\n",
            "\n",
            "Epoch 00048: val_loss improved from 1.46147 to 1.45708, saving model to model.h5\n",
            "Epoch 49/100\n",
            "32/32 - 39s - loss: 0.6554 - val_loss: 1.4612\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.45708\n",
            "Epoch 50/100\n",
            "32/32 - 39s - loss: 0.6412 - val_loss: 1.4430\n",
            "\n",
            "Epoch 00050: val_loss improved from 1.45708 to 1.44303, saving model to model.h5\n",
            "Epoch 51/100\n",
            "32/32 - 38s - loss: 0.6101 - val_loss: 1.4380\n",
            "\n",
            "Epoch 00051: val_loss improved from 1.44303 to 1.43800, saving model to model.h5\n",
            "Epoch 52/100\n",
            "32/32 - 39s - loss: 0.5823 - val_loss: 1.4333\n",
            "\n",
            "Epoch 00052: val_loss improved from 1.43800 to 1.43326, saving model to model.h5\n",
            "Epoch 53/100\n",
            "32/32 - 39s - loss: 0.5595 - val_loss: 1.4325\n",
            "\n",
            "Epoch 00053: val_loss improved from 1.43326 to 1.43245, saving model to model.h5\n",
            "Epoch 54/100\n",
            "32/32 - 39s - loss: 0.5376 - val_loss: 1.4243\n",
            "\n",
            "Epoch 00054: val_loss improved from 1.43245 to 1.42430, saving model to model.h5\n",
            "Epoch 55/100\n",
            "32/32 - 39s - loss: 0.5178 - val_loss: 1.4254\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.42430\n",
            "Epoch 56/100\n",
            "32/32 - 39s - loss: 0.4951 - val_loss: 1.4165\n",
            "\n",
            "Epoch 00056: val_loss improved from 1.42430 to 1.41651, saving model to model.h5\n",
            "Epoch 57/100\n",
            "32/32 - 39s - loss: 0.4739 - val_loss: 1.4213\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.41651\n",
            "Epoch 58/100\n",
            "32/32 - 39s - loss: 0.4525 - val_loss: 1.4121\n",
            "\n",
            "Epoch 00058: val_loss improved from 1.41651 to 1.41213, saving model to model.h5\n",
            "Epoch 59/100\n",
            "32/32 - 42s - loss: 0.4301 - val_loss: 1.4054\n",
            "\n",
            "Epoch 00059: val_loss improved from 1.41213 to 1.40544, saving model to model.h5\n",
            "Epoch 60/100\n",
            "32/32 - 39s - loss: 0.4123 - val_loss: 1.4100\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.40544\n",
            "Epoch 61/100\n",
            "32/32 - 39s - loss: 0.3951 - val_loss: 1.4055\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.40544\n",
            "Epoch 62/100\n",
            "32/32 - 39s - loss: 0.3736 - val_loss: 1.3949\n",
            "\n",
            "Epoch 00062: val_loss improved from 1.40544 to 1.39487, saving model to model.h5\n",
            "Epoch 63/100\n",
            "32/32 - 39s - loss: 0.3500 - val_loss: 1.3997\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.39487\n",
            "Epoch 64/100\n",
            "32/32 - 40s - loss: 0.3304 - val_loss: 1.3965\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.39487\n",
            "Epoch 65/100\n",
            "32/32 - 40s - loss: 0.3120 - val_loss: 1.3986\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.39487\n",
            "Epoch 66/100\n",
            "32/32 - 39s - loss: 0.2926 - val_loss: 1.3917\n",
            "\n",
            "Epoch 00066: val_loss improved from 1.39487 to 1.39167, saving model to model.h5\n",
            "Epoch 67/100\n",
            "32/32 - 39s - loss: 0.2787 - val_loss: 1.3953\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.39167\n",
            "Epoch 68/100\n",
            "32/32 - 39s - loss: 0.2660 - val_loss: 1.3944\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.39167\n",
            "Epoch 69/100\n",
            "32/32 - 39s - loss: 0.2517 - val_loss: 1.3925\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.39167\n",
            "Epoch 70/100\n",
            "32/32 - 39s - loss: 0.2359 - val_loss: 1.3906\n",
            "\n",
            "Epoch 00070: val_loss improved from 1.39167 to 1.39064, saving model to model.h5\n",
            "Epoch 71/100\n",
            "32/32 - 39s - loss: 0.2223 - val_loss: 1.3919\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.39064\n",
            "Epoch 72/100\n",
            "32/32 - 39s - loss: 0.2075 - val_loss: 1.3935\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.39064\n",
            "Epoch 73/100\n",
            "32/32 - 39s - loss: 0.1946 - val_loss: 1.3994\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.39064\n",
            "Epoch 74/100\n",
            "32/32 - 39s - loss: 0.1850 - val_loss: 1.3944\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.39064\n",
            "Epoch 75/100\n",
            "32/32 - 41s - loss: 0.1753 - val_loss: 1.3959\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.39064\n",
            "Epoch 76/100\n",
            "32/32 - 39s - loss: 0.1659 - val_loss: 1.3946\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.39064\n",
            "Epoch 77/100\n",
            "32/32 - 39s - loss: 0.1578 - val_loss: 1.3960\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.39064\n",
            "Epoch 78/100\n",
            "32/32 - 39s - loss: 0.1452 - val_loss: 1.3917\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.39064\n",
            "Epoch 79/100\n",
            "32/32 - 39s - loss: 0.1340 - val_loss: 1.3947\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 1.39064\n",
            "Epoch 80/100\n",
            "32/32 - 40s - loss: 0.1261 - val_loss: 1.3968\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.39064\n",
            "Epoch 81/100\n",
            "32/32 - 39s - loss: 0.1162 - val_loss: 1.3972\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.39064\n",
            "Epoch 82/100\n",
            "32/32 - 39s - loss: 0.1083 - val_loss: 1.3964\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.39064\n",
            "Epoch 83/100\n",
            "32/32 - 39s - loss: 0.1004 - val_loss: 1.4020\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.39064\n",
            "Epoch 84/100\n",
            "32/32 - 39s - loss: 0.0962 - val_loss: 1.4057\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.39064\n",
            "Epoch 85/100\n",
            "32/32 - 39s - loss: 0.0890 - val_loss: 1.4068\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.39064\n",
            "Epoch 86/100\n",
            "32/32 - 38s - loss: 0.0847 - val_loss: 1.4104\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.39064\n",
            "Epoch 87/100\n",
            "32/32 - 39s - loss: 0.0788 - val_loss: 1.4088\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.39064\n",
            "Epoch 88/100\n",
            "32/32 - 38s - loss: 0.0729 - val_loss: 1.4100\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.39064\n",
            "Epoch 89/100\n",
            "32/32 - 39s - loss: 0.0671 - val_loss: 1.4122\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 1.39064\n",
            "Epoch 90/100\n",
            "32/32 - 41s - loss: 0.0639 - val_loss: 1.4139\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.39064\n",
            "Epoch 91/100\n",
            "32/32 - 39s - loss: 0.0621 - val_loss: 1.4170\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 1.39064\n",
            "Epoch 92/100\n",
            "32/32 - 39s - loss: 0.0589 - val_loss: 1.4179\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.39064\n",
            "Epoch 93/100\n",
            "32/32 - 39s - loss: 0.0563 - val_loss: 1.4237\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.39064\n",
            "Epoch 94/100\n",
            "32/32 - 39s - loss: 0.0536 - val_loss: 1.4256\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.39064\n",
            "Epoch 95/100\n",
            "32/32 - 39s - loss: 0.0492 - val_loss: 1.4224\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.39064\n",
            "Epoch 96/100\n",
            "32/32 - 38s - loss: 0.0447 - val_loss: 1.4243\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 1.39064\n",
            "Epoch 97/100\n",
            "32/32 - 38s - loss: 0.0434 - val_loss: 1.4287\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.39064\n",
            "Epoch 98/100\n",
            "32/32 - 38s - loss: 0.0413 - val_loss: 1.4275\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.39064\n",
            "Epoch 99/100\n",
            "32/32 - 39s - loss: 0.0383 - val_loss: 1.4339\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.39064\n",
            "Epoch 100/100\n",
            "32/32 - 39s - loss: 0.0361 - val_loss: 1.4365\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.39064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d60c87c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5KNnJ7CyiQA",
        "outputId": "2e956bae-9389-427f-fc7d-03d307c3f6c8"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Thai Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Thai Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 128)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 1977\n",
            "English Max Length: 20\n",
            "Thai Vocabulary Size: 2971\n",
            "Thai Max Length: 6\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 6, 128)            380288    \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "repeat_vector_2 (RepeatVecto (None, 20, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 20, 128)           131584    \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 20, 1977)          255033    \n",
            "=================================================================\n",
            "Total params: 898,489\n",
            "Trainable params: 898,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "32/32 - 14s - loss: 5.5859 - val_loss: 2.0369\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.03685, saving model to model.h5\n",
            "Epoch 2/100\n",
            "32/32 - 7s - loss: 1.9046 - val_loss: 1.8414\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.03685 to 1.84136, saving model to model.h5\n",
            "Epoch 3/100\n",
            "32/32 - 7s - loss: 1.8212 - val_loss: 1.8045\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.84136 to 1.80455, saving model to model.h5\n",
            "Epoch 4/100\n",
            "32/32 - 7s - loss: 1.7770 - val_loss: 1.8613\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.80455\n",
            "Epoch 5/100\n",
            "32/32 - 7s - loss: 1.7758 - val_loss: 1.7714\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.80455 to 1.77141, saving model to model.h5\n",
            "Epoch 6/100\n",
            "32/32 - 7s - loss: 1.7198 - val_loss: 1.7637\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.77141 to 1.76369, saving model to model.h5\n",
            "Epoch 7/100\n",
            "32/32 - 7s - loss: 1.7006 - val_loss: 1.7401\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.76369 to 1.74013, saving model to model.h5\n",
            "Epoch 8/100\n",
            "32/32 - 7s - loss: 1.6748 - val_loss: 1.7254\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.74013 to 1.72540, saving model to model.h5\n",
            "Epoch 9/100\n",
            "32/32 - 7s - loss: 1.6464 - val_loss: 1.7141\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.72540 to 1.71409, saving model to model.h5\n",
            "Epoch 10/100\n",
            "32/32 - 7s - loss: 1.6330 - val_loss: 1.6977\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.71409 to 1.69766, saving model to model.h5\n",
            "Epoch 11/100\n",
            "32/32 - 7s - loss: 1.6147 - val_loss: 1.6886\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.69766 to 1.68863, saving model to model.h5\n",
            "Epoch 12/100\n",
            "32/32 - 7s - loss: 1.6010 - val_loss: 1.6758\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.68863 to 1.67580, saving model to model.h5\n",
            "Epoch 13/100\n",
            "32/32 - 7s - loss: 1.5870 - val_loss: 1.6656\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.67580 to 1.66562, saving model to model.h5\n",
            "Epoch 14/100\n",
            "32/32 - 7s - loss: 1.5723 - val_loss: 1.6515\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.66562 to 1.65148, saving model to model.h5\n",
            "Epoch 15/100\n",
            "32/32 - 7s - loss: 1.5543 - val_loss: 1.6356\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.65148 to 1.63559, saving model to model.h5\n",
            "Epoch 16/100\n",
            "32/32 - 7s - loss: 1.5335 - val_loss: 1.6179\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.63559 to 1.61789, saving model to model.h5\n",
            "Epoch 17/100\n",
            "32/32 - 7s - loss: 1.5073 - val_loss: 1.6001\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.61789 to 1.60012, saving model to model.h5\n",
            "Epoch 18/100\n",
            "32/32 - 7s - loss: 1.4758 - val_loss: 1.5895\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.60012 to 1.58949, saving model to model.h5\n",
            "Epoch 19/100\n",
            "32/32 - 7s - loss: 1.4428 - val_loss: 1.5736\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.58949 to 1.57357, saving model to model.h5\n",
            "Epoch 20/100\n",
            "32/32 - 7s - loss: 1.4102 - val_loss: 1.5515\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.57357 to 1.55147, saving model to model.h5\n",
            "Epoch 21/100\n",
            "32/32 - 7s - loss: 1.3801 - val_loss: 1.5357\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.55147 to 1.53567, saving model to model.h5\n",
            "Epoch 22/100\n",
            "32/32 - 7s - loss: 1.3469 - val_loss: 1.5268\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.53567 to 1.52681, saving model to model.h5\n",
            "Epoch 23/100\n",
            "32/32 - 7s - loss: 1.3197 - val_loss: 1.5173\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.52681 to 1.51725, saving model to model.h5\n",
            "Epoch 24/100\n",
            "32/32 - 7s - loss: 1.3014 - val_loss: 1.5265\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 1.51725\n",
            "Epoch 25/100\n",
            "32/32 - 7s - loss: 1.2876 - val_loss: 1.5049\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.51725 to 1.50490, saving model to model.h5\n",
            "Epoch 26/100\n",
            "32/32 - 7s - loss: 1.2739 - val_loss: 1.5138\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 1.50490\n",
            "Epoch 27/100\n",
            "32/32 - 7s - loss: 1.2582 - val_loss: 1.5005\n",
            "\n",
            "Epoch 00027: val_loss improved from 1.50490 to 1.50053, saving model to model.h5\n",
            "Epoch 28/100\n",
            "32/32 - 7s - loss: 1.2540 - val_loss: 1.5154\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 1.50053\n",
            "Epoch 29/100\n",
            "32/32 - 7s - loss: 1.2433 - val_loss: 1.5076\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 1.50053\n",
            "Epoch 30/100\n",
            "32/32 - 7s - loss: 1.2242 - val_loss: 1.5101\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 1.50053\n",
            "Epoch 31/100\n",
            "32/32 - 7s - loss: 1.2120 - val_loss: 1.5016\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 1.50053\n",
            "Epoch 32/100\n",
            "32/32 - 7s - loss: 1.2013 - val_loss: 1.5085\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 1.50053\n",
            "Epoch 33/100\n",
            "32/32 - 7s - loss: 1.1915 - val_loss: 1.5014\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 1.50053\n",
            "Epoch 34/100\n",
            "32/32 - 9s - loss: 1.1833 - val_loss: 1.5062\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 1.50053\n",
            "Epoch 35/100\n",
            "32/32 - 7s - loss: 1.1762 - val_loss: 1.5084\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 1.50053\n",
            "Epoch 36/100\n",
            "32/32 - 7s - loss: 1.1678 - val_loss: 1.5244\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 1.50053\n",
            "Epoch 37/100\n",
            "32/32 - 7s - loss: 1.1577 - val_loss: 1.5103\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 1.50053\n",
            "Epoch 38/100\n",
            "32/32 - 7s - loss: 1.1477 - val_loss: 1.5061\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 1.50053\n",
            "Epoch 39/100\n",
            "32/32 - 7s - loss: 1.1376 - val_loss: 1.5124\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 1.50053\n",
            "Epoch 40/100\n",
            "32/32 - 7s - loss: 1.1296 - val_loss: 1.5150\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 1.50053\n",
            "Epoch 41/100\n",
            "32/32 - 7s - loss: 1.1223 - val_loss: 1.5182\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 1.50053\n",
            "Epoch 42/100\n",
            "32/32 - 7s - loss: 1.1145 - val_loss: 1.5256\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 1.50053\n",
            "Epoch 43/100\n",
            "32/32 - 7s - loss: 1.1056 - val_loss: 1.5178\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 1.50053\n",
            "Epoch 44/100\n",
            "32/32 - 7s - loss: 1.0963 - val_loss: 1.5302\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 1.50053\n",
            "Epoch 45/100\n",
            "32/32 - 7s - loss: 1.0895 - val_loss: 1.5191\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 1.50053\n",
            "Epoch 46/100\n",
            "32/32 - 7s - loss: 1.0810 - val_loss: 1.5274\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 1.50053\n",
            "Epoch 47/100\n",
            "32/32 - 7s - loss: 1.0692 - val_loss: 1.5208\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 1.50053\n",
            "Epoch 48/100\n",
            "32/32 - 7s - loss: 1.0601 - val_loss: 1.5248\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 1.50053\n",
            "Epoch 49/100\n",
            "32/32 - 7s - loss: 1.0501 - val_loss: 1.5252\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 1.50053\n",
            "Epoch 50/100\n",
            "32/32 - 7s - loss: 1.0408 - val_loss: 1.5285\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 1.50053\n",
            "Epoch 51/100\n",
            "32/32 - 7s - loss: 1.0323 - val_loss: 1.5253\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 1.50053\n",
            "Epoch 52/100\n",
            "32/32 - 7s - loss: 1.0250 - val_loss: 1.5325\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 1.50053\n",
            "Epoch 53/100\n",
            "32/32 - 7s - loss: 1.0189 - val_loss: 1.5539\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 1.50053\n",
            "Epoch 54/100\n",
            "32/32 - 7s - loss: 1.0123 - val_loss: 1.5326\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 1.50053\n",
            "Epoch 55/100\n",
            "32/32 - 7s - loss: 1.0038 - val_loss: 1.5291\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 1.50053\n",
            "Epoch 56/100\n",
            "32/32 - 7s - loss: 0.9954 - val_loss: 1.5244\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 1.50053\n",
            "Epoch 57/100\n",
            "32/32 - 7s - loss: 0.9866 - val_loss: 1.5473\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 1.50053\n",
            "Epoch 58/100\n",
            "32/32 - 7s - loss: 0.9769 - val_loss: 1.5387\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 1.50053\n",
            "Epoch 59/100\n",
            "32/32 - 7s - loss: 0.9665 - val_loss: 1.5213\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 1.50053\n",
            "Epoch 60/100\n",
            "32/32 - 7s - loss: 0.9545 - val_loss: 1.5337\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 1.50053\n",
            "Epoch 61/100\n",
            "32/32 - 7s - loss: 0.9451 - val_loss: 1.5305\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 1.50053\n",
            "Epoch 62/100\n",
            "32/32 - 7s - loss: 0.9365 - val_loss: 1.5409\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 1.50053\n",
            "Epoch 63/100\n",
            "32/32 - 7s - loss: 0.9268 - val_loss: 1.5201\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 1.50053\n",
            "Epoch 64/100\n",
            "32/32 - 7s - loss: 0.9162 - val_loss: 1.5248\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 1.50053\n",
            "Epoch 65/100\n",
            "32/32 - 7s - loss: 0.9055 - val_loss: 1.5210\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 1.50053\n",
            "Epoch 66/100\n",
            "32/32 - 7s - loss: 0.8953 - val_loss: 1.5187\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 1.50053\n",
            "Epoch 67/100\n",
            "32/32 - 7s - loss: 0.8852 - val_loss: 1.5256\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 1.50053\n",
            "Epoch 68/100\n",
            "32/32 - 7s - loss: 0.8741 - val_loss: 1.5063\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 1.50053\n",
            "Epoch 69/100\n",
            "32/32 - 7s - loss: 0.8640 - val_loss: 1.5137\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 1.50053\n",
            "Epoch 70/100\n",
            "32/32 - 7s - loss: 0.8555 - val_loss: 1.5267\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 1.50053\n",
            "Epoch 71/100\n",
            "32/32 - 7s - loss: 0.8451 - val_loss: 1.5032\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 1.50053\n",
            "Epoch 72/100\n",
            "32/32 - 7s - loss: 0.8348 - val_loss: 1.5096\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 1.50053\n",
            "Epoch 73/100\n",
            "32/32 - 7s - loss: 0.8231 - val_loss: 1.5177\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 1.50053\n",
            "Epoch 74/100\n",
            "32/32 - 7s - loss: 0.8162 - val_loss: 1.5259\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 1.50053\n",
            "Epoch 75/100\n",
            "32/32 - 7s - loss: 0.8078 - val_loss: 1.5069\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 1.50053\n",
            "Epoch 76/100\n",
            "32/32 - 7s - loss: 0.7944 - val_loss: 1.5103\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 1.50053\n",
            "Epoch 77/100\n",
            "32/32 - 7s - loss: 0.7833 - val_loss: 1.5209\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 1.50053\n",
            "Epoch 78/100\n",
            "32/32 - 7s - loss: 0.7735 - val_loss: 1.5167\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 1.50053\n",
            "Epoch 79/100\n",
            "32/32 - 7s - loss: 0.7599 - val_loss: 1.4879\n",
            "\n",
            "Epoch 00079: val_loss improved from 1.50053 to 1.48787, saving model to model.h5\n",
            "Epoch 80/100\n",
            "32/32 - 7s - loss: 0.7471 - val_loss: 1.4923\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 1.48787\n",
            "Epoch 81/100\n",
            "32/32 - 7s - loss: 0.7349 - val_loss: 1.5027\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 1.48787\n",
            "Epoch 82/100\n",
            "32/32 - 7s - loss: 0.7230 - val_loss: 1.4951\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 1.48787\n",
            "Epoch 83/100\n",
            "32/32 - 7s - loss: 0.7102 - val_loss: 1.4989\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 1.48787\n",
            "Epoch 84/100\n",
            "32/32 - 7s - loss: 0.7007 - val_loss: 1.4911\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 1.48787\n",
            "Epoch 85/100\n",
            "32/32 - 7s - loss: 0.6917 - val_loss: 1.4996\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 1.48787\n",
            "Epoch 86/100\n",
            "32/32 - 7s - loss: 0.6832 - val_loss: 1.5045\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 1.48787\n",
            "Epoch 87/100\n",
            "32/32 - 7s - loss: 0.6712 - val_loss: 1.4963\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 1.48787\n",
            "Epoch 88/100\n",
            "32/32 - 7s - loss: 0.6590 - val_loss: 1.4884\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 1.48787\n",
            "Epoch 89/100\n",
            "32/32 - 7s - loss: 0.6507 - val_loss: 1.4822\n",
            "\n",
            "Epoch 00089: val_loss improved from 1.48787 to 1.48216, saving model to model.h5\n",
            "Epoch 90/100\n",
            "32/32 - 7s - loss: 0.6418 - val_loss: 1.4953\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 1.48216\n",
            "Epoch 91/100\n",
            "32/32 - 7s - loss: 0.6317 - val_loss: 1.4811\n",
            "\n",
            "Epoch 00091: val_loss improved from 1.48216 to 1.48108, saving model to model.h5\n",
            "Epoch 92/100\n",
            "32/32 - 7s - loss: 0.6195 - val_loss: 1.4971\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 1.48108\n",
            "Epoch 93/100\n",
            "32/32 - 7s - loss: 0.6093 - val_loss: 1.4909\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 1.48108\n",
            "Epoch 94/100\n",
            "32/32 - 7s - loss: 0.6018 - val_loss: 1.4907\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 1.48108\n",
            "Epoch 95/100\n",
            "32/32 - 7s - loss: 0.5952 - val_loss: 1.4832\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 1.48108\n",
            "Epoch 96/100\n",
            "32/32 - 7s - loss: 0.5862 - val_loss: 1.4746\n",
            "\n",
            "Epoch 00096: val_loss improved from 1.48108 to 1.47460, saving model to model.h5\n",
            "Epoch 97/100\n",
            "32/32 - 7s - loss: 0.5732 - val_loss: 1.5046\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 1.47460\n",
            "Epoch 98/100\n",
            "32/32 - 7s - loss: 0.5630 - val_loss: 1.4877\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 1.47460\n",
            "Epoch 99/100\n",
            "32/32 - 7s - loss: 0.5530 - val_loss: 1.4865\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 1.47460\n",
            "Epoch 100/100\n",
            "32/32 - 7s - loss: 0.5420 - val_loss: 1.4820\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 1.47460\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1d5eba0a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CclrQhRZ2hUp",
        "outputId": "ea089652-48e1-47b9-8384-aa9bf5163654"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# one hot encode target sequence\n",
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y\n",
        "\n",
        "# define NMT model\n",
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
        "print('English Max Length: %d' % (eng_length))\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "print('Thai Vocabulary Size: %d' % ger_vocab_size)\n",
        "print('Thai Max Length: %d' % (ger_length))\n",
        "\n",
        "# prepare training data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
        "trainY = encode_output(trainY, eng_vocab_size)\n",
        "# prepare validation data\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
        "testY = encode_output(testY, eng_vocab_size)\n",
        "\n",
        "# define model\n",
        "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 2048)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "# summarize defined model\n",
        "print(model.summary())\n",
        "plot_model(model, to_file='model.png', show_shapes=True)\n",
        "# fit model\n",
        "filename = 'model.h5'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "model.fit(trainX, trainY, epochs=100, batch_size=64, validation_data=(testX, testY), callbacks=[checkpoint], verbose=2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 1977\n",
            "English Max Length: 20\n",
            "Thai Vocabulary Size: 2971\n",
            "Thai Max Length: 6\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 6, 2048)           6084608   \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 2048)              33562624  \n",
            "_________________________________________________________________\n",
            "repeat_vector_3 (RepeatVecto (None, 20, 2048)          0         \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (None, 20, 2048)          33562624  \n",
            "_________________________________________________________________\n",
            "time_distributed_3 (TimeDist (None, 20, 1977)          4050873   \n",
            "=================================================================\n",
            "Total params: 77,260,729\n",
            "Trainable params: 77,260,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "32/32 - 599s - loss: 2.9676 - val_loss: 2.4212\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.42125, saving model to model.h5\n",
            "Epoch 2/100\n",
            "32/32 - 591s - loss: 2.0923 - val_loss: 1.9396\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.42125 to 1.93963, saving model to model.h5\n",
            "Epoch 3/100\n",
            "32/32 - 588s - loss: 1.7746 - val_loss: 1.7375\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.93963 to 1.73755, saving model to model.h5\n",
            "Epoch 4/100\n",
            "32/32 - 590s - loss: 1.6215 - val_loss: 1.6590\n",
            "\n",
            "Epoch 00004: val_loss improved from 1.73755 to 1.65902, saving model to model.h5\n",
            "Epoch 5/100\n",
            "32/32 - 589s - loss: 1.5069 - val_loss: 1.5993\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.65902 to 1.59929, saving model to model.h5\n",
            "Epoch 6/100\n",
            "32/32 - 592s - loss: 1.4098 - val_loss: 1.5751\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.59929 to 1.57514, saving model to model.h5\n",
            "Epoch 7/100\n",
            "32/32 - 590s - loss: 1.3333 - val_loss: 1.5578\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.57514 to 1.55784, saving model to model.h5\n",
            "Epoch 8/100\n",
            "32/32 - 589s - loss: 1.2849 - val_loss: 1.5457\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.55784 to 1.54573, saving model to model.h5\n",
            "Epoch 9/100\n",
            "32/32 - 593s - loss: 1.2431 - val_loss: 1.5399\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.54573 to 1.53986, saving model to model.h5\n",
            "Epoch 10/100\n",
            "32/32 - 591s - loss: 1.2128 - val_loss: 1.5264\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.53986 to 1.52642, saving model to model.h5\n",
            "Epoch 11/100\n",
            "32/32 - 589s - loss: 1.1756 - val_loss: 1.5084\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.52642 to 1.50844, saving model to model.h5\n",
            "Epoch 12/100\n",
            "32/32 - 595s - loss: 1.1443 - val_loss: 1.4952\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.50844 to 1.49521, saving model to model.h5\n",
            "Epoch 13/100\n",
            "32/32 - 590s - loss: 1.1100 - val_loss: 1.4834\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.49521 to 1.48341, saving model to model.h5\n",
            "Epoch 14/100\n",
            "32/32 - 591s - loss: 1.0747 - val_loss: 1.4720\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.48341 to 1.47205, saving model to model.h5\n",
            "Epoch 15/100\n",
            "32/32 - 591s - loss: 1.0431 - val_loss: 1.4641\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.47205 to 1.46415, saving model to model.h5\n",
            "Epoch 16/100\n",
            "32/32 - 594s - loss: 1.0139 - val_loss: 1.4594\n",
            "\n",
            "Epoch 00016: val_loss improved from 1.46415 to 1.45937, saving model to model.h5\n",
            "Epoch 17/100\n",
            "32/32 - 589s - loss: 0.9834 - val_loss: 1.4493\n",
            "\n",
            "Epoch 00017: val_loss improved from 1.45937 to 1.44934, saving model to model.h5\n",
            "Epoch 18/100\n",
            "32/32 - 590s - loss: 0.9654 - val_loss: 1.4328\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.44934 to 1.43278, saving model to model.h5\n",
            "Epoch 19/100\n",
            "32/32 - 598s - loss: 0.9375 - val_loss: 1.4333\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 1.43278\n",
            "Epoch 20/100\n",
            "32/32 - 588s - loss: 0.9124 - val_loss: 1.4261\n",
            "\n",
            "Epoch 00020: val_loss improved from 1.43278 to 1.42611, saving model to model.h5\n",
            "Epoch 21/100\n",
            "32/32 - 590s - loss: 0.8881 - val_loss: 1.4243\n",
            "\n",
            "Epoch 00021: val_loss improved from 1.42611 to 1.42432, saving model to model.h5\n",
            "Epoch 22/100\n",
            "32/32 - 595s - loss: 0.8734 - val_loss: 1.4172\n",
            "\n",
            "Epoch 00022: val_loss improved from 1.42432 to 1.41718, saving model to model.h5\n",
            "Epoch 23/100\n",
            "32/32 - 592s - loss: 0.8456 - val_loss: 1.4147\n",
            "\n",
            "Epoch 00023: val_loss improved from 1.41718 to 1.41466, saving model to model.h5\n",
            "Epoch 24/100\n",
            "32/32 - 597s - loss: 0.8235 - val_loss: 1.4044\n",
            "\n",
            "Epoch 00024: val_loss improved from 1.41466 to 1.40441, saving model to model.h5\n",
            "Epoch 25/100\n",
            "32/32 - 591s - loss: 0.7977 - val_loss: 1.4004\n",
            "\n",
            "Epoch 00025: val_loss improved from 1.40441 to 1.40038, saving model to model.h5\n",
            "Epoch 26/100\n",
            "32/32 - 588s - loss: 0.7682 - val_loss: 1.3887\n",
            "\n",
            "Epoch 00026: val_loss improved from 1.40038 to 1.38871, saving model to model.h5\n",
            "Epoch 27/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-026330d1e5f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'model.h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr6YfXKc7Yqi",
        "outputId": "6c311a0f-697b-4564-bdb4-b1d5eabab9a2"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณเกิดที่นี่ไหม], target=[Were you born here], predicted=[you you born here]\n",
            "src=[ใครสน], target=[Who cares], predicted=[who cares]\n",
            "src=[ฉันกังวล], target=[Im worried], predicted=[im worried]\n",
            "src=[ปัญหาคือคุณยังเด็กเกินไป], target=[The trouble is that you are too young], predicted=[the trouble is are are are too young]\n",
            "src=[ทอมไม่มา], target=[Tom isnt coming], predicted=[tom isnt coming]\n",
            "src=[เราจะทำอย่างนั้นได้อย่างไร], target=[How do we do that], predicted=[how do we do that]\n",
            "src=[ฉันนอนหลับดีมากเมื่อคืน], target=[I slept very well last night], predicted=[i slept very well last night]\n",
            "src=[เธอปฏิเสธข้อเสนอของเขา], target=[She refused his offer], predicted=[she never his offer]\n",
            "src=[ฉันได้ยินเด็ก ๆ ร้องเพลงด้วยกัน], target=[I heard the children singing together], predicted=[i heard the children singing together]\n",
            "src=[ฉันกำลังต้มน้ำ], target=[I am boiling water], predicted=[i am boiling today]\n",
            "BLEU-1: 0.569653\n",
            "BLEU-2: 0.463111\n",
            "BLEU-3: 0.399564\n",
            "BLEU-4: 0.260862\n",
            "test\n",
            "src=[ฉันรอทอม], target=[I waited for Tom], predicted=[i waited this tom]\n",
            "src=[พวกนี้ไม่ใช่หนังสือของคุณเหรอ], target=[Arent these your books], predicted=[your your your books]\n",
            "src=[หมายความว่าอะไร], target=[What does that mean], predicted=[what does that mean]\n",
            "src=[ตื่น], target=[Wake up], predicted=[wake up]\n",
            "src=[ฉันไม่ใช่คนดัง], target=[Im not a celebrity], predicted=[im not a celebrity]\n",
            "src=[ทำไมคุณถึงเรียนภาษาฝรั่งเศส], target=[Why are you studying French], predicted=[you are you studying french]\n",
            "src=[ฉันพอใจกับงานของฉัน], target=[Im content with my job], predicted=[im satisfied with my work]\n",
            "src=[ฉันชอบเพลงอาร์แอนด์บี], target=[I like R  B], predicted=[i like r b]\n",
            "src=[ฉันเป็นผู้หญิง], target=[Im a woman], predicted=[im a woman]\n",
            "src=[คุณหมายถึงอะไร], target=[What do you mean], predicted=[what do you mean]\n",
            "BLEU-1: 0.316375\n",
            "BLEU-2: 0.248595\n",
            "BLEU-3: 0.223346\n",
            "BLEU-4: 0.132261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jB1GAlrsxVry",
        "outputId": "9348030b-dd02-4665-a34f-5ea203bd4d47"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณเกิดที่นี่ไหม], target=[Were you born here], predicted=[were you born here]\n",
            "src=[ใครสน], target=[Who cares], predicted=[who cares]\n",
            "src=[ฉันกังวล], target=[Im worried], predicted=[im worried]\n",
            "src=[ปัญหาคือคุณยังเด็กเกินไป], target=[The trouble is that you are too young], predicted=[the trouble the that are is too]\n",
            "src=[ทอมไม่มา], target=[Tom isnt coming], predicted=[tom isnt coming]\n",
            "src=[เราจะทำอย่างนั้นได้อย่างไร], target=[How do we do that], predicted=[how do we do that]\n",
            "src=[ฉันนอนหลับดีมากเมื่อคืน], target=[I slept very well last night], predicted=[i slept very well last night]\n",
            "src=[เธอปฏิเสธข้อเสนอของเขา], target=[She refused his offer], predicted=[she refused his offer]\n",
            "src=[ฉันได้ยินเด็ก ๆ ร้องเพลงด้วยกัน], target=[I heard the children singing together], predicted=[i heard the children singing together]\n",
            "src=[ฉันกำลังต้มน้ำ], target=[I am boiling water], predicted=[i am boiling water]\n",
            "BLEU-1: 0.626647\n",
            "BLEU-2: 0.533923\n",
            "BLEU-3: 0.475710\n",
            "BLEU-4: 0.337213\n",
            "test\n",
            "src=[ฉันรอทอม], target=[I waited for Tom], predicted=[i waited for tom]\n",
            "src=[พวกนี้ไม่ใช่หนังสือของคุณเหรอ], target=[Arent these your books], predicted=[arent these your books]\n",
            "src=[หมายความว่าอะไร], target=[What does that mean], predicted=[what does that mean]\n",
            "src=[ตื่น], target=[Wake up], predicted=[get up]\n",
            "src=[ฉันไม่ใช่คนดัง], target=[Im not a celebrity], predicted=[im not a celebrity]\n",
            "src=[ทำไมคุณถึงเรียนภาษาฝรั่งเศส], target=[Why are you studying French], predicted=[why are you studying french]\n",
            "src=[ฉันพอใจกับงานของฉัน], target=[Im content with my job], predicted=[im satisfied with my french]\n",
            "src=[ฉันชอบเพลงอาร์แอนด์บี], target=[I like R  B], predicted=[i like r b]\n",
            "src=[ฉันเป็นผู้หญิง], target=[Im a woman], predicted=[im a woman]\n",
            "src=[คุณหมายถึงอะไร], target=[What do you mean], predicted=[what do you mean]\n",
            "BLEU-1: 0.344372\n",
            "BLEU-2: 0.287925\n",
            "BLEU-3: 0.269976\n",
            "BLEU-4: 0.179281\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho84ObYq1rei",
        "outputId": "54326d58-cda7-4dd9-95b7-419ca98b089e"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณเกิดที่นี่ไหม], target=[Were you born here], predicted=[tom you born here]\n",
            "src=[ใครสน], target=[Who cares], predicted=[who cares]\n",
            "src=[ฉันกังวล], target=[Im worried], predicted=[im worried]\n",
            "src=[ปัญหาคือคุณยังเด็กเกินไป], target=[The trouble is that you are too young], predicted=[the is is you you you too]\n",
            "src=[ทอมไม่มา], target=[Tom isnt coming], predicted=[tom isnt happy]\n",
            "src=[เราจะทำอย่างนั้นได้อย่างไร], target=[How do we do that], predicted=[what do do do that]\n",
            "src=[ฉันนอนหลับดีมากเมื่อคืน], target=[I slept very well last night], predicted=[i slept very in last night]\n",
            "src=[เธอปฏิเสธข้อเสนอของเขา], target=[She refused his offer], predicted=[i never his offer]\n",
            "src=[ฉันได้ยินเด็ก ๆ ร้องเพลงด้วยกัน], target=[I heard the children singing together], predicted=[i had the the the morning]\n",
            "src=[ฉันกำลังต้มน้ำ], target=[I am boiling water], predicted=[i am be french]\n",
            "BLEU-1: 0.357712\n",
            "BLEU-2: 0.230587\n",
            "BLEU-3: 0.166469\n",
            "BLEU-4: 0.072304\n",
            "test\n",
            "src=[ฉันรอทอม], target=[I waited for Tom], predicted=[i made tom tom]\n",
            "src=[พวกนี้ไม่ใช่หนังสือของคุณเหรอ], target=[Arent these your books], predicted=[that was my small]\n",
            "src=[หมายความว่าอะไร], target=[What does that mean], predicted=[what that that that]\n",
            "src=[ตื่น], target=[Wake up], predicted=[go up]\n",
            "src=[ฉันไม่ใช่คนดัง], target=[Im not a celebrity], predicted=[im not a spy]\n",
            "src=[ทำไมคุณถึงเรียนภาษาฝรั่งเศส], target=[Why are you studying French], predicted=[tom are you french french]\n",
            "src=[ฉันพอใจกับงานของฉัน], target=[Im content with my job], predicted=[im like to my question]\n",
            "src=[ฉันชอบเพลงอาร์แอนด์บี], target=[I like R  B], predicted=[i like like air]\n",
            "src=[ฉันเป็นผู้หญิง], target=[Im a woman], predicted=[im a woman]\n",
            "src=[คุณหมายถึงอะไร], target=[What do you mean], predicted=[what do you it]\n",
            "BLEU-1: 0.200883\n",
            "BLEU-2: 0.122882\n",
            "BLEU-3: 0.088166\n",
            "BLEU-4: 0.033255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5A-nCBY2v-y",
        "outputId": "999037c3-f62d-4d0e-922e-6215855e96ab"
      },
      "source": [
        "from pickle import load\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "# fit a tokenizer\n",
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer\n",
        "\n",
        "# max sentence length\n",
        "def max_length(lines):\n",
        "\treturn max(len(line.split()) for line in lines)\n",
        "\n",
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X\n",
        "\n",
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == integer:\n",
        "\t\t\treturn word\n",
        "\treturn None\n",
        "\n",
        "# generate target given source sequence\n",
        "def predict_sequence(model, tokenizer, source):\n",
        "\tprediction = model.predict(source, verbose=0)[0]\n",
        "\tintegers = [argmax(vector) for vector in prediction]\n",
        "\ttarget = list()\n",
        "\tfor i in integers:\n",
        "\t\tword = word_for_id(i, tokenizer)\n",
        "\t\tif word is None:\n",
        "\t\t\tbreak\n",
        "\t\ttarget.append(word)\n",
        "\treturn ' '.join(target)\n",
        "\n",
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
        "\tactual, predicted = list(), list()\n",
        "\tfor i, source in enumerate(sources):\n",
        "\t\t# translate encoded source text\n",
        "\t\tsource = source.reshape((1, source.shape[0]))\n",
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n",
        "\t\traw_target, raw_src,test = raw_dataset[i]\n",
        "\t\tif i < 10:\n",
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
        "\t\tactual.append([raw_target.split()])\n",
        "\t\tpredicted.append(translation.split())\n",
        "\t# calculate BLEU score\n",
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences('english-thai-both.pkl')\n",
        "train = load_clean_sentences('english-thai-train.pkl')\n",
        "test = load_clean_sentences('english-thai-test.pkl')\n",
        "# prepare english tokenizer\n",
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "eng_length = max_length(dataset[:, 0])\n",
        "# prepare german tokenizer\n",
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
        "ger_length = max_length(dataset[:, 1])\n",
        "# prepare data\n",
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
        "\n",
        "# load model\n",
        "model = load_model('model.h5')\n",
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(model, eng_tokenizer, trainX, train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(model, eng_tokenizer, testX, test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "src=[คุณเกิดที่นี่ไหม], target=[Were you born here], predicted=[she you met them]\n",
            "src=[ใครสน], target=[Who cares], predicted=[im quarreled]\n",
            "src=[ฉันกังวล], target=[Im worried], predicted=[im fell]\n",
            "src=[ปัญหาคือคุณยังเด็กเกินไป], target=[The trouble is that you are too young], predicted=[the husband is the good as a]\n",
            "src=[ทอมไม่มา], target=[Tom isnt coming], predicted=[it isnt coming]\n",
            "src=[เราจะทำอย่างนั้นได้อย่างไร], target=[How do we do that], predicted=[what i to to you]\n",
            "src=[ฉันนอนหลับดีมากเมื่อคืน], target=[I slept very well last night], predicted=[i have to to last]\n",
            "src=[เธอปฏิเสธข้อเสนอของเขา], target=[She refused his offer], predicted=[he may very me]\n",
            "src=[ฉันได้ยินเด็ก ๆ ร้องเพลงด้วยกัน], target=[I heard the children singing together], predicted=[i had breakfast at at]\n",
            "src=[ฉันกำลังต้มน้ำ], target=[I am boiling water], predicted=[i dont a it]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}